<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v0.0.0-4193">
<link rel="alternate" type="application/rss+xml" href="/FLAML/blog/rss.xml" title="FLAML RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/FLAML/blog/atom.xml" title="FLAML Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-react-helmet="true">5 posts tagged with &quot;LLM&quot; | FLAML</title><meta data-react-helmet="true" property="og:title" content="5 posts tagged with &quot;LLM&quot; | FLAML"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://microsoft.github.io//FLAML/blog/tags/llm"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/FLAML/img/flaml_logo.ico"><link data-react-helmet="true" rel="canonical" href="https://microsoft.github.io//FLAML/blog/tags/llm"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io//FLAML/blog/tags/llm" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io//FLAML/blog/tags/llm" hreflang="x-default"><link rel="stylesheet" href="/FLAML/assets/css/styles.a35b243d.css">
<link rel="preload" href="/FLAML/assets/js/runtime~main.597cf7c4.js" as="script">
<link rel="preload" href="/FLAML/assets/js/main.ced98023.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/FLAML/"><div class="navbar__logo"><img src="/FLAML/img/flaml_logo_fill.svg" alt="FLAML" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/FLAML/img/flaml_logo_fill.svg" alt="FLAML" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">FLAML</b></a><a class="navbar__item navbar__link" href="/FLAML/docs/Getting-Started">Docs</a><a class="navbar__item navbar__link" href="/FLAML/docs/reference/automl/automl">SDK</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/FLAML/blog">Blog</a><a class="navbar__item navbar__link" href="/FLAML/docs/FAQ">FAQ</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/FLAML" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ðŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ðŸŒž</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">Recent posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/FLAML/blog/2023/07/14/Local-LLMs">Use flaml.autogen for Local LLMs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/FLAML/blog/2023/06/28/MathChat">MathChat - An Conversational Framework to Solve Math Problems</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/FLAML/blog/2023/05/18/GPT-adaptive-humaneval">Achieve More, Pay Less - Use GPT-4 Smartly</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/FLAML/blog/2023/05/07/1M-milestone">Surpassing 1 Million Downloads - A Retrospective and a Look into the Future</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/FLAML/blog/2023/04/21/LLM-tuning-math">Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>5 posts tagged with &quot;LLM&quot;</h1><a href="/FLAML/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/FLAML/blog/2023/07/14/Local-LLMs">Use flaml.autogen for Local LLMs</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-07-14T00:00:00.000Z" itemprop="datePublished">July 14, 2023</time> Â· <!-- -->3 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://leoljl.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/LeoLjl/leoljl.github.io/blob/main/profile.jpg?raw=true" alt="Jiale Liu"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://leoljl.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jiale Liu</span></a></div><small class="avatar__subtitle" itemprop="description">Undergraduate student at Xidian University</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><strong>TL;DR:</strong>
We demonstrate how to use flaml.autogen for local LLM application. As an example, we will initiate an endpoint using <a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noopener noreferrer">FastChat</a> and perform inference on <a href="https://github.com/THUDM/ChatGLM2-6B" target="_blank" rel="noopener noreferrer">ChatGLMv2-6b</a>.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="preparations">Preparations<a aria-hidden="true" class="hash-link" href="#preparations" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="clone-fastchat">Clone FastChat<a aria-hidden="true" class="hash-link" href="#clone-fastchat" title="Direct link to heading">â€‹</a></h3><p>FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.</p><div class="codeBlockContainer_J+bg language-bash"><div class="codeBlockContent_csEI bash"><pre tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token function" style="color:rgb(130, 170, 255)">git</span><span class="token plain"> clone https://github.com/lm-sys/FastChat.git</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token builtin class-name" style="color:rgb(255, 203, 107)">cd</span><span class="token plain"> FastChat</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_y2LR" id="download-checkpoint">Download checkpoint<a aria-hidden="true" class="hash-link" href="#download-checkpoint" title="Direct link to heading">â€‹</a></h3><p>ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.</p><p>Before downloading from HuggingFace Hub, you need to have Git LFS <a href="https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage" target="_blank" rel="noopener noreferrer">installed</a>.</p><div class="codeBlockContainer_J+bg language-bash"><div class="codeBlockContent_csEI bash"><pre tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token function" style="color:rgb(130, 170, 255)">git</span><span class="token plain"> clone https://huggingface.co/THUDM/chatglm2-6b</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="initiate-server">Initiate server<a aria-hidden="true" class="hash-link" href="#initiate-server" title="Direct link to heading">â€‹</a></h2><p>First, launch the controller</p><div class="codeBlockContainer_J+bg language-bash"><div class="codeBlockContent_csEI bash"><pre tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">python -m fastchat.serve.controller</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Then, launch the model worker(s)</p><div class="codeBlockContainer_J+bg language-bash"><div class="codeBlockContent_csEI bash"><pre tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">python -m fastchat.serve.model_worker --model-path chatglm2-6b</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Finally, launch the RESTful API server</p><div class="codeBlockContainer_J+bg language-bash"><div class="codeBlockContent_csEI bash"><pre tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">python -m fastchat.serve.openai_api_server --host localhost --port </span><span class="token number" style="color:rgb(247, 140, 108)">8000</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Normally this will work. However, if you encounter error like <a href="https://github.com/lm-sys/FastChat/issues/1641" target="_blank" rel="noopener noreferrer">this</a>, commenting out all the lines containing <code>finish_reason</code> in <code>fastchat/protocol/api_protocal.py</code> and <code>fastchat/protocol/openai_api_protocol.py</code> will fix the problem. The modified code looks like:</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">class</span><span class="token plain"> </span><span class="token class-name" style="color:rgb(255, 203, 107)">CompletionResponseChoice</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">BaseModel</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    index</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">int</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    text</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    logprobs</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Optional</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token builtin" style="color:rgb(130, 170, 255)">int</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">None</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># finish_reason: Optional[Literal[&quot;stop&quot;, &quot;length&quot;]]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">class</span><span class="token plain"> </span><span class="token class-name" style="color:rgb(255, 203, 107)">CompletionResponseStreamChoice</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">BaseModel</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    index</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">int</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    text</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    logprobs</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Optional</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token builtin" style="color:rgb(130, 170, 255)">float</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">None</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># finish_reason: Optional[Literal[&quot;stop&quot;, &quot;length&quot;]] = None</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="interact-with-model-using-oaicompletion">Interact with model using <code>oai.Completion</code><a aria-hidden="true" class="hash-link" href="#interact-with-model-using-oaicompletion" title="Direct link to heading">â€‹</a></h2><p>Now the models can be directly accessed through openai-python library as well as <code>flaml.oai.Completion</code> and <code>flaml.oai.ChatCompletion</code>.</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> flaml </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> oai</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># create a text completion request</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">response </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> oai</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">Completion</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">create</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    config_list</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;chatglm2-6b&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_base&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;http://localhost:8000/v1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_type&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;open_ai&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_key&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;NULL&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># just a placeholder</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    prompt</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Hi&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">print</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">response</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># create a chat completion request</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">response </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> oai</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">ChatCompletion</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">create</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    config_list</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;chatglm2-6b&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_base&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;http://localhost:8000/v1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_type&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;open_ai&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_key&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;NULL&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    messages</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;role&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;user&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Hi&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">print</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">response</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>If you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s).</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="interacting-with-multiple-local-llms">interacting with multiple local LLMs<a aria-hidden="true" class="hash-link" href="#interacting-with-multiple-local-llms" title="Direct link to heading">â€‹</a></h2><p>If you would like to interact with multiple LLMs on your local machine, replace the <code>model_worker</code> step above with a multi model variant:</p><div class="codeBlockContainer_J+bg language-bash"><div class="codeBlockContent_csEI bash"><pre tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">python -m fastchat.serve.multi_model_worker </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    --model-path lmsys/vicuna-7b-v1.3 </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    --model-names vicuna-7b-v1.3 </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    --model-path chatglm2-6b </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    --model-names chatglm2-6b</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>The inference code would be:</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> flaml </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> oai</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># create a chat completion request</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">response </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> oai</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">ChatCompletion</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">create</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    config_list</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;chatglm2-6b&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_base&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;http://localhost:8000/v1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_type&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;open_ai&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_key&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;NULL&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;vicuna-7b-v1.3&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_base&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;http://localhost:8000/v1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_type&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;open_ai&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_key&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;NULL&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    messages</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;role&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;user&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Hi&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">print</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">response</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="for-further-reading">For Further Reading<a aria-hidden="true" class="hash-link" href="#for-further-reading" title="Direct link to heading">â€‹</a></h2><ul><li><a href="https://microsoft.github.io/autogen/" target="_blank" rel="noopener noreferrer">Documentation</a> about <code>flaml.autogen</code></li><li><a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noopener noreferrer">Documentation</a> about FastChat.</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/llm">LLM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/flam-lv-2">FLAMLv2</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Use flaml.autogen for Local LLMs" href="/FLAML/blog/2023/07/14/Local-LLMs"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/FLAML/blog/2023/06/28/MathChat">MathChat - An Conversational Framework to Solve Math Problems</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-06-28T00:00:00.000Z" itemprop="datePublished">June 28, 2023</time> Â· <!-- -->8 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/kevin666aa" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/kevin666aa.png" alt="Yiran Wu"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/kevin666aa" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Yiran Wu</span></a></div><small class="avatar__subtitle" itemprop="description">PhD student at Pennsylvania State University</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img alt="MathChat WorkFlow" src="/FLAML/assets/images/mathchatflow-926a8ed1975a114ab76c69996942c23a.png">
<strong>TL;DR:</strong></p><ul><li><strong>We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.</strong></li><li><strong>MathChat improves LLM&#x27;s performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.</strong></li><li><strong>Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.</strong></li></ul><p>Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.</p><p>In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.</p><p>We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="the-mathchat-framework">The MathChat Framework<a aria-hidden="true" class="hash-link" href="#the-mathchat-framework" title="Direct link to heading">â€‹</a></h2><p>MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.</p><p>The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:</p><ul><li><p><strong>Tool-using Prompt:</strong> This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.</p></li><li><p><strong>Problem-Solving Strategy Selection Prompt:</strong> The assistant is instructed to choose one of three potential problem-solving strategies, including:</p><ol><li>Writing a Python program to solve the problem directly.</li><li>Solving the problem directly without using Python, which exercises LLMs&#x27; inherent reasoning capabilities.</li><li>Solving the problem step by step with Python. This should be used when the first two strategies aren&#x27;t suitable.</li></ol></li><li><p><strong>Final Answer Encapsulation Prompt:</strong> This part instructs the assistant to put the final answer in <em>\boxed</em>{}.</p></li></ul><p>The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.</p><p>Let&#x27;s take a look at an example between the <strong>User Proxy Agent</strong> and the <strong>LLM Assistant</strong> (GPT-4). The conversation focuses on how to solve inequality using Python.
(The conversation is modified for readability.)</p><table><thead><tr><th>Participant</th><th>Dialogue</th></tr></thead><tbody><tr><td>User Proxy Agent</td><td>(Initial Prompt: Let&#x27;s solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) &lt; (3x+9)(x+8). Express your answer in interval notation.</td></tr><tr><td>LLM assistant</td><td>The key idea to solve this problem is to find the critical points, which are the points where the inequality is equal. Then, test the intervals between the critical points to find the interval for which the inequality holds.</td></tr><tr><td>LLM assistant (continued)</td><td>Here&#x27;s Python code to find the critical points: <code>from sympy import symbols, Eq, solve; x = symbols(&quot;x&quot;); lhs = (2*x + 10)*(x + 3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points = solve(equation, x); critical_points</code></td></tr><tr><td>User Proxy Agent</td><td>The critical points are: <!-- -->[-14, -3]</td></tr><tr><td>LLM assistant</td><td>We found the critical points to be x = -14 and x = -3. Now we&#x27;ll test the intervals between these critical points as well as points before and after the critical points. The intervals are: x &lt; -14, -14 &lt; x &lt; -3, x &gt; -3.</td></tr><tr><td>LLM assistant (continued)</td><td>Here&#x27;s Python code to test the inequality for each interval: <code>from sympy import symbols; x = symbols(&quot;x&quot;); inequality = (2*x + 10)*(x + 3) &lt; (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = []; for point in test_points: if inequality.subs(x, point): intervals.append(point); intervals</code></td></tr><tr><td>User Proxy Agent</td><td>The intervals where the inequality holds true are for test points: <!-- -->[-15, 0]</td></tr><tr><td>LLM assistant</td><td>We have tested the intervals and found that the inequality holds for: 1. x &lt; -14 (test point: -15) 2. x &gt; -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) &lt; (3x+9)(x+8) is x âˆˆ (-âˆž, -14) âˆª (-3, âˆž)</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_y2LR" id="experiment-setup">Experiment Setup<a aria-hidden="true" class="hash-link" href="#experiment-setup" title="Direct link to heading">â€‹</a></h2><p>We evaluate the improvement brought by MathChat.</p><p>For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.</p><p>We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer.  For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in <em>\boxed</em>{}, and we take the return of the function in PoT as the final answer.</p><p>We also evaluate the following methods for comparison:</p><ol><li><p><strong>Vanilla prompting:</strong> Evaluates GPT-4&#x27;s direct problem-solving capability. The prompt used is: <em>&quot; Solve the problem carefully. Put the final answer in \boxed{}&quot;</em>.</p></li><li><p><strong>Program of Thoughts (PoT):</strong> Uses a zero-shot PoT prompt that requests the model to create a <em>Solver</em> function to solve the problem and return the final answer.</p></li><li><p><strong>Program Synthesis (PS) prompting:</strong> Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: <em>&quot;Write a program that answers the following question: <!-- -->{<!-- -->Problem<!-- -->}<!-- -->&quot;</em>.</p></li></ol><h2 class="anchor anchorWithStickyNavbar_y2LR" id="experiment-results">Experiment Results<a aria-hidden="true" class="hash-link" href="#experiment-results" title="Direct link to heading">â€‹</a></h2><p>The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:</p><p><img alt="Result" src="/FLAML/assets/images/result-514251f4c387d5b6e11656e1fa33de08.png"></p><p>We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting &amp; Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.</p><p>For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.</p><p>The code for experiments can be found at this <a href="https://github.com/kevin666aa/FLAML/tree/gpt_math_solver/flaml/autogen/math" target="_blank" rel="noopener noreferrer">repository</a>.
We now provide an implementation of MathChat using the interactive agents in FLAML. See this <a href="https://github.com/microsoft/FLAML/blob/main/notebook/autogen_agentchat_MathChat.ipynb" target="_blank" rel="noopener noreferrer">notebook</a> for example usage.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="future-directions">Future Directions<a aria-hidden="true" class="hash-link" href="#future-directions" title="Direct link to heading">â€‹</a></h2><p>Despite MathChat&#x27;s improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.</p><p>Further work can be done to enhance this framework or math problem-solving in general:</p><ul><li>Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.</li><li>Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.</li><li>MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="for-further-reading">For Further Reading<a aria-hidden="true" class="hash-link" href="#for-further-reading" title="Direct link to heading">â€‹</a></h2><ul><li><a href="https://arxiv.org/abs/2306.01337" target="_blank" rel="noopener noreferrer">Research paper of MathChat</a></li><li><a href="https://microsoft.github.io/autogen/" target="_blank" rel="noopener noreferrer">Documentation about <code>flaml.autogen</code></a></li></ul><p><em>Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our <a href="https://discord.gg/Cppx2vSPVP" target="_blank" rel="noopener noreferrer">Discord</a> server for discussion.</em></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/llm">LLM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/gpt">GPT</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/research">research</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MathChat - An Conversational Framework to Solve Math Problems" href="/FLAML/blog/2023/06/28/MathChat"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/FLAML/blog/2023/05/18/GPT-adaptive-humaneval">Achieve More, Pay Less - Use GPT-4 Smartly</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-05-18T00:00:00.000Z" itemprop="datePublished">May 18, 2023</time> Â· <!-- -->8 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/chi-wang-49b15b16/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/sonichi.png" alt="Chi Wang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/chi-wang-49b15b16/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Chi Wang</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Researcher at Microsoft Research</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img alt="An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding success rate and inference cost" src="/FLAML/assets/images/humaneval-b2718b0b15d76b8fc59a4094e1bd684c.png"></p><p><strong>TL;DR:</strong></p><ul><li><strong>A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.</strong></li></ul><p>GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, <a href="https://huggingface.co/datasets/openai_humaneval" target="_blank" rel="noopener noreferrer">HumanEval</a>, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?</p><p>In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="observations">Observations<a aria-hidden="true" class="hash-link" href="#observations" title="Direct link to heading">â€‹</a></h2><ul><li>GPT-3.5-Turbo can alrady solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.</li><li>If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.</li></ul><p>The obstacle of leveraging these observations is that we do not know <em>a priori</em> which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.</p><p>To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let&#x27;s look at one example code completion task:</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">vowels_count</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;Write a function vowels_count which takes a string representing</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    a word as input and returns the number of vowels in the string.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Vowels in this case are &#x27;a&#x27;, &#x27;e&#x27;, &#x27;i&#x27;, &#x27;o&#x27;, &#x27;u&#x27;. Here, &#x27;y&#x27; is also a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    vowel, but only when it is at the end of the given word.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Example:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &gt;&gt;&gt; vowels_count(&quot;abcde&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    2</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &gt;&gt;&gt; vowels_count(&quot;ACEDY&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    3</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &quot;&quot;&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It&#x27;s not obvious (but an interesting research question!) how to predict the performance without actually trying.</p><p>What else can we do? We notice that:
<strong>It&#x27;s &quot;easier&quot; to verify a given solution than finding a correct solution from scratch.</strong></p><p>Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="solution">Solution<a aria-hidden="true" class="hash-link" href="#solution" title="Direct link to heading">â€‹</a></h2><p>Combining these observations, we can design a solution with two intuitive ideas:</p><ul><li>Make use of auto-generated feedback, i.e., code execution results, to filter responses.</li><li>Try inference configurations one by one, until one response can pass the filter.</li></ul><p><img alt="Design" src="/FLAML/assets/images/design-88630f6d8d95f1e31ef5ce79e81aab54.png"></p><p>This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.</p><p>An implementation of this solution is provided in <a href="/FLAML/docs/reference/autogen/code_utils#implement">flaml.autogen</a>. It uses the following sequence of configurations:</p><ol><li>GPT-3.5-Turbo, n=1, temperature=0</li><li>GPT-3.5-Turbo, n=7, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]</li><li>GPT-4, n=1, temperature=0</li><li>GPT-4, n=2, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]</li><li>GPT-4, n=1, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]</li></ol><h2 class="anchor anchorWithStickyNavbar_y2LR" id="experiment-results">Experiment Results<a aria-hidden="true" class="hash-link" href="#experiment-results" title="Direct link to heading">â€‹</a></h2><p>The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.
The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.</p><p>Here are a few examples of function definitions which are solved by different configurations in the portfolio.</p><ol><li>Solved by GPT-3.5-Turbo, n=1, temperature=0</li></ol><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">compare</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">game</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">guess</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;I think we all remember that feeling when the result of some long-awaited</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    event is finally known. The feelings and thoughts you have at that moment are</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    definitely worth noting down and comparing.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Your task is to determine if a person correctly guessed the results of a number of matches.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    You are given two arrays of scores and guesses of equal length, where each index shows a match.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Return an array of the same length denoting how far off each guess was. If they have guessed correctly,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    the value is 0, and if not, the value is the absolute difference between the guess and the score.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    example:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -&gt; [0,0,0,0,3,3]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -&gt; [4,4,1,0,0,6]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &quot;&quot;&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><ol start="2"><li>Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]<!-- -->: the <code>vowels_count</code> function presented earlier.</li><li>Solved by GPT-4, n=1, temperature=0:</li></ol><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">string_xor</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> b</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token operator" style="color:rgb(137, 221, 255)">&gt;</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot; Input are two strings a and b consisting only of 1s and 0s.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Perform binary XOR on these inputs and return result also as a string.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &gt;&gt;&gt; string_xor(&#x27;010&#x27;, &#x27;110&#x27;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &#x27;100&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &quot;&quot;&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><ol start="4"><li>Solved by GPT-4, n=2, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]<!-- -->:</li></ol><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">is_palindrome</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">string</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token operator" style="color:rgb(137, 221, 255)">&gt;</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">bool</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot; Test if given string is a palindrome &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> string </span><span class="token operator" style="color:rgb(137, 221, 255)">==</span><span class="token plain"> string</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">make_palindrome</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">string</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token operator" style="color:rgb(137, 221, 255)">&gt;</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot; Find the shortest palindrome that begins with a supplied string.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Algorithm idea is simple:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    - Find the longest postfix of supplied string that is a palindrome.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &gt;&gt;&gt; make_palindrome(&#x27;&#x27;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &#x27;&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &gt;&gt;&gt; make_palindrome(&#x27;cat&#x27;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &#x27;catac&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &gt;&gt;&gt; make_palindrome(&#x27;cata&#x27;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &#x27;catac&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &quot;&quot;&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><ol start="5"><li>Solved by GPT-4, n=1, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]<!-- -->:</li></ol><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">sort_array</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">arr</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    In this Kata, you have to sort an array of non-negative integers according to</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    number of ones in their binary representation in ascending order.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    For similar number of ones, sort based on decimal value.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    It must be implemented like this:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &gt;&gt;&gt; sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &gt;&gt;&gt; sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &gt;&gt;&gt; sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &quot;&quot;&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:</p><ul><li>Our adaptive solution has a certain degree of fault tolerance.</li><li>The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.</li></ul><p>It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.</p><p>An example notebook to run this experiment can be found at: <a href="https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb</a></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="discussion">Discussion<a aria-hidden="true" class="hash-link" href="#discussion" title="Direct link to heading">â€‹</a></h2><p>Our solution is quite simple to implement using a generic interface offered in <a href="https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#logic-error" target="_blank" rel="noopener noreferrer"><code>flaml.autogen</code></a>, yet the result is quite encouraging.</p><p>While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:</p><ul><li>Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.</li><li>Consider multiple configurations to generate responses - especially useful when:<ul><li>Model and other inference parameter choice affect the utility-cost tradeoff; or</li><li>Different configurations have complementary effect.</li></ul></li></ul><p>A <a href="/FLAML/blog/2023/04/21/LLM-tuning-math">previous blog post</a> provides evidence that these ideas are relevant in solving math problems too.
<code>flaml.autogen</code> uses a technique <a href="https://arxiv.org/abs/2303.04673" target="_blank" rel="noopener noreferrer">EcoOptiGen</a> to support inference parameter tuning and model selection.</p><p>There are many directions of extensions in research and development:</p><ul><li>Generalize the way to provide feedback.</li><li>Automate the process of optimizing the configurations.</li><li>Build adaptive agents for different applications.</li></ul><p><em>Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our <a href="https://discord.gg/Cppx2vSPVP" target="_blank" rel="noopener noreferrer">Discord</a> server for discussion.</em></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="for-further-reading">For Further Reading<a aria-hidden="true" class="hash-link" href="#for-further-reading" title="Direct link to heading">â€‹</a></h2><ul><li><a href="https://microsoft.github.io/autogen/" target="_blank" rel="noopener noreferrer">Documentation</a> about <code>flaml.autogen</code> and <a href="https://arxiv.org/abs/2303.04673" target="_blank" rel="noopener noreferrer">Research paper</a>.</li><li><a href="/FLAML/blog/2023/04/21/LLM-tuning-math">Blog post</a> about a related study for math.</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/llm">LLM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/gpt">GPT</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/research">research</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Achieve More, Pay Less - Use GPT-4 Smartly" href="/FLAML/blog/2023/05/18/GPT-adaptive-humaneval"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/FLAML/blog/2023/05/07/1M-milestone">Surpassing 1 Million Downloads - A Retrospective and a Look into the Future</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-05-07T00:00:00.000Z" itemprop="datePublished">May 7, 2023</time> Â· <!-- -->4 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://qingyun-wu.github.io/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/qingyun-wu.png" alt="Qingyun Wu"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://qingyun-wu.github.io/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Qingyun Wu</span></a></div><small class="avatar__subtitle" itemprop="description">Assistant Professor at the Pennsylvania State University</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><strong>TL;DR:</strong></p><ul><li><strong>Celebrating FLAML&#x27;s milestone: 1 million downloads</strong></li><li><strong>Introducing Large Language Model (LLM) support in the upcoming FLAML v2</strong></li></ul><p>This week, FLAML has reached a significant milestone: 1 million downloads. Originating as an intern research project within Microsoft Research, FLAML has grown into an open-source library used widely across the industry and supported by an active community.
As we celebrate this milestone, we want to recognize the passionate contributors and users who have played an essential role in molding FLAML into the flourishing project it is today. Our heartfelt gratitude goes out to each of you for your unwavering support, constructive feedback, and innovative contributions that have driven FLAML to new heights.
A big shoutout to our industrial collaborators from Azure Core, Azure Machine Learning, Azure Synapse Analytics, Microsoft 365, ML.NET, Vowpal Wabbit, Anyscale, Databricks, and Wise; and academic collaborators from MIT, Penn State University, Stevens Institute of Technology, Tel Aviv University, Texas A &amp; M University, University of Manchester, University of Washington, and The Chinese University of Hong Kong etc.</p><p>We&#x27;d also like to take the opportunity to reflect on FLAML&#x27;s past achievements and its future roadmap, with a particular focus on large language models (LLM) and LLMOps.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="flamls-journey-past-achievements-and-milestones">FLAML&#x27;s Journey: Past Achievements and Milestones<a aria-hidden="true" class="hash-link" href="#flamls-journey-past-achievements-and-milestones" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="bring-automl-to-ones-fingertips">Bring AutoML to One&#x27;s Fingertips<a aria-hidden="true" class="hash-link" href="#bring-automl-to-ones-fingertips" title="Direct link to heading">â€‹</a></h3><p>FLAML offers an off-the-shelf AutoML solution that enables users to quickly discover high-quality models or configurations for common ML/AI tasks. By automatically selecting models and hyperparameters for training or inference, FLAML saves users time and effort. FLAML has significantly reduced development time for developers and data scientists alike, while also providing a convenient way to integrate new algorithms into the pipeline, enabling easy extensions and large-scale parallel tuning. These features make FLAML a valuable tool in R&amp;D efforts for many enterprise users.
FLAML is capable of handling a variety of common ML tasks, such as <a href="https://microsoft.github.io/FLAML/docs/Examples/AutoML-Classification" target="_blank" rel="noopener noreferrer">classification</a>, <a href="https://microsoft.github.io/FLAML/docs/Examples/AutoML-Regression" target="_blank" rel="noopener noreferrer">regression</a>, <a href="https://microsoft.github.io/FLAML/docs/Examples/AutoML-Time%20series%20forecast" target="_blank" rel="noopener noreferrer">time series forecasting</a>, <a href="https://microsoft.github.io/FLAML/docs/Examples/AutoML-Rank" target="_blank" rel="noopener noreferrer">NLP tasks</a>, and <a href="https://microsoft.github.io/autogen/" target="_blank" rel="noopener noreferrer">generative tasks</a>, providing a comprehensive solution for various applications.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="speed-and-efficiency-the-flaml-advantage">Speed and Efficiency: The FLAML Advantage<a aria-hidden="true" class="hash-link" href="#speed-and-efficiency-the-flaml-advantage" title="Direct link to heading">â€‹</a></h3><p>What sets FLAML apart from other AutoML libraries is its exceptional efficiency, thanks to the economical and efficient hyperparameter optimization and model selection methods developed in our <a href="https://microsoft.github.io/FLAML/docs/Research" target="_blank" rel="noopener noreferrer">research</a>. FLAML is also capable of handling large search spaces with heterogeneous evaluation costs, complex constraints, guidance, and early stopping. The <a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Zero-Shot-AutoML" target="_blank" rel="noopener noreferrer">zero-shot AutoML</a> option further reduces the cost of AutoML, making FLAML an even more attractive solution for a wide range of applications with low resources.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="easy-customization-and-extensibility">Easy Customization and Extensibility<a aria-hidden="true" class="hash-link" href="#easy-customization-and-extensibility" title="Direct link to heading">â€‹</a></h3><p>FLAML is designed for easy extensibility and customization, allowing users to add custom learners, metrics, search space, etc. For example, the support of hierarchical search spaces allows one to first choose an ML learner and then sampling from the hyperparameter space specific to that learner. The level of customization ranges from minimal (providing only training data and task type as input) to full (tuning a user-defined function). This flexibility and support for easy customization have led to FLAML&#x27;s adoption in various domains, including security, finance, marketing, engineering, supply chain, insurance, and healthcare, delivering highly accurate results.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="embracing-large-language-models-in-flaml-v2">Embracing Large Language Models in FLAML v2<a aria-hidden="true" class="hash-link" href="#embracing-large-language-models-in-flaml-v2" title="Direct link to heading">â€‹</a></h2><p>As large language models continue to reshape the AI ecosystem, FLAML is poised to adapt and grow alongside these advancements. Recognizing the importance of large language models, we have recently incorporated an autogen package into FLAML, and are committed to focusing our collective efforts on addressing the unique challenges that arise in LLMOps (Large Language Model Operations).</p><p>In its current iteration, FLAML offers support for model selection and inference parameter tuning for large language models. We are actively working on the development of new features, such as low-level inference API with caching, templating, filtering, and higher-level components like LLM-based coding and interactive agents, to enable more effective and economical usage of LLM.</p><p>We are eagerly preparing for the launch of FLAML v2, where we will place special emphasis on incorporating and enhancing features specifically tailored for large language models (LLMs), further expanding FLAML&#x27;s capabilities.
We invite contributions from anyone interested in this topic and look forward to collaborating with the community as we shape the future of FLAML and LLMOps together.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="for-further-reading">For Further Reading<a aria-hidden="true" class="hash-link" href="#for-further-reading" title="Direct link to heading">â€‹</a></h2><ul><li><a href="https://microsoft.github.io/autogen/" target="_blank" rel="noopener noreferrer">Documentation about <code>flaml.autogen</code></a></li><li><a href="https://github.com/microsoft/FLAML/blob/main/notebook/autogen_chatgpt_gpt4.ipynb" target="_blank" rel="noopener noreferrer">Code Example: Tune chatGPT for Math Problem Solving with FLAML</a></li></ul><p><em>Do you have any experience to share about LLM applications? Do you like to see more support or research of LLMOps? Please join our <a href="https://discord.gg/Cppx2vSPVP" target="_blank" rel="noopener noreferrer">Discord</a> server for discussion.</em></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/llm">LLM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/llm-ops">LLMOps</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/flam-lv-2">FLAMLv2</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Surpassing 1 Million Downloads - A Retrospective and a Look into the Future" href="/FLAML/blog/2023/05/07/1M-milestone"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/FLAML/blog/2023/04/21/LLM-tuning-math">Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-04-21T00:00:00.000Z" itemprop="datePublished">April 21, 2023</time> Â· <!-- -->5 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/chi-wang-49b15b16/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/sonichi.png" alt="Chi Wang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/chi-wang-49b15b16/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Chi Wang</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Researcher at Microsoft Research</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img alt="level 2 algebra" src="/FLAML/assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png"></p><p><strong>TL;DR:</strong></p><ul><li><strong>Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.</strong></li><li><strong>For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.</strong></li><li><strong>FLAML can help with model selection, parameter tuning, and cost-saving in LLM applications.</strong></li></ul><p>Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?</p><p>In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for <a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html" target="_blank" rel="noopener noreferrer">MATH</a>, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.</p><p>We will use the new subpackage <a href="/FLAML/blog/tags/docs/Use-Cases/Autogen"><code>flaml.autogen</code></a> to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search &amp; pruning strategy. FLAML currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.</p><p>We will use FLAML to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="experiment-setup">Experiment Setup<a aria-hidden="true" class="hash-link" href="#experiment-setup" title="Direct link to heading">â€‹</a></h2><p>We use FLAML to select between the following models with a target inference budget $0.02 per instance:</p><ul><li>gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app</li><li>gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo</li></ul><p>We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:</p><ul><li>temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of <!-- -->[0, 1]<!-- -->.</li><li>top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of <!-- -->[0, 1]<!-- -->.</li><li>max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of <!-- -->[50, 1000]<!-- -->.</li><li>n: The number of responses to generate. We search for the optimal n in the range of <!-- -->[1, 100]<!-- -->.</li><li>prompt: We use the template: &quot;{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in <!-- -->\<!-- -->boxed{{}}.&quot; where {problem} will be replaced by the math problem instance.</li></ul><p>In this experiment, when n &gt; 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="experiment-results">Experiment Results<a aria-hidden="true" class="hash-link" href="#experiment-results" title="Direct link to heading">â€‹</a></h2><p>The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.</p><p>Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.
The same observation can be obtained on the level 3 Algebra test set.</p><p><img alt="level 3 algebra" src="/FLAML/assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png"></p><p>However, the selected model changes on level 4 Algebra.</p><p><img alt="level 4 algebra" src="/FLAML/assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png"></p><p>This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.
On level 5 the result is similar.</p><p><img alt="level 5 algebra" src="/FLAML/assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png"></p><p>We can see that FLAML has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.</p><p>An example notebook to run these experiments can be found at: <a href="https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb</a></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="analysis-and-discussion">Analysis and Discussion<a aria-hidden="true" class="hash-link" href="#analysis-and-discussion" title="Direct link to heading">â€‹</a></h2><p>While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.</p><p>There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via <a href="/FLAML/blog/tags/docs/Use-Cases/Tune-User-Defined-Function"><code>flaml.tune</code></a>.</p><p>The need for model selection, parameter tuning and cost saving is not specific to the math problems. The <a href="https://github.com/Significant-Gravitas/Auto-GPT" target="_blank" rel="noopener noreferrer">Auto-GPT</a> project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="for-further-reading">For Further Reading<a aria-hidden="true" class="hash-link" href="#for-further-reading" title="Direct link to heading">â€‹</a></h2><ul><li><a href="https://arxiv.org/abs/2303.04673" target="_blank" rel="noopener noreferrer">Research paper about the tuning technique</a></li><li><a href="https://microsoft.github.io/autogen/" target="_blank" rel="noopener noreferrer">Documentation about <code>flaml.autogen</code></a></li></ul><p><em>Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our <a href="https://discord.gg/Cppx2vSPVP" target="_blank" rel="noopener noreferrer">Discord</a> server for discussion.</em></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/llm">LLM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/gpt">GPT</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/research">research</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH" href="/FLAML/blog/2023/04/21/LLM-tuning-math"><b>Read More</b></a></div></footer></article></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://discord.gg/Cppx2vSPVP" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2025 FLAML Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/FLAML/assets/js/runtime~main.597cf7c4.js"></script>
<script src="/FLAML/assets/js/main.ced98023.js"></script>
</body>
</html>