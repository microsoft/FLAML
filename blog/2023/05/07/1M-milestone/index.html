<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v0.0.0-4193">
<link rel="alternate" type="application/rss+xml" href="/FLAML/blog/rss.xml" title="FLAML RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/FLAML/blog/atom.xml" title="FLAML Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-react-helmet="true">Surpassing 1 Million Downloads - A Retrospective and a Look into the Future | FLAML</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://microsoft.github.io//FLAML/blog/2023/05/07/1M-milestone"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" property="og:title" content="Surpassing 1 Million Downloads - A Retrospective and a Look into the Future | FLAML"><meta data-react-helmet="true" name="description" content="TL;DR:"><meta data-react-helmet="true" property="og:description" content="TL;DR:"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="article:published_time" content="2023-05-07T00:00:00.000Z"><meta data-react-helmet="true" property="article:author" content="https://qingyun-wu.github.io/"><meta data-react-helmet="true" property="article:tag" content="LLM,LLMOps,FLAMLv2"><link data-react-helmet="true" rel="shortcut icon" href="/FLAML/img/flaml_logo.ico"><link data-react-helmet="true" rel="canonical" href="https://microsoft.github.io//FLAML/blog/2023/05/07/1M-milestone"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io//FLAML/blog/2023/05/07/1M-milestone" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io//FLAML/blog/2023/05/07/1M-milestone" hreflang="x-default"><link rel="stylesheet" href="/FLAML/assets/css/styles.a35b243d.css">
<link rel="preload" href="/FLAML/assets/js/runtime~main.b9b0c2e0.js" as="script">
<link rel="preload" href="/FLAML/assets/js/main.ec32d380.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/FLAML/"><div class="navbar__logo"><img src="/FLAML/img/flaml_logo_fill.svg" alt="FLAML" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/FLAML/img/flaml_logo_fill.svg" alt="FLAML" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">FLAML</b></a><a class="navbar__item navbar__link" href="/FLAML/docs/Getting-Started">Docs</a><a class="navbar__item navbar__link" href="/FLAML/docs/reference/automl/automl">SDK</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/FLAML/blog">Blog</a><a class="navbar__item navbar__link" href="/FLAML/docs/FAQ">FAQ</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/FLAML" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ðŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ðŸŒž</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">Recent posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/FLAML/blog/2023/05/18/GPT-adaptive-humaneval">Achieve More, Pay Less - Use GPT-4 Smartly</a></li><li class="sidebarItem_cjdF"><a aria-current="page" class="sidebarItemLink_zyXk sidebarItemLinkActive_wcJs" href="/FLAML/blog/2023/05/07/1M-milestone">Surpassing 1 Million Downloads - A Retrospective and a Look into the Future</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/FLAML/blog/2023/04/21/LLM-tuning-math">Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_d4p0" itemprop="headline">Surpassing 1 Million Downloads - A Retrospective and a Look into the Future</h1><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-05-07T00:00:00.000Z" itemprop="datePublished">May 7, 2023</time> Â· <!-- -->4 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://qingyun-wu.github.io/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/qingyun-wu.png" alt="Qingyun Wu"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://qingyun-wu.github.io/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Qingyun Wu</span></a></div><small class="avatar__subtitle" itemprop="description">Assistant Professor at the Pennsylvania State University</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><strong>TL;DR:</strong></p><ul><li><strong>Celebrating FLAML&#x27;s milestone: 1 million downloads</strong></li><li><strong>Introducing Large Language Model (LLM) support in the upcoming FLAML v2</strong></li></ul><p>This week, FLAML has reached a significant milestone: 1 million downloads. Originating as an intern research project within Microsoft Research, FLAML has grown into an open-source library used widely across the industry and supported by an active community.
As we celebrate this milestone, we want to recognize the passionate contributors and users who have played an essential role in molding FLAML into the flourishing project it is today. Our heartfelt gratitude goes out to each of you for your unwavering support, constructive feedback, and innovative contributions that have driven FLAML to new heights.
A big shoutout to our industrial collaborators from Azure Core, Azure Machine Learning, Azure Synapse Analytics, Microsoft 365, ML.NET, Vowpal Wabbit, Anyscale, Databricks, and Wise; and academic collaborators from MIT, Penn State University, Stevens Institute of Technology, Tel Aviv University, Texas A &amp; M University, University of Manchester, University of Washington, and The Chinese University of Hong Kong etc.</p><p>We&#x27;d also like to take the opportunity to reflect on FLAML&#x27;s past achievements and its future roadmap, with a particular focus on large language models (LLM) and LLMOps.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="flamls-journey-past-achievements-and-milestones">FLAML&#x27;s Journey: Past Achievements and Milestones<a aria-hidden="true" class="hash-link" href="#flamls-journey-past-achievements-and-milestones" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="bring-automl-to-ones-fingertips">Bring AutoML to One&#x27;s Fingertips<a aria-hidden="true" class="hash-link" href="#bring-automl-to-ones-fingertips" title="Direct link to heading">â€‹</a></h3><p>FLAML offers an off-the-shelf AutoML solution that enables users to quickly discover high-quality models or configurations for common ML/AI tasks. By automatically selecting models and hyperparameters for training or inference, FLAML saves users time and effort. FLAML has significantly reduced development time for developers and data scientists alike, while also providing a convenient way to integrate new algorithms into the pipeline, enabling easy extensions and large-scale parallel tuning. These features make FLAML a valuable tool in R&amp;D efforts for many enterprise users.
FLAML is capable of handling a variety of common ML tasks, such as <a href="https://microsoft.github.io/FLAML/docs/Examples/AutoML-Classification" target="_blank" rel="noopener noreferrer">classification</a>, <a href="https://microsoft.github.io/FLAML/docs/Examples/AutoML-Regression" target="_blank" rel="noopener noreferrer">regression</a>, <a href="https://microsoft.github.io/FLAML/docs/Examples/AutoML-Time%20series%20forecast" target="_blank" rel="noopener noreferrer">time series forecasting</a>, <a href="https://microsoft.github.io/FLAML/docs/Examples/AutoML-Rank" target="_blank" rel="noopener noreferrer">NLP tasks</a>, and <a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Auto-Generation" target="_blank" rel="noopener noreferrer">generative tasks</a>, providing a comprehensive solution for various applications.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="speed-and-efficiency-the-flaml-advantage">Speed and Efficiency: The FLAML Advantage<a aria-hidden="true" class="hash-link" href="#speed-and-efficiency-the-flaml-advantage" title="Direct link to heading">â€‹</a></h3><p>What sets FLAML apart from other AutoML libraries is its exceptional efficiency, thanks to the economical and efficient hyperparameter optimization and model selection methods developed in our <a href="https://microsoft.github.io/FLAML/docs/Research" target="_blank" rel="noopener noreferrer">research</a>. FLAML is also capable of handling large search spaces with heterogeneous evaluation costs, complex constraints, guidance, and early stopping. The <a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Zero-Shot-AutoML" target="_blank" rel="noopener noreferrer">zero-shot AutoML</a> option further reduces the cost of AutoML, making FLAML an even more attractive solution for a wide range of applications with low resources.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="easy-customization-and-extensibility">Easy Customization and Extensibility<a aria-hidden="true" class="hash-link" href="#easy-customization-and-extensibility" title="Direct link to heading">â€‹</a></h3><p>FLAML is designed for easy extensibility and customization, allowing users to add custom learners, metrics, search space, etc. For example, the support of hierarchical search spaces allows one to first choose an ML learner and then sampling from the hyperparameter space specific to that learner. The level of customization ranges from minimal (providing only training data and task type as input) to full (tuning a user-defined function). This flexibility and support for easy customization have led to FLAML&#x27;s adoption in various domains, including security, finance, marketing, engineering, supply chain, insurance, and healthcare, delivering highly accurate results.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="embracing-large-language-models-in-flaml-v2">Embracing Large Language Models in FLAML v2<a aria-hidden="true" class="hash-link" href="#embracing-large-language-models-in-flaml-v2" title="Direct link to heading">â€‹</a></h2><p>As large language models continue to reshape the AI ecosystem, FLAML is poised to adapt and grow alongside these advancements. Recognizing the importance of large language models, we have recently incorporated an autogen package into FLAML, and are committed to focusing our collective efforts on addressing the unique challenges that arise in LLMOps (Large Language Model Operations).</p><p>In its current iteration, FLAML offers support for model selection and inference parameter tuning for large language models. We are actively working on the development of new features, such as low-level inference API with caching, templating, filtering, and higher-level components like LLM-based coding and interactive agents, to enable more effective and economical usage of LLM.</p><p>We are eagerly preparing for the launch of FLAML v2, where we will place special emphasis on incorporating and enhancing features specifically tailored for large language models (LLMs), further expanding FLAML&#x27;s capabilities.
We invite contributions from anyone interested in this topic and look forward to collaborating with the community as we shape the future of FLAML and LLMOps together.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="for-further-reading">For Further Reading<a aria-hidden="true" class="hash-link" href="#for-further-reading" title="Direct link to heading">â€‹</a></h2><ul><li><a href="/FLAML/docs/Use-Cases/Auto-Generation">Documentation about <code>flaml.autogen</code></a></li><li><a href="https://github.com/microsoft/FLAML/blob/main/notebook/autogen_chatgpt_gpt4.ipynb" target="_blank" rel="noopener noreferrer">Code Example: Tune chatGPT for Math Problem Solving with FLAML</a></li></ul><p><em>Do you have any experience to share about LLM applications? Do you like to see more support or research of LLMOps? Please join our <a href="https://discord.gg/Cppx2vSPVP" target="_blank" rel="noopener noreferrer">Discord</a> server for discussion.</em></p></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_xD8n"><div class="col"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/llm">LLM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/llm-ops">LLMOps</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/FLAML/blog/tags/flam-lv-2">FLAMLv2</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/FLAML/blog/2023/05/18/GPT-adaptive-humaneval"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Â« <!-- -->Achieve More, Pay Less - Use GPT-4 Smartly</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/FLAML/blog/2023/04/21/LLM-tuning-math"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH<!-- --> Â»</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#flamls-journey-past-achievements-and-milestones" class="table-of-contents__link toc-highlight">FLAML&#39;s Journey: Past Achievements and Milestones</a><ul><li><a href="#bring-automl-to-ones-fingertips" class="table-of-contents__link toc-highlight">Bring AutoML to One&#39;s Fingertips</a></li><li><a href="#speed-and-efficiency-the-flaml-advantage" class="table-of-contents__link toc-highlight">Speed and Efficiency: The FLAML Advantage</a></li><li><a href="#easy-customization-and-extensibility" class="table-of-contents__link toc-highlight">Easy Customization and Extensibility</a></li></ul></li><li><a href="#embracing-large-language-models-in-flaml-v2" class="table-of-contents__link toc-highlight">Embracing Large Language Models in FLAML v2</a></li><li><a href="#for-further-reading" class="table-of-contents__link toc-highlight">For Further Reading</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://discord.gg/Cppx2vSPVP" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023 FLAML Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/FLAML/assets/js/runtime~main.b9b0c2e0.js"></script>
<script src="/FLAML/assets/js/main.ec32d380.js"></script>
</body>
</html>