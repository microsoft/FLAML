{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/autogen_agentchat_chess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Group Chat\n",
    "\n",
    "Modified based on https://github.com/microsoft/FLAML/blob/4ea686af5c3e8ff24d9076a7a626c8b28ab5b1d7/notebook/autogen_multiagent_roleplay_chat.ipynb\n",
    "\n",
    "## Requirements\n",
    "\n",
    "FLAML requires `Python>=3.8`. To run this notebook example, please install flaml with the [autogen] option:\n",
    "```bash\n",
    "pip install flaml[autogen]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install flaml[autogen]~=2.0.0rc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/FLAML/docs/reference/autogen/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\"],\n",
    "    },\n",
    ")\n",
    "# config_list_gpt35 = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": {\n",
    "#             \"gpt-3.5-turbo\",\n",
    "#             \"gpt-3.5-turbo-16k\",\n",
    "#             \"gpt-3.5-turbo-0301\",\n",
    "#             \"chatgpt-35-turbo-0301\",\n",
    "#             \"gpt-35-turbo-v0301\",\n",
    "#         },\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "llm_config = {\"config_list\": config_list_gpt4}\n",
    "broadcaster = autogen.BroadcastAgent()\n",
    "chat_manager = autogen.ChatManagerAgent(broadcaster=broadcaster, max_round=3)\n",
    "human = autogen.GroupChatParticipant(\n",
    "   name=\"Human\",\n",
    "   system_message=\"A human admin.\",\n",
    "   human_input_mode=\"ALWAYS\",\n",
    "   llm_config=False,\n",
    "   chat_manager=chat_manager,\n",
    ")\n",
    "alice = autogen.GroupChatParticipant(\n",
    "    name=\"Alice\",\n",
    "    system_message=autogen.AssistantAgent.DEFAULT_SYSTEM_MESSAGE,\n",
    "    max_consecutive_auto_reply=sys.maxsize,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False,\n",
    "    chat_manager=chat_manager,\n",
    ")\n",
    "bob = autogen.GroupChatParticipant(\n",
    "    name=\"Bob\",\n",
    "    system_message=\"Code reviewer. Prevent code execution if unsafe or not well documented. Suggest changes. Otherwise, approve and return the final code to execute.\",\n",
    "    max_consecutive_auto_reply=sys.maxsize,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False,\n",
    "    chat_manager=chat_manager,\n",
    ")\n",
    "\n",
    "broadcaster.agents = [human, alice, bob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mbroadcaster\u001b[0m (to chat_manager):\n",
      "\n",
      "find a latest paper about generative agents\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchat_manager\u001b[0m (to broadcaster):\n",
      "\n",
      "find a latest paper about generative agents\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mbroadcaster\u001b[0m (to Human):\n",
      "\n",
      "find a latest paper about generative agents\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mbroadcaster\u001b[0m (to Alice):\n",
      "\n",
      "find a latest paper about generative agents\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mbroadcaster\u001b[0m (to Bob):\n",
      "\n",
      "find a latest paper about generative agents\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchat_manager\u001b[0m (to Human):\n",
      "\n",
      "You are in a role play game. Read the following conversation.\n",
      "Then select the next role from ['Human', 'Alice', 'Bob'] to play. Only return the role.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mHuman\u001b[0m (to chat_manager):\n",
      "\n",
      "Alice\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchat_manager\u001b[0m (to Alice):\n",
      "\n",
      "speak\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAlice\u001b[0m (to broadcaster):\n",
      "\n",
      "As an AI, I can't browse the web, but I can suggest Python code that allows you to do so. You can use Python to perform a Google Scholar search and scrape data from result pages. Here's a simple example using the BeautifulSoup and requests libraries:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Google scholar search URL\n",
      "url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=generative+agents&btnG=\"\n",
      "\n",
      "# Send a request to the website\n",
      "r = requests.get(url)\n",
      "\n",
      "# Get the data from the website\n",
      "webpage = r.text\n",
      "\n",
      "# Create a BeautifulSoup object and specify the parser\n",
      "soup = BeautifulSoup(webpage, 'html.parser')\n",
      "\n",
      "# Find all the research paper titles in the webpage\n",
      "titles = soup.find_all('h3', {'class': 'gs_rt'})\n",
      "\n",
      "# Print the latest paper title\n",
      "print(\"Latest paper about 'Generative Agents': \", titles[0].text)\n",
      "```\n",
      "\n",
      "This code will output the title of the latest paper about 'Generative Agents'. Please kindly note that this code might not always work as expected due to Google's frequent updates to their websites or restrictions for web scraping. In a professional setting, official APIs or databases should be used for finding scientific literature.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mbroadcaster\u001b[0m (to Human):\n",
      "\n",
      "As an AI, I can't browse the web, but I can suggest Python code that allows you to do so. You can use Python to perform a Google Scholar search and scrape data from result pages. Here's a simple example using the BeautifulSoup and requests libraries:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Google scholar search URL\n",
      "url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=generative+agents&btnG=\"\n",
      "\n",
      "# Send a request to the website\n",
      "r = requests.get(url)\n",
      "\n",
      "# Get the data from the website\n",
      "webpage = r.text\n",
      "\n",
      "# Create a BeautifulSoup object and specify the parser\n",
      "soup = BeautifulSoup(webpage, 'html.parser')\n",
      "\n",
      "# Find all the research paper titles in the webpage\n",
      "titles = soup.find_all('h3', {'class': 'gs_rt'})\n",
      "\n",
      "# Print the latest paper title\n",
      "print(\"Latest paper about 'Generative Agents': \", titles[0].text)\n",
      "```\n",
      "\n",
      "This code will output the title of the latest paper about 'Generative Agents'. Please kindly note that this code might not always work as expected due to Google's frequent updates to their websites or restrictions for web scraping. In a professional setting, official APIs or databases should be used for finding scientific literature.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mbroadcaster\u001b[0m (to Bob):\n",
      "\n",
      "As an AI, I can't browse the web, but I can suggest Python code that allows you to do so. You can use Python to perform a Google Scholar search and scrape data from result pages. Here's a simple example using the BeautifulSoup and requests libraries:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Google scholar search URL\n",
      "url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=generative+agents&btnG=\"\n",
      "\n",
      "# Send a request to the website\n",
      "r = requests.get(url)\n",
      "\n",
      "# Get the data from the website\n",
      "webpage = r.text\n",
      "\n",
      "# Create a BeautifulSoup object and specify the parser\n",
      "soup = BeautifulSoup(webpage, 'html.parser')\n",
      "\n",
      "# Find all the research paper titles in the webpage\n",
      "titles = soup.find_all('h3', {'class': 'gs_rt'})\n",
      "\n",
      "# Print the latest paper title\n",
      "print(\"Latest paper about 'Generative Agents': \", titles[0].text)\n",
      "```\n",
      "\n",
      "This code will output the title of the latest paper about 'Generative Agents'. Please kindly note that this code might not always work as expected due to Google's frequent updates to their websites or restrictions for web scraping. In a professional setting, official APIs or databases should be used for finding scientific literature.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchat_manager\u001b[0m (to Alice):\n",
      "\n",
      "You are in a role play game. Read the following conversation.\n",
      "Then select the next role from ['Human', 'Alice', 'Bob'] to play. Only return the role.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAlice\u001b[0m (to chat_manager):\n",
      "\n",
      "Bob\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchat_manager\u001b[0m (to Bob):\n",
      "\n",
      "speak\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mBob\u001b[0m (to broadcaster):\n",
      "\n",
      "It looks like the code is generally safe to execute. But as noted, it's always best to access official APIs or databases especially in a professional setting in order to avoid any legal issues related to web scraping. Google may also block your IP if it detects you're scraping its pages without permission. \n",
      "\n",
      "Further, the code doesn't handle possible exceptions that could occur while making a request. We should catch any potential exceptions to avoid crashes. Also, if the page HTML structure changes and a \"`<h3>`\" tag or class \"`gs_rt`\" is not found, the code will fail. It is a good practice to check if the data we've scraped contains any elements before trying to access them.\n",
      "\n",
      "Here is the improved version:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Google scholar search URL\n",
      "url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=generative+agents&btnG=\"\n",
      "\n",
      "try: \n",
      "    # Send a request to the website\n",
      "    r = requests.get(url)\n",
      "    r.raise_for_status()  # If the request is successful, no Exception will be raised\n",
      "    \n",
      "    # Get the data from the website\n",
      "    webpage = r.text\n",
      "    \n",
      "    # Create a BeautifulSoup object and specify the parser\n",
      "    soup = BeautifulSoup(webpage, 'html.parser')\n",
      "    \n",
      "    # Find all the research paper titles in the webpage\n",
      "    titles = soup.find_all('h3', {'class': 'gs_rt'})\n",
      "    \n",
      "    # Check if the titles list contain any elements\n",
      "    if titles:\n",
      "        # Print the latest paper title\n",
      "        print(\"Latest paper about 'Generative Agents': \", titles[0].text)\n",
      "    else:\n",
      "        print(\"No paper found.\")\n",
      "except requests.exceptions.RequestException as err:\n",
      "    print(\"Oops: Something went wrong \",err)\n",
      "except Exception as e:\n",
      "    print(\"Oops: \", e)\n",
      "```\n",
      "\n",
      "This modification ensures that we can catch and handle HTTP errors or any other exceptions to prevent crash and make debugging easier. It also checks if any titles were found before trying to display the first one.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mbroadcaster\u001b[0m (to Human):\n",
      "\n",
      "It looks like the code is generally safe to execute. But as noted, it's always best to access official APIs or databases especially in a professional setting in order to avoid any legal issues related to web scraping. Google may also block your IP if it detects you're scraping its pages without permission. \n",
      "\n",
      "Further, the code doesn't handle possible exceptions that could occur while making a request. We should catch any potential exceptions to avoid crashes. Also, if the page HTML structure changes and a \"`<h3>`\" tag or class \"`gs_rt`\" is not found, the code will fail. It is a good practice to check if the data we've scraped contains any elements before trying to access them.\n",
      "\n",
      "Here is the improved version:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Google scholar search URL\n",
      "url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=generative+agents&btnG=\"\n",
      "\n",
      "try: \n",
      "    # Send a request to the website\n",
      "    r = requests.get(url)\n",
      "    r.raise_for_status()  # If the request is successful, no Exception will be raised\n",
      "    \n",
      "    # Get the data from the website\n",
      "    webpage = r.text\n",
      "    \n",
      "    # Create a BeautifulSoup object and specify the parser\n",
      "    soup = BeautifulSoup(webpage, 'html.parser')\n",
      "    \n",
      "    # Find all the research paper titles in the webpage\n",
      "    titles = soup.find_all('h3', {'class': 'gs_rt'})\n",
      "    \n",
      "    # Check if the titles list contain any elements\n",
      "    if titles:\n",
      "        # Print the latest paper title\n",
      "        print(\"Latest paper about 'Generative Agents': \", titles[0].text)\n",
      "    else:\n",
      "        print(\"No paper found.\")\n",
      "except requests.exceptions.RequestException as err:\n",
      "    print(\"Oops: Something went wrong \",err)\n",
      "except Exception as e:\n",
      "    print(\"Oops: \", e)\n",
      "```\n",
      "\n",
      "This modification ensures that we can catch and handle HTTP errors or any other exceptions to prevent crash and make debugging easier. It also checks if any titles were found before trying to display the first one.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mbroadcaster\u001b[0m (to Alice):\n",
      "\n",
      "It looks like the code is generally safe to execute. But as noted, it's always best to access official APIs or databases especially in a professional setting in order to avoid any legal issues related to web scraping. Google may also block your IP if it detects you're scraping its pages without permission. \n",
      "\n",
      "Further, the code doesn't handle possible exceptions that could occur while making a request. We should catch any potential exceptions to avoid crashes. Also, if the page HTML structure changes and a \"`<h3>`\" tag or class \"`gs_rt`\" is not found, the code will fail. It is a good practice to check if the data we've scraped contains any elements before trying to access them.\n",
      "\n",
      "Here is the improved version:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Google scholar search URL\n",
      "url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=generative+agents&btnG=\"\n",
      "\n",
      "try: \n",
      "    # Send a request to the website\n",
      "    r = requests.get(url)\n",
      "    r.raise_for_status()  # If the request is successful, no Exception will be raised\n",
      "    \n",
      "    # Get the data from the website\n",
      "    webpage = r.text\n",
      "    \n",
      "    # Create a BeautifulSoup object and specify the parser\n",
      "    soup = BeautifulSoup(webpage, 'html.parser')\n",
      "    \n",
      "    # Find all the research paper titles in the webpage\n",
      "    titles = soup.find_all('h3', {'class': 'gs_rt'})\n",
      "    \n",
      "    # Check if the titles list contain any elements\n",
      "    if titles:\n",
      "        # Print the latest paper title\n",
      "        print(\"Latest paper about 'Generative Agents': \", titles[0].text)\n",
      "    else:\n",
      "        print(\"No paper found.\")\n",
      "except requests.exceptions.RequestException as err:\n",
      "    print(\"Oops: Something went wrong \",err)\n",
      "except Exception as e:\n",
      "    print(\"Oops: \", e)\n",
      "```\n",
      "\n",
      "This modification ensures that we can catch and handle HTTP errors or any other exceptions to prevent crash and make debugging easier. It also checks if any titles were found before trying to display the first one.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchat_manager\u001b[0m (to Human):\n",
      "\n",
      "You are in a role play game. Read the following conversation.\n",
      "Then select the next role from ['Human', 'Alice', 'Bob'] to play. Only return the role.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mHuman\u001b[0m (to chat_manager):\n",
      "\n",
      "Human\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchat_manager\u001b[0m (to Human):\n",
      "\n",
      "speak\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mHuman\u001b[0m (to broadcaster):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Latest paper about 'Generative Agents':  Generative agents for player decision modeling in games\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mbroadcaster\u001b[0m (to Alice):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Latest paper about 'Generative Agents':  Generative agents for player decision modeling in games\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mbroadcaster\u001b[0m (to Bob):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Latest paper about 'Generative Agents':  Generative agents for player decision modeling in games\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "broadcaster.send(\"find a latest paper about generative agents\", chat_manager)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
