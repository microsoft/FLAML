{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Copyright (c) 2020-2021 Microsoft Corporation. All rights reserved. \n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Demo of AutoML with FLAML Library\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "FLAML is a Python library (https://github.com/microsoft/FLAML) designed to automatically produce accurate machine learning models \n",
    "with low computational cost. It is fast and cheap. The simple and lightweight design makes it easy \n",
    "to use and extend, such as adding new learners. FLAML can \n",
    "- serve as an economical AutoML engine,\n",
    "- be used as a fast hyperparameter tuning tool, or \n",
    "- be embedded in self-tuning software that requires low latency & resource in repetitive\n",
    "   tuning tasks.\n",
    "\n",
    "In this notebook, we demonstrate how to ues FLAML library to tune hyperparameters of LightGBM with a regression example.\n",
    "\n",
    "FLAML requires `Python>=3.6`. To run this notebook example, please install flaml with the `notebook` option:\n",
    "```bash\n",
    "pip install flaml[notebook]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: flaml[notebook] in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: catboost>=0.23 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from flaml[notebook]) (0.23.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from flaml[notebook]) (1.4.1)\n",
      "Requirement already satisfied: optuna==2.3.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from flaml[notebook]) (2.3.0)\n",
      "Requirement already satisfied: xgboost>=0.90 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from flaml[notebook]) (1.3.3)\n",
      "Requirement already satisfied: NumPy>=1.16.2 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from flaml[notebook]) (1.18.4)\n",
      "Requirement already satisfied: lightgbm>=2.3.1 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from flaml[notebook]) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23.2 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from flaml[notebook]) (0.23.2)\n",
      "Requirement already satisfied: rgf-python; extra == \"notebook\" in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from flaml[notebook]) (3.9.0)\n",
      "Requirement already satisfied: matplotlib==3.2.0; extra == \"notebook\" in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from flaml[notebook]) (3.2.0)\n",
      "Requirement already satisfied: openml==0.10.2; extra == \"notebook\" in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from flaml[notebook]) (0.10.2)\n",
      "Requirement already satisfied: jupyter; extra == \"notebook\" in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from flaml[notebook]) (1.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from catboost>=0.23->flaml[notebook]) (1.14.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from catboost>=0.23->flaml[notebook]) (0.24.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from catboost>=0.23->flaml[notebook]) (0.14.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from catboost>=0.23->flaml[notebook]) (4.9.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from optuna==2.3.0->flaml[notebook]) (4.6.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from optuna==2.3.0->flaml[notebook]) (4.56.1)\n",
      "Requirement already satisfied: cliff in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from optuna==2.3.0->flaml[notebook]) (3.5.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from optuna==2.3.0->flaml[notebook]) (1.3.20)\n",
      "Requirement already satisfied: alembic in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from optuna==2.3.0->flaml[notebook]) (1.4.1)\n",
      "Requirement already satisfied: cmaes>=0.6.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from optuna==2.3.0->flaml[notebook]) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from optuna==2.3.0->flaml[notebook]) (20.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from optuna==2.3.0->flaml[notebook]) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from scikit-learn>=0.23.2->flaml[notebook]) (2.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from matplotlib==3.2.0; extra == \"notebook\"->flaml[notebook]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from matplotlib==3.2.0; extra == \"notebook\"->flaml[notebook]) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from matplotlib==3.2.0; extra == \"notebook\"->flaml[notebook]) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from matplotlib==3.2.0; extra == \"notebook\"->flaml[notebook]) (2.4.7)\n",
      "Requirement already satisfied: xmltodict in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from openml==0.10.2; extra == \"notebook\"->flaml[notebook]) (0.12.0)\n",
      "Requirement already satisfied: liac-arff>=2.4.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from openml==0.10.2; extra == \"notebook\"->flaml[notebook]) (2.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from openml==0.10.2; extra == \"notebook\"->flaml[notebook]) (2.25.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from jupyter; extra == \"notebook\"->flaml[notebook]) (7.5.1)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from jupyter; extra == \"notebook\"->flaml[notebook]) (6.2.0)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from jupyter; extra == \"notebook\"->flaml[notebook]) (4.7.7)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from jupyter; extra == \"notebook\"->flaml[notebook]) (5.3.4)\n",
      "Requirement already satisfied: notebook in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from jupyter; extra == \"notebook\"->flaml[notebook]) (6.1.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from jupyter; extra == \"notebook\"->flaml[notebook]) (5.6.1)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from pandas>=0.24.0->catboost>=0.23->flaml[notebook]) (2020.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from plotly->catboost>=0.23->flaml[notebook]) (1.3.3)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from colorlog->optuna==2.3.0->flaml[notebook]) (0.4.3)\n",
      "Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cliff->optuna==2.3.0->flaml[notebook]) (1.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.12 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cliff->optuna==2.3.0->flaml[notebook]) (5.3.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cliff->optuna==2.3.0->flaml[notebook]) (5.5.1)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cliff->optuna==2.3.0->flaml[notebook]) (3.2.2)\n",
      "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cliff->optuna==2.3.0->flaml[notebook]) (0.7.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from alembic->optuna==2.3.0->flaml[notebook]) (1.1.3)\n",
      "Requirement already satisfied: python-editor>=0.3 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from alembic->optuna==2.3.0->flaml[notebook]) (1.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from requests->openml==0.10.2; extra == \"notebook\"->flaml[notebook]) (1.26.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from requests->openml==0.10.2; extra == \"notebook\"->flaml[notebook]) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from requests->openml==0.10.2; extra == \"notebook\"->flaml[notebook]) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from requests->openml==0.10.2; extra == \"notebook\"->flaml[notebook]) (2.6)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (7.19.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (5.0.7)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (5.0.5)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-console->jupyter; extra == \"notebook\"->flaml[notebook]) (6.1.7)\n",
      "Requirement already satisfied: pygments in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-console->jupyter; extra == \"notebook\"->flaml[notebook]) (2.7.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-console->jupyter; extra == \"notebook\"->flaml[notebook]) (3.0.8)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from qtconsole->jupyter; extra == \"notebook\"->flaml[notebook]) (4.7.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from qtconsole->jupyter; extra == \"notebook\"->flaml[notebook]) (0.2.0)\n",
      "Requirement already satisfied: qtpy in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from qtconsole->jupyter; extra == \"notebook\"->flaml[notebook]) (1.9.0)\n",
      "Requirement already satisfied: pyzmq>=17.1 in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from qtconsole->jupyter; extra == \"notebook\"->flaml[notebook]) (20.0.0)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel->jupyter; extra == \"notebook\"->flaml[notebook]) (6.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from notebook->jupyter; extra == \"notebook\"->flaml[notebook]) (0.8.3)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from notebook->jupyter; extra == \"notebook\"->flaml[notebook]) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from notebook->jupyter; extra == \"notebook\"->flaml[notebook]) (0.8.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from notebook->jupyter; extra == \"notebook\"->flaml[notebook]) (20.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from notebook->jupyter; extra == \"notebook\"->flaml[notebook]) (2.11.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from nbconvert->jupyter; extra == \"notebook\"->flaml[notebook]) (1.4.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from nbconvert->jupyter; extra == \"notebook\"->flaml[notebook]) (0.4.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from nbconvert->jupyter; extra == \"notebook\"->flaml[notebook]) (0.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from nbconvert->jupyter; extra == \"notebook\"->flaml[notebook]) (3.1.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from nbconvert->jupyter; extra == \"notebook\"->flaml[notebook]) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from nbconvert->jupyter; extra == \"notebook\"->flaml[notebook]) (0.6.0)\n",
      "Requirement already satisfied: pyreadline; sys_platform == \"win32\" in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna==2.3.0->flaml[notebook]) (2.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna==2.3.0->flaml[notebook]) (1.7.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna==2.3.0->flaml[notebook]) (1.8.1)\n",
      "Requirement already satisfied: attrs>=16.3.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna==2.3.0->flaml[notebook]) (19.3.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna==2.3.0->flaml[notebook]) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from Mako->alembic->optuna==2.3.0->flaml[notebook]) (1.1.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (0.17.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (50.3.2)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (4.4.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (3.2.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from jupyter-core->qtconsole->jupyter; extra == \"notebook\"->flaml[notebook]) (228)\n",
      "Requirement already satisfied: pywinpty>=0.5; os_name == \"nt\" in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter; extra == \"notebook\"->flaml[notebook]) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from argon2-cffi->notebook->jupyter; extra == \"notebook\"->flaml[notebook]) (1.14.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from bleach->nbconvert->jupyter; extra == \"notebook\"->flaml[notebook]) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna==2.3.0->flaml[notebook]) (3.1.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\chiw\\appdata\\roaming\\python\\python37\\site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (0.7.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter; extra == \"notebook\"->flaml[notebook]) (0.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\chiw\\miniconda3\\envs\\flaml\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter; extra == \"notebook\"->flaml[notebook]) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install flaml[notebook];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Real Data Example\n",
    "### Load data and preprocess\n",
    "\n",
    "Download [Houses dataset](https://www.openml.org/d/537) from OpenML. The task is to predict median price of the house in the region based on demographic composition and a state of housing market in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "download dataset from openml\n",
      "Dataset name: houses\n",
      "X_train.shape: (15480, 8), y_train.shape: (15480,);\n",
      "X_test.shape: (5160, 8), y_test.shape: (5160,)\n"
     ]
    }
   ],
   "source": [
    "from flaml.data import load_openml_dataset\n",
    "X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id = 537, data_dir = './')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Run FLAML\n",
    "In the FLAML automl run configuration, users can specify the task type, time budget, error metric, learner list, whether to subsample, resampling strategy type, and so on. All these arguments have default values which will be used if users do not provide them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "''' import AutoML class from flaml package '''\n",
    "from flaml import AutoML\n",
    "automl = AutoML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"time_budget\": 180, # total running time in seconds\n",
    "    \"metric\": 'r2', # primary metrics for regression can be chosen from: ['mae','mse','r2']\n",
    "    \"estimator_list\": ['lgbm'], # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'regression', # task type    \n",
    "    \"log_file_name\": 'houses_experiment.log', # flaml log file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[flaml.automl: 02-22 11:53:25] {839} INFO - Evaluation method: cv\n",
      "[flaml.automl: 02-22 11:53:25] {568} INFO - Using RepeatedKFold\n",
      "[flaml.automl: 02-22 11:53:25] {860} INFO - Minimizing error metric: 1-r2\n",
      "[flaml.automl: 02-22 11:53:25] {880} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
      "[flaml.automl: 02-22 11:53:25] {939} INFO - iteration 0  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:25] {1093} INFO -  at 0.2s,\tbest lgbm's error=0.7383,\tbest lgbm's error=0.7383\n",
      "[flaml.automl: 02-22 11:53:25] {939} INFO - iteration 1  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:25] {1093} INFO -  at 0.3s,\tbest lgbm's error=0.7383,\tbest lgbm's error=0.7383\n",
      "[flaml.automl: 02-22 11:53:25] {939} INFO - iteration 2  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:25] {1093} INFO -  at 0.4s,\tbest lgbm's error=0.4578,\tbest lgbm's error=0.4578\n",
      "[flaml.automl: 02-22 11:53:25] {939} INFO - iteration 3  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:25] {1093} INFO -  at 0.5s,\tbest lgbm's error=0.4578,\tbest lgbm's error=0.4578\n",
      "[flaml.automl: 02-22 11:53:25] {939} INFO - iteration 4  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:26] {1093} INFO -  at 0.8s,\tbest lgbm's error=0.2637,\tbest lgbm's error=0.2637\n",
      "[flaml.automl: 02-22 11:53:26] {939} INFO - iteration 5  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:26] {1093} INFO -  at 1.1s,\tbest lgbm's error=0.2284,\tbest lgbm's error=0.2284\n",
      "[flaml.automl: 02-22 11:53:26] {939} INFO - iteration 6  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:26] {1093} INFO -  at 1.4s,\tbest lgbm's error=0.2284,\tbest lgbm's error=0.2284\n",
      "[flaml.automl: 02-22 11:53:26] {939} INFO - iteration 7  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:27] {1093} INFO -  at 1.8s,\tbest lgbm's error=0.2284,\tbest lgbm's error=0.2284\n",
      "[flaml.automl: 02-22 11:53:27] {939} INFO - iteration 8  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:27] {1093} INFO -  at 2.0s,\tbest lgbm's error=0.2284,\tbest lgbm's error=0.2284\n",
      "[flaml.automl: 02-22 11:53:27] {939} INFO - iteration 9  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:27] {1093} INFO -  at 2.2s,\tbest lgbm's error=0.2284,\tbest lgbm's error=0.2284\n",
      "[flaml.automl: 02-22 11:53:27] {939} INFO - iteration 10  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:27] {1093} INFO -  at 2.4s,\tbest lgbm's error=0.2284,\tbest lgbm's error=0.2284\n",
      "[flaml.automl: 02-22 11:53:27] {939} INFO - iteration 11  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:28] {1093} INFO -  at 2.8s,\tbest lgbm's error=0.2262,\tbest lgbm's error=0.2262\n",
      "[flaml.automl: 02-22 11:53:28] {939} INFO - iteration 12  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:29] {1093} INFO -  at 4.2s,\tbest lgbm's error=0.2009,\tbest lgbm's error=0.2009\n",
      "[flaml.automl: 02-22 11:53:29] {939} INFO - iteration 13  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:29] {1093} INFO -  at 4.4s,\tbest lgbm's error=0.2009,\tbest lgbm's error=0.2009\n",
      "[flaml.automl: 02-22 11:53:29] {939} INFO - iteration 14  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:34] {1093} INFO -  at 8.9s,\tbest lgbm's error=0.1854,\tbest lgbm's error=0.1854\n",
      "[flaml.automl: 02-22 11:53:34] {939} INFO - iteration 15  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:36] {1093} INFO -  at 11.4s,\tbest lgbm's error=0.1854,\tbest lgbm's error=0.1854\n",
      "[flaml.automl: 02-22 11:53:36] {939} INFO - iteration 16  current learner lgbm\n",
      "[flaml.automl: 02-22 11:53:48] {1093} INFO -  at 23.3s,\tbest lgbm's error=0.1761,\tbest lgbm's error=0.1761\n",
      "[flaml.automl: 02-22 11:53:48] {939} INFO - iteration 17  current learner lgbm\n",
      "[flaml.automl: 02-22 11:54:23] {1093} INFO -  at 57.9s,\tbest lgbm's error=0.1725,\tbest lgbm's error=0.1725\n",
      "[flaml.automl: 02-22 11:54:23] {939} INFO - iteration 18  current learner lgbm\n",
      "[flaml.automl: 02-22 11:55:01] {1093} INFO -  at 96.5s,\tbest lgbm's error=0.1725,\tbest lgbm's error=0.1725\n",
      "[flaml.automl: 02-22 11:55:01] {939} INFO - iteration 19  current learner lgbm\n",
      "[flaml.automl: 02-22 11:55:04] {1093} INFO -  at 99.6s,\tbest lgbm's error=0.1725,\tbest lgbm's error=0.1725\n",
      "[flaml.automl: 02-22 11:55:04] {939} INFO - iteration 20  current learner lgbm\n",
      "[flaml.automl: 02-22 11:55:20] {1093} INFO -  at 114.7s,\tbest lgbm's error=0.1563,\tbest lgbm's error=0.1563\n",
      "[flaml.automl: 02-22 11:55:20] {939} INFO - iteration 21  current learner lgbm\n",
      "[flaml.automl: 02-22 11:55:23] {1093} INFO -  at 118.0s,\tbest lgbm's error=0.1563,\tbest lgbm's error=0.1563\n",
      "[flaml.automl: 02-22 11:55:23] {939} INFO - iteration 22  current learner lgbm\n",
      "[flaml.automl: 02-22 11:56:06] {1093} INFO -  at 161.2s,\tbest lgbm's error=0.1563,\tbest lgbm's error=0.1563\n",
      "[flaml.automl: 02-22 11:56:06] {939} INFO - iteration 23  current learner lgbm\n",
      "[flaml.automl: 02-22 11:56:19] {1093} INFO -  at 174.0s,\tbest lgbm's error=0.1563,\tbest lgbm's error=0.1563\n",
      "[flaml.automl: 02-22 11:56:19] {1133} INFO - selected model: LGBMRegressor(colsample_bytree=0.9046814915274195,\n",
      "              learning_rate=0.025065630491840726, max_bin=255,\n",
      "              min_child_weight=20.0, n_estimators=451, num_leaves=113,\n",
      "              objective='regression', reg_alpha=8.352751749829367e-10,\n",
      "              reg_lambda=0.13991138691596908)\n",
      "[flaml.automl: 02-22 11:56:19] {894} INFO - fit succeeded\n"
     ]
    }
   ],
   "source": [
    "'''The main flaml automl API'''\n",
    "automl.fit(X_train = X_train, y_train = y_train, **settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Best model and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best hyperparmeter config: {'n_estimators': 451.0, 'max_leaves': 113.0, 'min_child_weight': 20.0, 'learning_rate': 0.025065630491840726, 'subsample': 1.0, 'log_max_bin': 8.0, 'colsample_bytree': 0.9046814915274195, 'reg_alpha': 8.352751749829367e-10, 'reg_lambda': 0.13991138691596908}\nBest r2 on validation data: 0.8437\nTraining duration of best run: 15.14 s\n"
     ]
    }
   ],
   "source": [
    "''' retrieve best config'''\n",
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best r2 on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMRegressor(learning_rate=0.02944211100744363, max_bin=1023,\n",
       "              min_child_weight=20.0, n_estimators=329, num_leaves=17,\n",
       "              objective='regression', reg_alpha=3.617531807484476e-05,\n",
       "              reg_lambda=1.0)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "automl.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "''' pickle and save the best model '''\n",
    "import pickle\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(automl.model, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted labels [147056.672508   246591.18821626 155253.69332074 ... 196516.76693923\n 235571.37776252 270133.77185961]\nTrue labels [136900. 241300. 200700. ... 160900. 227300. 265600.]\n"
     ]
    }
   ],
   "source": [
    "''' compute predictions of testing dataset ''' \n",
    "y_pred = automl.predict(X_test)\n",
    "print('Predicted labels', y_pred)\n",
    "print('True labels', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "r2 = 0.8503723727607084\nmse = 1977853769.4384706\nmae = 29258.487121555943\n"
     ]
    }
   ],
   "source": [
    "''' compute different metric values on testing dataset'''\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))\n",
    "print('mse', '=', sklearn_metric_loss_score('mse', y_pred, y_test))\n",
    "print('mae', '=', sklearn_metric_loss_score('mae', y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Current Learner': 'lgbm', 'Current Sample': 10000, 'Current Hyper-parameters': {'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 20.0, 'learning_rate': 0.1, 'subsample': 1.0, 'log_max_bin': 8, 'colsample_bytree': 1.0, 'reg_alpha': 1e-10, 'reg_lambda': 1.0, 'FLAML_sample_size': 10000}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 20.0, 'learning_rate': 0.1, 'subsample': 1.0, 'log_max_bin': 8, 'colsample_bytree': 1.0, 'reg_alpha': 1e-10, 'reg_lambda': 1.0, 'FLAML_sample_size': 10000}}\n{'Current Learner': 'lgbm', 'Current Sample': 10000, 'Current Hyper-parameters': {'n_estimators': 4.0, 'max_leaves': 4.0, 'min_child_weight': 20.0, 'learning_rate': 0.46335414315327306, 'subsample': 0.9339389930838808, 'log_max_bin': 10.0, 'colsample_bytree': 0.9904286645657556, 'reg_alpha': 2.841147337412889e-10, 'reg_lambda': 0.12000833497054482, 'FLAML_sample_size': 10000}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 4.0, 'max_leaves': 4.0, 'min_child_weight': 20.0, 'learning_rate': 0.46335414315327306, 'subsample': 0.9339389930838808, 'log_max_bin': 10.0, 'colsample_bytree': 0.9904286645657556, 'reg_alpha': 2.841147337412889e-10, 'reg_lambda': 0.12000833497054482, 'FLAML_sample_size': 10000}}\n{'Current Learner': 'lgbm', 'Current Sample': 10000, 'Current Hyper-parameters': {'n_estimators': 23.0, 'max_leaves': 4.0, 'min_child_weight': 20.0, 'learning_rate': 1.0, 'subsample': 0.9917683183663918, 'log_max_bin': 10.0, 'colsample_bytree': 0.9858892907525497, 'reg_alpha': 3.8783982645515837e-10, 'reg_lambda': 0.36607431863072826, 'FLAML_sample_size': 10000}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 23.0, 'max_leaves': 4.0, 'min_child_weight': 20.0, 'learning_rate': 1.0, 'subsample': 0.9917683183663918, 'log_max_bin': 10.0, 'colsample_bytree': 0.9858892907525497, 'reg_alpha': 3.8783982645515837e-10, 'reg_lambda': 0.36607431863072826, 'FLAML_sample_size': 10000}}\n{'Current Learner': 'lgbm', 'Current Sample': 10000, 'Current Hyper-parameters': {'n_estimators': 11.0, 'max_leaves': 17.0, 'min_child_weight': 14.947587304572773, 'learning_rate': 0.6092558236172073, 'subsample': 0.9659256891661986, 'log_max_bin': 10.0, 'colsample_bytree': 1.0, 'reg_alpha': 3.816590663384559e-08, 'reg_lambda': 0.4482946615262561, 'FLAML_sample_size': 10000}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 11.0, 'max_leaves': 17.0, 'min_child_weight': 14.947587304572773, 'learning_rate': 0.6092558236172073, 'subsample': 0.9659256891661986, 'log_max_bin': 10.0, 'colsample_bytree': 1.0, 'reg_alpha': 3.816590663384559e-08, 'reg_lambda': 0.4482946615262561, 'FLAML_sample_size': 10000}}\n{'Current Learner': 'lgbm', 'Current Sample': 10000, 'Current Hyper-parameters': {'n_estimators': 6.0, 'max_leaves': 4.0, 'min_child_weight': 2.776007506782275, 'learning_rate': 0.7179196339383696, 'subsample': 0.8746997476758036, 'log_max_bin': 9.0, 'colsample_bytree': 1.0, 'reg_alpha': 9.69511928836042e-10, 'reg_lambda': 0.17744769739709204, 'FLAML_sample_size': 10000}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 6.0, 'max_leaves': 4.0, 'min_child_weight': 2.776007506782275, 'learning_rate': 0.7179196339383696, 'subsample': 0.8746997476758036, 'log_max_bin': 9.0, 'colsample_bytree': 1.0, 'reg_alpha': 9.69511928836042e-10, 'reg_lambda': 0.17744769739709204, 'FLAML_sample_size': 10000}}\n{'Current Learner': 'lgbm', 'Current Sample': 364083, 'Current Hyper-parameters': {'n_estimators': 4.0, 'max_leaves': 12.0, 'min_child_weight': 1.0757027869964557, 'learning_rate': 0.450276708780562, 'subsample': 0.9006357480496961, 'log_max_bin': 10.0, 'colsample_bytree': 1.0, 'reg_alpha': 1e-10, 'reg_lambda': 1.0, 'FLAML_sample_size': 364083}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 4.0, 'max_leaves': 12.0, 'min_child_weight': 1.0757027869964557, 'learning_rate': 0.450276708780562, 'subsample': 0.9006357480496961, 'log_max_bin': 10.0, 'colsample_bytree': 1.0, 'reg_alpha': 1e-10, 'reg_lambda': 1.0, 'FLAML_sample_size': 364083}}\n{'Current Learner': 'lgbm', 'Current Sample': 364083, 'Current Hyper-parameters': {'n_estimators': 12.0, 'max_leaves': 21.0, 'min_child_weight': 1.4188300323104601, 'learning_rate': 0.21434585687003363, 'subsample': 0.96058565726185, 'log_max_bin': 9.0, 'colsample_bytree': 1.0, 'reg_alpha': 3.209664512322882e-10, 'reg_lambda': 0.8927146483558472, 'FLAML_sample_size': 364083}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 12.0, 'max_leaves': 21.0, 'min_child_weight': 1.4188300323104601, 'learning_rate': 0.21434585687003363, 'subsample': 0.96058565726185, 'log_max_bin': 9.0, 'colsample_bytree': 1.0, 'reg_alpha': 3.209664512322882e-10, 'reg_lambda': 0.8927146483558472, 'FLAML_sample_size': 364083}}\n{'Current Learner': 'lgbm', 'Current Sample': 364083, 'Current Hyper-parameters': {'n_estimators': 4.0, 'max_leaves': 33.0, 'min_child_weight': 2.124277220000237, 'learning_rate': 0.6104117418606608, 'subsample': 0.8953865349615586, 'log_max_bin': 10.0, 'colsample_bytree': 0.9549508919646192, 'reg_alpha': 1.775430936881156e-08, 'reg_lambda': 1.0, 'FLAML_sample_size': 364083}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 4.0, 'max_leaves': 33.0, 'min_child_weight': 2.124277220000237, 'learning_rate': 0.6104117418606608, 'subsample': 0.8953865349615586, 'log_max_bin': 10.0, 'colsample_bytree': 0.9549508919646192, 'reg_alpha': 1.775430936881156e-08, 'reg_lambda': 1.0, 'FLAML_sample_size': 364083}}\n{'Current Learner': 'lgbm', 'Current Sample': 364083, 'Current Hyper-parameters': {'n_estimators': 14.0, 'max_leaves': 207.0, 'min_child_weight': 5.096453108920863, 'learning_rate': 0.31631252713344904, 'subsample': 0.9433718793194579, 'log_max_bin': 10.0, 'colsample_bytree': 0.9950430045845675, 'reg_alpha': 1e-10, 'reg_lambda': 1.0, 'FLAML_sample_size': 364083}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 14.0, 'max_leaves': 207.0, 'min_child_weight': 5.096453108920863, 'learning_rate': 0.31631252713344904, 'subsample': 0.9433718793194579, 'log_max_bin': 10.0, 'colsample_bytree': 0.9950430045845675, 'reg_alpha': 1e-10, 'reg_lambda': 1.0, 'FLAML_sample_size': 364083}}\n{'Current Learner': 'lgbm', 'Current Sample': 364083, 'Current Hyper-parameters': {'n_estimators': 31.0, 'max_leaves': 809.0, 'min_child_weight': 18.922277448271377, 'learning_rate': 0.0572324550115096, 'subsample': 0.9695636894945154, 'log_max_bin': 10.0, 'colsample_bytree': 0.9817199261897989, 'reg_alpha': 1.112259151174658e-10, 'reg_lambda': 1.0, 'FLAML_sample_size': 364083}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 31.0, 'max_leaves': 809.0, 'min_child_weight': 18.922277448271377, 'learning_rate': 0.0572324550115096, 'subsample': 0.9695636894945154, 'log_max_bin': 10.0, 'colsample_bytree': 0.9817199261897989, 'reg_alpha': 1.112259151174658e-10, 'reg_lambda': 1.0, 'FLAML_sample_size': 364083}}\n{'Current Learner': 'lgbm', 'Current Sample': 364083, 'Current Hyper-parameters': {'n_estimators': 62.0, 'max_leaves': 721.0, 'min_child_weight': 20.0, 'learning_rate': 0.11571885450381766, 'subsample': 1.0, 'log_max_bin': 8.0, 'colsample_bytree': 0.9806374300599766, 'reg_alpha': 1e-10, 'reg_lambda': 0.20115461099818857, 'FLAML_sample_size': 364083}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 62.0, 'max_leaves': 721.0, 'min_child_weight': 20.0, 'learning_rate': 0.11571885450381766, 'subsample': 1.0, 'log_max_bin': 8.0, 'colsample_bytree': 0.9806374300599766, 'reg_alpha': 1e-10, 'reg_lambda': 0.20115461099818857, 'FLAML_sample_size': 364083}}\n"
     ]
    }
   ],
   "source": [
    "from flaml.data import get_output_from_log\n",
    "time_history, best_valid_loss_history, valid_loss_history, config_history, train_loss_history = \\\n",
    "    get_output_from_log(filename = settings['log_file_name'], time_budget = 60)\n",
    "\n",
    "for config in config_history:\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'time_history' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c627b919b8ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Wall Clock Time (s)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Validation r2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_valid_loss_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time_history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Wall Clock Time (s)')\n",
    "plt.ylabel('Validation r2')\n",
    "plt.scatter(time_history, 1-np.array(valid_loss_history))\n",
    "plt.step(time_history, 1-np.array(best_valid_loss_history), where='post')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## 3. Comparison with alternatives\n",
    "\n",
    "### FLAML's accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "flaml r2 = 0.8503723727607084\n"
     ]
    }
   ],
   "source": [
    "print('flaml r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[flaml.automl: 02-22 10:47:51] {839} INFO - Evaluation method: cv\n",
      "[flaml.automl: 02-22 10:47:51] {568} INFO - Using RepeatedKFold\n",
      "[flaml.automl: 02-22 10:47:51] {860} INFO - Minimizing error metric: 1-r2\n",
      "[flaml.automl: 02-22 10:47:51] {880} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
      "[flaml.automl: 02-22 10:47:51] {939} INFO - iteration 0  current learner lgbm\n",
      "[flaml.searcher.suggestion: 02-22 10:47:51] {518} WARNING - You passed a `space` parameter to <class 'flaml.searcher.suggestion.OptunaSearch'> that contained unresolved search space definitions. <class 'flaml.searcher.suggestion.OptunaSearch'> should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "[flaml.searcher.suggestion: 02-22 10:47:51] {655} WARNING - Optuna does not support both quantization and sampling from LogUniform. Dropped quantization.\n",
      "[flaml.searcher.suggestion: 02-22 10:47:51] {655} WARNING - Optuna does not support both quantization and sampling from LogUniform. Dropped quantization.\n",
      "[flaml.searcher.suggestion: 02-22 10:47:51] {655} WARNING - Optuna does not support both quantization and sampling from LogUniform. Dropped quantization.\n",
      "\u001b[32m[I 2021-02-22 10:47:51,645]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.automl: 02-22 10:47:52] {1093} INFO -  at 0.6s,\tbest lgbm's error=0.7383,\tbest lgbm's error=0.7383\n",
      "[flaml.automl: 02-22 10:47:52] {939} INFO - iteration 1  current learner lgbm\n",
      "[flaml.automl: 02-22 10:53:03] {1093} INFO -  at 312.3s,\tbest lgbm's error=0.4677,\tbest lgbm's error=0.4677\n",
      "[flaml.automl: 02-22 10:53:03] {1133} INFO - selected model: LGBMRegressor(colsample_bytree=0.8144790108933789,\n",
      "              learning_rate=0.22856133480502477, max_bin=7,\n",
      "              min_child_weight=18.35626377882768, n_estimators=1534,\n",
      "              num_leaves=619, objective='regression',\n",
      "              reg_alpha=0.074378081349901, reg_lambda=3.073731299746213e-10,\n",
      "              subsample=0.9462591184526863)\n",
      "[flaml.automl: 02-22 10:53:03] {894} INFO - fit succeeded\n",
      "flaml r2 using optuna as the hpo method = 0.49550426822530824\n"
     ]
    }
   ],
   "source": [
    "settings = {\n",
    "    \"time_budget\": 300, # total running time in seconds\n",
    "    \"metric\": 'r2', \n",
    "    \"task\": 'regression', # task type    \n",
    "    \"estimator_list\": ['lgbm'],\n",
    "    \"log_file_name\": 'houses_experiment_optuna.log', \n",
    "    \"hpo_method\": 'optuna',\n",
    "}\n",
    "automl.fit(X_train = X_train, y_train = y_train, **settings)\n",
    "y_pred = automl.predict(X_test)\n",
    "print('flaml r2 using optuna as the hpo method', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))"
   ]
  },
  {
   "source": [
    "### Default LightGBM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "lgbm = LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "default lgbm r2 = 0.8296179648694404\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgbm.predict(X_test)\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('default lgbm r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))"
   ]
  },
  {
   "source": [
    "### Optuna LightGBM Tuner"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, val_x, train_y, val_y = train_test_split(X_train, y_train, test_size=0.1)\n",
    "import optuna.integration.lightgbm as lgb\n",
    "dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "dval = lgb.Dataset(val_x, label=val_y)\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"regression\",\n",
    "    \"verbosity\": -1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "6 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 2183189353.109814.\u001b[0m\n",
      "feature_fraction, val_score: 2183189353.109814:  43%|####2     | 3/7 [00:05<00:07,  1.94s/it]\u001b[32m[I 2021-02-22 10:56:33,312]\u001b[0m Trial 2 finished with value: 2344622924.705835 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 2183189353.109814.\u001b[0m\n",
      "feature_fraction, val_score: 2183189353.109814:  57%|#####7    | 4/7 [00:07<00:05,  1.95s/it]\u001b[32m[I 2021-02-22 10:56:35,270]\u001b[0m Trial 3 finished with value: 2202003393.4830375 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 2183189353.109814.\u001b[0m\n",
      "feature_fraction, val_score: 2183189353.109814:  71%|#######1  | 5/7 [00:09<00:03,  1.96s/it]\u001b[32m[I 2021-02-22 10:56:37,258]\u001b[0m Trial 4 finished with value: 2202003393.4830375 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 2183189353.109814.\u001b[0m\n",
      "feature_fraction, val_score: 2183189353.109814:  86%|########5 | 6/7 [00:11<00:01,  1.99s/it]\u001b[32m[I 2021-02-22 10:56:39,316]\u001b[0m Trial 5 finished with value: 2254538205.168261 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 2183189353.109814.\u001b[0m\n",
      "feature_fraction, val_score: 2183189353.109814: 100%|##########| 7/7 [00:14<00:00,  2.06s/it]\u001b[32m[I 2021-02-22 10:56:41,510]\u001b[0m Trial 6 finished with value: 2283558009.6360044 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 2183189353.109814.\u001b[0m\n",
      "feature_fraction, val_score: 2183189353.109814: 100%|##########| 7/7 [00:14<00:00,  2.00s/it]\n",
      "num_leaves, val_score: 2183189353.109814:   5%|5         | 1/20 [00:06<01:54,  6.01s/it]\u001b[32m[I 2021-02-22 10:56:47,536]\u001b[0m Trial 7 finished with value: 2302836972.3778815 and parameters: {'num_leaves': 180}. Best is trial 7 with value: 2302836972.3778815.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  10%|#         | 2/20 [00:09<01:25,  4.77s/it]\u001b[32m[I 2021-02-22 10:56:51,439]\u001b[0m Trial 8 finished with value: 2286689919.4862747 and parameters: {'num_leaves': 116}. Best is trial 8 with value: 2286689919.4862747.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  15%|#5        | 3/20 [00:17<01:43,  6.06s/it]\u001b[32m[I 2021-02-22 10:56:59,039]\u001b[0m Trial 9 finished with value: 2353650653.1081195 and parameters: {'num_leaves': 242}. Best is trial 8 with value: 2286689919.4862747.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  20%|##        | 4/20 [00:18<01:05,  4.07s/it]\u001b[32m[I 2021-02-22 10:57:00,039]\u001b[0m Trial 10 finished with value: 2198682837.6066217 and parameters: {'num_leaves': 20}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  25%|##5       | 5/20 [00:19<00:43,  2.89s/it]\u001b[32m[I 2021-02-22 10:57:00,835]\u001b[0m Trial 11 finished with value: 2247879185.1119595 and parameters: {'num_leaves': 12}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  30%|###       | 6/20 [00:19<00:28,  2.07s/it]\u001b[32m[I 2021-02-22 10:57:01,316]\u001b[0m Trial 12 finished with value: 3639417483.4666734 and parameters: {'num_leaves': 2}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  35%|###5      | 7/20 [00:22<00:29,  2.29s/it]\u001b[32m[I 2021-02-22 10:57:04,074]\u001b[0m Trial 13 finished with value: 2279800953.937247 and parameters: {'num_leaves': 69}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  40%|####      | 8/20 [00:25<00:28,  2.35s/it]\u001b[32m[I 2021-02-22 10:57:06,542]\u001b[0m Trial 14 finished with value: 2212164565.6298757 and parameters: {'num_leaves': 64}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  45%|####5     | 9/20 [00:30<00:36,  3.32s/it]\u001b[32m[I 2021-02-22 10:57:11,987]\u001b[0m Trial 15 finished with value: 2320714121.8927603 and parameters: {'num_leaves': 176}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  50%|#####     | 10/20 [00:32<00:30,  3.01s/it]\u001b[32m[I 2021-02-22 10:57:14,298]\u001b[0m Trial 16 finished with value: 2226809231.5384884 and parameters: {'num_leaves': 49}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  55%|#####5    | 11/20 [00:39<00:36,  4.03s/it]\u001b[32m[I 2021-02-22 10:57:20,652]\u001b[0m Trial 17 finished with value: 2292422426.5527515 and parameters: {'num_leaves': 129}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  60%|######    | 12/20 [00:47<00:43,  5.45s/it]\u001b[32m[I 2021-02-22 10:57:29,353]\u001b[0m Trial 18 finished with value: 2360700891.2187963 and parameters: {'num_leaves': 255}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  65%|######5   | 13/20 [00:49<00:29,  4.21s/it]\u001b[32m[I 2021-02-22 10:57:30,716]\u001b[0m Trial 19 finished with value: 2225930719.507271 and parameters: {'num_leaves': 26}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  70%|#######   | 14/20 [00:52<00:24,  4.04s/it]\u001b[32m[I 2021-02-22 10:57:34,365]\u001b[0m Trial 20 finished with value: 2261106911.9732027 and parameters: {'num_leaves': 104}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  75%|#######5  | 15/20 [00:57<00:21,  4.30s/it]\u001b[32m[I 2021-02-22 10:57:39,265]\u001b[0m Trial 21 finished with value: 2308541066.930408 and parameters: {'num_leaves': 163}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  80%|########  | 16/20 [01:04<00:19,  4.95s/it]\u001b[32m[I 2021-02-22 10:57:45,714]\u001b[0m Trial 22 finished with value: 2357366463.0670652 and parameters: {'num_leaves': 215}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  85%|########5 | 17/20 [01:07<00:13,  4.49s/it]\u001b[32m[I 2021-02-22 10:57:49,138]\u001b[0m Trial 23 finished with value: 2289284994.882068 and parameters: {'num_leaves': 94}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  90%|######### | 18/20 [01:09<00:07,  3.67s/it]\u001b[32m[I 2021-02-22 10:57:50,891]\u001b[0m Trial 24 finished with value: 2225842428.3816347 and parameters: {'num_leaves': 38}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814:  95%|#########5| 19/20 [01:14<00:04,  4.19s/it]\u001b[32m[I 2021-02-22 10:57:56,310]\u001b[0m Trial 25 finished with value: 2344632423.478032 and parameters: {'num_leaves': 147}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814: 100%|##########| 20/20 [01:18<00:00,  3.98s/it]\u001b[32m[I 2021-02-22 10:57:59,791]\u001b[0m Trial 26 finished with value: 2252334359.094741 and parameters: {'num_leaves': 85}. Best is trial 10 with value: 2198682837.6066217.\u001b[0m\n",
      "num_leaves, val_score: 2183189353.109814: 100%|##########| 20/20 [01:18<00:00,  3.91s/it]\n",
      "bagging, val_score: 2183189353.109814:  10%|#         | 1/10 [00:01<00:15,  1.68s/it]\u001b[32m[I 2021-02-22 10:58:01,479]\u001b[0m Trial 27 finished with value: 2349163161.5755215 and parameters: {'bagging_fraction': 0.46266079860350695, 'bagging_freq': 3}. Best is trial 27 with value: 2349163161.5755215.\u001b[0m\n",
      "bagging, val_score: 2176143553.992680:  20%|##        | 2/10 [00:03<00:12,  1.62s/it]\u001b[32m[I 2021-02-22 10:58:03,050]\u001b[0m Trial 28 finished with value: 2176143553.99268 and parameters: {'bagging_fraction': 0.9850812317994322, 'bagging_freq': 7}. Best is trial 28 with value: 2176143553.99268.\u001b[0m\n",
      "bagging, val_score: 2176143553.992680:  30%|###       | 3/10 [00:04<00:11,  1.62s/it]\u001b[32m[I 2021-02-22 10:58:04,679]\u001b[0m Trial 29 finished with value: 2176143553.99268 and parameters: {'bagging_fraction': 0.985570027305506, 'bagging_freq': 7}. Best is trial 28 with value: 2176143553.99268.\u001b[0m\n",
      "bagging, val_score: 2148826667.484390:  40%|####      | 4/10 [00:06<00:09,  1.66s/it]\u001b[32m[I 2021-02-22 10:58:06,406]\u001b[0m Trial 30 finished with value: 2148826667.48439 and parameters: {'bagging_fraction': 0.9837594961057172, 'bagging_freq': 7}. Best is trial 30 with value: 2148826667.48439.\u001b[0m\n",
      "bagging, val_score: 2120154666.907345:  50%|#####     | 5/10 [00:08<00:08,  1.71s/it]\u001b[32m[I 2021-02-22 10:58:08,211]\u001b[0m Trial 31 finished with value: 2120154666.9073446 and parameters: {'bagging_fraction': 0.9985083325888207, 'bagging_freq': 7}. Best is trial 31 with value: 2120154666.9073446.\u001b[0m\n",
      "bagging, val_score: 2120154666.907345:  60%|######    | 6/10 [00:10<00:06,  1.67s/it]\u001b[32m[I 2021-02-22 10:58:09,808]\u001b[0m Trial 32 finished with value: 2212141269.627054 and parameters: {'bagging_fraction': 0.9844685267582313, 'bagging_freq': 7}. Best is trial 31 with value: 2120154666.9073446.\u001b[0m\n",
      "bagging, val_score: 2120154666.907345:  70%|#######   | 7/10 [00:12<00:05,  1.79s/it]\u001b[32m[I 2021-02-22 10:58:11,834]\u001b[0m Trial 33 finished with value: 2168033356.24624 and parameters: {'bagging_fraction': 0.9942472610281351, 'bagging_freq': 7}. Best is trial 31 with value: 2120154666.9073446.\u001b[0m\n",
      "bagging, val_score: 2120154666.907345:  80%|########  | 8/10 [00:14<00:03,  1.94s/it]\u001b[32m[I 2021-02-22 10:58:14,103]\u001b[0m Trial 34 finished with value: 2198756960.5062327 and parameters: {'bagging_fraction': 0.9878942152170641, 'bagging_freq': 7}. Best is trial 31 with value: 2120154666.9073446.\u001b[0m\n",
      "bagging, val_score: 2120154666.907345:  90%|######### | 9/10 [00:16<00:01,  1.92s/it]\u001b[32m[I 2021-02-22 10:58:15,972]\u001b[0m Trial 35 finished with value: 2167138660.2695484 and parameters: {'bagging_fraction': 0.9909148097028619, 'bagging_freq': 7}. Best is trial 31 with value: 2120154666.9073446.\u001b[0m\n",
      "bagging, val_score: 2120154666.907345: 100%|##########| 10/10 [00:18<00:00,  2.10s/it]\u001b[32m[I 2021-02-22 10:58:18,481]\u001b[0m Trial 36 finished with value: 2267716035.187311 and parameters: {'bagging_fraction': 0.7884630053038221, 'bagging_freq': 5}. Best is trial 31 with value: 2120154666.9073446.\u001b[0m\n",
      "bagging, val_score: 2120154666.907345: 100%|##########| 10/10 [00:18<00:00,  1.87s/it]\n",
      "feature_fraction_stage2, val_score: 2120154666.907345:  17%|#6        | 1/6 [00:02<00:11,  2.36s/it]\u001b[32m[I 2021-02-22 10:58:20,858]\u001b[0m Trial 37 finished with value: 2120154666.9073446 and parameters: {'feature_fraction': 0.6799999999999999}. Best is trial 37 with value: 2120154666.9073446.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2120154666.907345:  33%|###3      | 2/6 [00:04<00:09,  2.29s/it]\u001b[32m[I 2021-02-22 10:58:23,103]\u001b[0m Trial 38 finished with value: 2120154666.9073446 and parameters: {'feature_fraction': 0.616}. Best is trial 37 with value: 2120154666.9073446.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2120154666.907345:  50%|#####     | 3/6 [00:06<00:06,  2.24s/it]\u001b[32m[I 2021-02-22 10:58:25,279]\u001b[0m Trial 39 finished with value: 2340044642.568378 and parameters: {'feature_fraction': 0.52}. Best is trial 37 with value: 2120154666.9073446.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2120154666.907345:  67%|######6   | 4/6 [00:08<00:04,  2.09s/it]\u001b[32m[I 2021-02-22 10:58:27,146]\u001b[0m Trial 40 finished with value: 2120154666.9073446 and parameters: {'feature_fraction': 0.584}. Best is trial 37 with value: 2120154666.9073446.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2120154666.907345:  83%|########3 | 5/6 [00:10<00:01,  1.96s/it]\u001b[32m[I 2021-02-22 10:58:28,879]\u001b[0m Trial 41 finished with value: 2120154666.9073446 and parameters: {'feature_fraction': 0.6479999999999999}. Best is trial 37 with value: 2120154666.9073446.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2120154666.907345: 100%|##########| 6/6 [00:12<00:00,  1.94s/it]\u001b[32m[I 2021-02-22 10:58:30,761]\u001b[0m Trial 42 finished with value: 2340044642.568378 and parameters: {'feature_fraction': 0.552}. Best is trial 37 with value: 2120154666.9073446.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2120154666.907345: 100%|##########| 6/6 [00:12<00:00,  2.05s/it]\n",
      "regularization_factors, val_score: 2120154666.700054:   5%|5         | 1/20 [00:01<00:30,  1.60s/it]\u001b[32m[I 2021-02-22 10:58:32,374]\u001b[0m Trial 43 finished with value: 2120154666.700054 and parameters: {'lambda_l1': 0.00042359126591045215, 'lambda_l2': 1.9103560738649045e-08}. Best is trial 43 with value: 2120154666.700054.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.700054:  10%|#         | 2/20 [00:03<00:29,  1.66s/it]\u001b[32m[I 2021-02-22 10:58:34,074]\u001b[0m Trial 44 finished with value: 2120154666.773498 and parameters: {'lambda_l1': 0.00046482003448174695, 'lambda_l2': 1.0782185457127057e-08}. Best is trial 43 with value: 2120154666.700054.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.700054:  15%|#5        | 3/20 [00:04<00:28,  1.67s/it]\u001b[32m[I 2021-02-22 10:58:35,767]\u001b[0m Trial 45 finished with value: 2120154666.767029 and parameters: {'lambda_l1': 0.0006423168319010262, 'lambda_l2': 1.4571006320945827e-08}. Best is trial 43 with value: 2120154666.700054.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.552776:  20%|##        | 4/20 [00:06<00:28,  1.78s/it]\u001b[32m[I 2021-02-22 10:58:37,694]\u001b[0m Trial 46 finished with value: 2120154666.5527756 and parameters: {'lambda_l1': 0.00047435606858295686, 'lambda_l2': 1.1660177462064854e-08}. Best is trial 46 with value: 2120154666.5527756.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.539871:  25%|##5       | 5/20 [00:08<00:26,  1.75s/it]\u001b[32m[I 2021-02-22 10:58:39,404]\u001b[0m Trial 47 finished with value: 2120154666.539871 and parameters: {'lambda_l1': 0.0005382826426699056, 'lambda_l2': 1.045996311009788e-08}. Best is trial 47 with value: 2120154666.539871.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.539871:  30%|###       | 6/20 [00:10<00:26,  1.89s/it]\u001b[32m[I 2021-02-22 10:58:41,576]\u001b[0m Trial 48 finished with value: 2120154666.5538645 and parameters: {'lambda_l1': 0.0005174588825492348, 'lambda_l2': 1.2260761282003162e-08}. Best is trial 47 with value: 2120154666.539871.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.539871:  35%|###5      | 7/20 [00:12<00:23,  1.79s/it]\u001b[32m[I 2021-02-22 10:58:43,158]\u001b[0m Trial 49 finished with value: 2120154666.5550237 and parameters: {'lambda_l1': 0.0006088192013425502, 'lambda_l2': 1.0264740697066586e-08}. Best is trial 47 with value: 2120154666.539871.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.539871:  40%|####      | 8/20 [00:14<00:24,  2.02s/it]\u001b[32m[I 2021-02-22 10:58:45,657]\u001b[0m Trial 50 finished with value: 2120154666.5910764 and parameters: {'lambda_l1': 0.0005147027914185771, 'lambda_l2': 1.061235724811096e-08}. Best is trial 47 with value: 2120154666.539871.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.539871:  45%|####5     | 9/20 [00:17<00:22,  2.07s/it]\u001b[32m[I 2021-02-22 10:58:47,849]\u001b[0m Trial 51 finished with value: 2120154666.636832 and parameters: {'lambda_l1': 0.00048672625785424375, 'lambda_l2': 1.2947935844931915e-08}. Best is trial 47 with value: 2120154666.539871.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.539871:  50%|#####     | 10/20 [00:18<00:19,  1.96s/it]\u001b[32m[I 2021-02-22 10:58:49,561]\u001b[0m Trial 52 finished with value: 2120154666.7190447 and parameters: {'lambda_l1': 0.0006463432766071972, 'lambda_l2': 1.3923615642103218e-08}. Best is trial 47 with value: 2120154666.539871.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.539871:  55%|#####5    | 11/20 [00:21<00:18,  2.05s/it]\u001b[32m[I 2021-02-22 10:58:51,817]\u001b[0m Trial 53 finished with value: 2120154666.5919886 and parameters: {'lambda_l1': 0.0010730822555809888, 'lambda_l2': 1.1351114472222415e-08}. Best is trial 47 with value: 2120154666.539871.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.539871:  60%|######    | 12/20 [00:24<00:20,  2.57s/it]\u001b[32m[I 2021-02-22 10:58:55,580]\u001b[0m Trial 54 finished with value: 2120154666.620593 and parameters: {'lambda_l1': 0.0007922817809428608, 'lambda_l2': 1.563371551776046e-08}. Best is trial 47 with value: 2120154666.539871.\u001b[0m\n",
      "regularization_factors, val_score: 2120154666.539871:  65%|######5   | 13/20 [00:27<00:18,  2.58s/it]\u001b[32m[I 2021-02-22 10:58:58,184]\u001b[0m Trial 55 finished with value: 2187619949.583114 and parameters: {'lambda_l1': 0.027809062393431852, 'lambda_l2': 6.060187304750044}. Best is trial 47 with value: 2120154666.539871.\u001b[0m\n",
      "regularization_factors, val_score: 2120154662.893443:  70%|#######   | 14/20 [00:29<00:14,  2.42s/it]\u001b[32m[I 2021-02-22 10:59:00,223]\u001b[0m Trial 56 finished with value: 2120154662.893443 and parameters: {'lambda_l1': 3.2593829505881596e-06, 'lambda_l2': 1.2657751809928526e-06}. Best is trial 56 with value: 2120154662.893443.\u001b[0m\n",
      "regularization_factors, val_score: 2120154655.016630:  75%|#######5  | 15/20 [00:31<00:11,  2.20s/it]\u001b[32m[I 2021-02-22 10:59:01,925]\u001b[0m Trial 57 finished with value: 2120154655.01663 and parameters: {'lambda_l1': 1.1345563451358894e-07, 'lambda_l2': 3.727125441302677e-06}. Best is trial 57 with value: 2120154655.01663.\u001b[0m\n",
      "regularization_factors, val_score: 2120154648.722831:  80%|########  | 16/20 [00:32<00:08,  2.05s/it]\u001b[32m[I 2021-02-22 10:59:03,615]\u001b[0m Trial 58 finished with value: 2120154648.7228308 and parameters: {'lambda_l1': 3.765416325952086e-08, 'lambda_l2': 5.498857786443967e-06}. Best is trial 58 with value: 2120154648.7228308.\u001b[0m\n",
      "regularization_factors, val_score: 2120154648.722831:  85%|########5 | 17/20 [00:35<00:06,  2.13s/it]\u001b[32m[I 2021-02-22 10:59:05,923]\u001b[0m Trial 59 finished with value: 2120154652.345538 and parameters: {'lambda_l1': 3.8241508340472736e-08, 'lambda_l2': 4.544160670837293e-06}. Best is trial 58 with value: 2120154648.7228308.\u001b[0m\n",
      "regularization_factors, val_score: 2120154631.334312:  90%|######### | 18/20 [00:37<00:04,  2.08s/it]\u001b[32m[I 2021-02-22 10:59:07,894]\u001b[0m Trial 60 finished with value: 2120154631.3343122 and parameters: {'lambda_l1': 3.361927540107397e-08, 'lambda_l2': 1.1033389452454068e-05}. Best is trial 60 with value: 2120154631.3343122.\u001b[0m\n",
      "regularization_factors, val_score: 2120154631.334312:  95%|#########5| 19/20 [00:38<00:01,  1.95s/it]\u001b[32m[I 2021-02-22 10:59:09,541]\u001b[0m Trial 61 finished with value: 2120154646.6590598 and parameters: {'lambda_l1': 3.3218520687364445e-08, 'lambda_l2': 6.090438979616111e-06}. Best is trial 60 with value: 2120154631.3343122.\u001b[0m\n",
      "regularization_factors, val_score: 2120154631.334312: 100%|##########| 20/20 [00:41<00:00,  2.06s/it]\u001b[32m[I 2021-02-22 10:59:11,870]\u001b[0m Trial 62 finished with value: 2120154649.875132 and parameters: {'lambda_l1': 2.6251909485232455e-08, 'lambda_l2': 5.195407534494352e-06}. Best is trial 60 with value: 2120154631.3343122.\u001b[0m\n",
      "regularization_factors, val_score: 2120154631.334312: 100%|##########| 20/20 [00:41<00:00,  2.06s/it]\n",
      "min_data_in_leaf, val_score: 2120154631.334312:  20%|##        | 1/5 [00:01<00:06,  1.70s/it]\u001b[32m[I 2021-02-22 10:59:13,580]\u001b[0m Trial 63 finished with value: 2131337792.5349934 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 2131337792.5349934.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2120154631.334312:  40%|####      | 2/5 [00:03<00:05,  1.70s/it]\u001b[32m[I 2021-02-22 10:59:15,290]\u001b[0m Trial 64 finished with value: 2216383690.6463246 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 2131337792.5349934.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2120154631.334312:  60%|######    | 3/5 [00:05<00:03,  1.85s/it]\u001b[32m[I 2021-02-22 10:59:17,316]\u001b[0m Trial 65 finished with value: 2202248212.189305 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 2131337792.5349934.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2120154631.334312:  80%|########  | 4/5 [00:08<00:02,  2.22s/it]\u001b[32m[I 2021-02-22 10:59:20,093]\u001b[0m Trial 66 finished with value: 2207331556.4829025 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 2131337792.5349934.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2120154631.334312: 100%|##########| 5/5 [00:10<00:00,  2.18s/it]\u001b[32m[I 2021-02-22 10:59:22,211]\u001b[0m Trial 67 finished with value: 2205756723.4765463 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 2131337792.5349934.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2120154631.334312: 100%|##########| 5/5 [00:10<00:00,  2.07s/it]Wall time: 2min 54s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], verbose_eval=10000)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optuna LightGBM Tuner r2 = 0.8451504635149211\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('Optuna LightGBM Tuner r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('flaml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "bfcd9a6a9254a5e160761a1fd7a9e444f011592c6770d9f4180dde058a9df5dd"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}