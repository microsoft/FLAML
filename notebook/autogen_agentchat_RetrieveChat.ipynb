{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Using RetrieveChat for Retrieve Augmented Code Generation and Question Answering\n",
    "\n",
    "RetrieveChat is a convesational framework for retrieve augmented code generation and question answering. In this notebook, we demonstrate how to utilize RetrieveChat to generate code and answer questions based on customized documentations that are not present in the LLM's training dataset. RetrieveChat uses the `RetrieveAssistantAgent` and `RetrieveUserProxyAgent`, which is similar to the usage of `AssistantAgent` and `UserProxyAgent` in other notebooks (e.g., [Automated Task Solving with Code Generation, Execution & Debugging](https://github.com/microsoft/FLAML/blob/main/notebook/autogen_agentchat_auto_feedback_from_code_execution.ipynb)). Essentially,`RetrieveAssistantAgent` and  `RetrieveUserProxyAgent` implements a different auto reply mechanism corresponding to the RetrieveChat prompts.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "FLAML requires `Python>=3.8`. To run this notebook example, please install flaml with the [mathchat] option.\n",
    "```bash\n",
    "pip install flaml[retrievechat]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install flaml[retrievechat]~=2.0.0rc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/FLAML/docs/reference/autogen/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-4']\n"
     ]
    }
   ],
   "source": [
    "from flaml import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\".config.local\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            \"gpt4\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4-32k-0314\",\n",
    "            \"gpt-35-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 and gpt-3.5-turbo models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct agents for RetrieveChat\n",
    "\n",
    "We start by initialzing the `RetrieveAssistantAgent` and `RetrieveUserProxyAgent`. The system message needs to be set to \"You are a helpful assistant.\" for RetrieveAssistantAgent. The detailed instructions are given in the user message. Later we will use the `RetrieveUserProxyAgent.generate_init_prompt` to combine the instructions and a math problem for an initial prompt to be sent to the LLM assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml.autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from flaml.autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "\n",
    "autogen.ChatCompletion.start_logging()\n",
    "\n",
    "# 1. create an RetrieveAssistantAgent instance named \"assistant\"\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\", \n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "# By default, the human_input_mode is \"ALWAYS\", which means the agent will ask for human input at every step. We set it to \"NEVER\" here.\n",
    "# `docs_path` is the path to the docs directory. By default, it is set to \"./docs\". Here we generated the documentations from FLAML's docstrings.\n",
    "# Navigate to the website folder and run `pydoc-markdown` and it will generate folder `reference` under `website/docs`.\n",
    "# `chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"docs_path\": \"../website/docs/reference\",\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Use RetrieveChat to help generate sample code and automatically run the code and fix errors if there is any.\n",
    "\n",
    "Problem: Which API should I use if I want to use FLAML for a classification task and I want to train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:flaml.autogen.retrieve_utils:Collection flaml-docs already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_35', 'doc_40', 'doc_14', 'doc_21', 'doc_15', 'doc_51', 'doc_44', 'doc_52', 'doc_41', 'doc_45', 'doc_13', 'doc_38', 'doc_36', 'doc_8']]\n",
      "Adding doc_id doc_35 to context.\n",
      "Adding doc_id doc_40 to context.\n",
      "Adding doc_id doc_14 to context.\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user. You should follow the following steps to answer a question:\n",
      "Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
      "a QA task.\n",
      "Step 2, you generate code or answer the question based on the intent.\n",
      "You should leverage the context provided by the user as much as possible. If you think the context is not enough, you\n",
      "can reply exactly \"UPDATE CONTEXT\" to ask the user to provide more contexts.\n",
      "For code generation, you must obey the following rules:\n",
      "You MUST NOT install any packages because all the packages needed are already installed.\n",
      "The code will be executed in IPython, you must follow the formats below to write your code:\n",
      "```python\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: How can I use FLAML to perform a classification task and use spark to do parallel training. Train 30 seconds and force cancel jobs if time limit is reached.\n",
      "\n",
      "Context is: \n",
      "new_automl = AutoML()\n",
      "new_automl.fit(X_train, y_train, starting_points=starting_points)\n",
      "```\n",
      "  \n",
      "- `seed` - int or None, default=None | The random seed for hpo.\n",
      "- `n_concurrent_trials` - [Experimental] int, default=1 | The number of\n",
      "  concurrent trials. When n_concurrent_trials > 1, flaml performes\n",
      "  [parallel tuning](../../Use-Cases/Task-Oriented-AutoML#parallel-tuning)\n",
      "  and installation of ray or spark is required: `pip install flaml[ray]`\n",
      "  or `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark.\n",
      "- `keep_search_state` - boolean, default=False | Whether to keep data needed\n",
      "  for model search after fit(). By default the state is deleted for\n",
      "  space saving.\n",
      "- `preserve_checkpoint` - boolean, default=True | Whether to preserve the saved checkpoint\n",
      "  on disk when deleting automl. By default the checkpoint is preserved.\n",
      "- `early_stop` - boolean, default=False | Whether to stop early if the\n",
      "  search is considered to converge.\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel Spark jobs if the\n",
      "  search time exceeded the time budget.\n",
      "- `append_log` - boolean, default=False | Whetehr to directly append the log\n",
      "  records to the input log file if it exists.\n",
      "- `auto_augment` - boolean, default=True | Whether to automatically\n",
      "  augment rare classes.\n",
      "- `min_sample_size` - int, default=MIN_SAMPLE_TRAIN | the minimal sample\n",
      "  size when sample=True.\n",
      "- `use_ray` - boolean or dict.\n",
      "  If boolean: default=False | Whether to use ray to run the training\n",
      "  in separate processes. This can be used to prevent OOM for large\n",
      "  datasets, but will incur more overhead in time.\n",
      "  If dict: the dict contains the keywords arguments to be passed to\n",
      "  [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).\n",
      "- `use_spark` - boolean, default=False | Whether to use spark to run the training\n",
      "  in parallel spark jobs. This can be used to accelerate training on large models\n",
      "  and large datasets, but will incur more overhead in time and thus slow down\n",
      "  training in some cases. GPU training is not supported yet when use_spark is True.\n",
      "  For Spark clusters, by default, we will launch one trial per executor. However,\n",
      "  sometimes we want to launch more trials than the number of executors (e.g., local mode).\n",
      "  In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override\n",
      "  the detected `num_executors`. The final number of concurrent trials will be the minimum\n",
      "  of `n_concurrent_trials` and `num_executors`.\n",
      "- `free_mem_ratio` - float between 0 and 1, default=0. The free memory ratio to keep during training.\n",
      "- `metric_constraints` - list, default=[] | The list of metric constraints.\n",
      "  Each element in this list is a 3-tuple, which shall be expressed\n",
      "  in the following format: the first element of the 3-tuple is the name of the\n",
      "  metric, the second element is the inequality sign chosen from \">=\" and \"<=\",\n",
      "  and the third element is the constraint value. E.g., `('val_loss', '<=', 0.1)`.\n",
      "  Note that all the metric names in metric_constraints need to be reported via\n",
      "  the metrics_to_log dictionary returned by a customized metric function.\n",
      "  The customized metric function shall be provided via the `metric` key word\n",
      "  argument of the fit() function or the automl constructor.\n",
      "  Find an example in the 4th constraint type in this [doc](../../Use-Cases/Task-Oriented-AutoML#constraint).\n",
      "  If `pred_time_limit` is provided as one of keyword arguments to fit() function or\n",
      "  the automl constructor, flaml will automatically (and under the hood)\n",
      "  add it as an additional element in the metric_constraints. Essentially 'pred_time_limit'\n",
      "  specifies a constraint about the prediction latency constraint in seconds.\n",
      "- `custom_hp` - dict, default=None | The custom search space specified by user.\n",
      "  It is a nested dict with keys being the estimator names, and values being dicts\n",
      "  per estimator search space. In the per estimator search space dict,\n",
      "  the keys are the hyperparameter names, and values are dicts of info (\"domain\",\n",
      "  \"init_value\", and \"low_cost_init_value\") about the search space associated with\n",
      "  the hyperparameter (i.e., per hyperparameter search space dict). When custom_hp\n",
      "  is provided, the built-in search space which is also a nested dict of per estimator\n",
      "  search space dict, will be updated with custom_hp. Note that during this nested dict update,\n",
      "  the per hyperparameter search space dicts will be replaced (instead of updated) by the ones\n",
      "  provided in custom_hp. Note that the value for \"domain\" can either be a constant\n",
      "  or a sample.Domain object.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "custom_hp = {\n",
      "     \"transformer_ms\": {\n",
      "         \"model_path\": {\n",
      "             \"domain\": \"albert-base-v2\",\n",
      "         },\n",
      "         \"learning_rate\": {\n",
      "             \"domain\": tune.choice([1e-4, 1e-5]),\n",
      "         }\n",
      "     }\n",
      " }\n",
      "```\n",
      "- `skip_transform` - boolean, default=False | Whether to pre-process data prior to modeling.\n",
      "- `fit_kwargs_by_estimator` - dict, default=None | The user specified keywords arguments, grouped by estimator name.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "fit_kwargs_by_estimator = {\n",
      "    \"transformer\": {\n",
      "        \"output_dir\": \"test/data/output/\",\n",
      "        \"fp16\": False,\n",
      "    }\n",
      "}\n",
      "```\n",
      "- `mlflow_logging` - boolean, default=True | Whether to log the training results to mlflow.\n",
      "  This requires mlflow to be installed and to have an active mlflow run.\n",
      "  FLAML will create nested runs.\n",
      "\n",
      "#### config\\_history\n",
      "\n",
      "```python\n",
      "@property\n",
      "def config_history() -> dict\n",
      "```\n",
      "\n",
      "A dictionary of iter->(estimator, config, time),\n",
      "storing the best estimator, config, and the time when the best\n",
      "model is updated each time.\n",
      "\n",
      "#### model\n",
      "\n",
      "```python\n",
      "@property\n",
      "def model()\n",
      "```\n",
      "\n",
      "An object with `predict()` and `predict_proba()` method (for\n",
      "classification), storing the best trained model.\n",
      "\n",
      "#### best\\_model\\_for\\_estimator\n",
      "\n",
      "```python\n",
      "def best_model_for_estimator(estimator_name: str)\n",
      "```\n",
      "\n",
      "Return the best model found for a particular estimator.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `estimator_name` - a str of the estimator's name.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  An object storing the best model for estimator_name.\n",
      "  If `model_history` was set to False during fit(), then the returned model\n",
      "  is untrained unless estimator_name is the best estimator.\n",
      "  If `model_history` was set to True, then the returned model is trained.\n",
      "\n",
      "#### best\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_estimator()\n",
      "```\n",
      "\n",
      "A string indicating the best estimator found.\n",
      "\n",
      "#### best\\_iteration\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_iteration()\n",
      "```\n",
      "\n",
      "An integer of the iteration number where the best\n",
      "config is found.\n",
      "\n",
      "#### best\\_config\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config()\n",
      "```\n",
      "\n",
      "A dictionary of the best configuration.\n",
      "\n",
      "#### best\\_config\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best configuration.\n",
      "\n",
      "#### best\\_loss\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best loss.\n",
      "\n",
      "#### best\\_loss\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss()\n",
      "```\n",
      "\n",
      "A float of the best loss found.\n",
      "\n",
      "#### best\\_result\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_result()\n",
      "```\n",
      "\n",
      "Result dictionary for model trained with the best config.\n",
      "\n",
      "#### metrics\\_for\\_best\\_config\n",
      "\n",
      "new_automl = AutoML()\n",
      "new_automl.fit(X_train, y_train, starting_points=starting_points)\n",
      "```\n",
      "  \n",
      "- `seed` - int or None, default=None | The random seed for hpo.\n",
      "- `n_concurrent_trials` - [Experimental] int, default=1 | The number of\n",
      "  concurrent trials. When n_concurrent_trials > 1, flaml performes\n",
      "  [parallel tuning](../../Use-Cases/Task-Oriented-AutoML#parallel-tuning)\n",
      "  and installation of ray or spark is required: `pip install flaml[ray]`\n",
      "  or `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark.\n",
      "- `keep_search_state` - boolean, default=False | Whether to keep data needed\n",
      "  for model search after fit(). By default the state is deleted for\n",
      "  space saving.\n",
      "- `preserve_checkpoint` - boolean, default=True | Whether to preserve the saved checkpoint\n",
      "  on disk when deleting automl. By default the checkpoint is preserved.\n",
      "- `early_stop` - boolean, default=False | Whether to stop early if the\n",
      "  search is considered to converge.\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel the PySpark job if overtime.\n",
      "- `append_log` - boolean, default=False | Whetehr to directly append the log\n",
      "  records to the input log file if it exists.\n",
      "- `auto_augment` - boolean, default=True | Whether to automatically\n",
      "  augment rare classes.\n",
      "- `min_sample_size` - int, default=MIN_SAMPLE_TRAIN | the minimal sample\n",
      "  size when sample=True.\n",
      "- `use_ray` - boolean or dict.\n",
      "  If boolean: default=False | Whether to use ray to run the training\n",
      "  in separate processes. This can be used to prevent OOM for large\n",
      "  datasets, but will incur more overhead in time.\n",
      "  If dict: the dict contains the keywords arguments to be passed to\n",
      "  [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).\n",
      "- `use_spark` - boolean, default=False | Whether to use spark to run the training\n",
      "  in parallel spark jobs. This can be used to accelerate training on large models\n",
      "  and large datasets, but will incur more overhead in time and thus slow down\n",
      "  training in some cases.\n",
      "- `free_mem_ratio` - float between 0 and 1, default=0. The free memory ratio to keep during training.\n",
      "- `metric_constraints` - list, default=[] | The list of metric constraints.\n",
      "  Each element in this list is a 3-tuple, which shall be expressed\n",
      "  in the following format: the first element of the 3-tuple is the name of the\n",
      "  metric, the second element is the inequality sign chosen from \">=\" and \"<=\",\n",
      "  and the third element is the constraint value. E.g., `('precision', '>=', 0.9)`.\n",
      "  Note that all the metric names in metric_constraints need to be reported via\n",
      "  the metrics_to_log dictionary returned by a customized metric function.\n",
      "  The customized metric function shall be provided via the `metric` key word argument\n",
      "  of the fit() function or the automl constructor.\n",
      "  Find examples in this [test](https://github.com/microsoft/FLAML/tree/main/test/automl/test_constraints.py).\n",
      "  If `pred_time_limit` is provided as one of keyword arguments to fit() function or\n",
      "  the automl constructor, flaml will automatically (and under the hood)\n",
      "  add it as an additional element in the metric_constraints. Essentially 'pred_time_limit'\n",
      "  specifies a constraint about the prediction latency constraint in seconds.\n",
      "- `custom_hp` - dict, default=None | The custom search space specified by user\n",
      "  Each key is the estimator name, each value is a dict of the custom search space for that estimator. Notice the\n",
      "  domain of the custom search space can either be a value of a sample.Domain object.\n",
      "  \n",
      "  \n",
      "  \n",
      "```python\n",
      "custom_hp = {\n",
      "    \"transformer_ms\": {\n",
      "        \"model_path\": {\n",
      "            \"domain\": \"albert-base-v2\",\n",
      "        },\n",
      "        \"learning_rate\": {\n",
      "            \"domain\": tune.choice([1e-4, 1e-5]),\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "- `time_col` - for a time series task, name of the column containing the timestamps. If not\n",
      "  provided, defaults to the first column of X_train/X_val\n",
      "  \n",
      "- `cv_score_agg_func` - customized cross-validation scores aggregate function. Default to average metrics across folds. If specificed, this function needs to\n",
      "  have the following input arguments:\n",
      "  \n",
      "  * val_loss_folds: list of floats, the loss scores of each fold;\n",
      "  * log_metrics_folds: list of dicts/floats, the metrics of each fold to log.\n",
      "  \n",
      "  This function should return the final aggregate result of all folds. A float number of the minimization objective, and a dictionary as the metrics to log or None.\n",
      "  E.g.,\n",
      "  \n",
      "```python\n",
      "def cv_score_agg_func(val_loss_folds, log_metrics_folds):\n",
      "    metric_to_minimize = sum(val_loss_folds)/len(val_loss_folds)\n",
      "    metrics_to_log = None\n",
      "    for single_fold in log_metrics_folds:\n",
      "        if metrics_to_log is None:\n",
      "            metrics_to_log = single_fold\n",
      "        elif isinstance(metrics_to_log, dict):\n",
      "            metrics_to_log = {k: metrics_to_log[k] + v for k, v in single_fold.items()}\n",
      "        else:\n",
      "            metrics_to_log += single_fold\n",
      "    if metrics_to_log:\n",
      "        n = len(val_loss_folds)\n",
      "        metrics_to_log = (\n",
      "            {k: v / n for k, v in metrics_to_log.items()}\n",
      "            if isinstance(metrics_to_log, dict)\n",
      "            else metrics_to_log / n\n",
      "        )\n",
      "    return metric_to_minimize, metrics_to_log\n",
      "```\n",
      "  \n",
      "- `skip_transform` - boolean, default=False | Whether to pre-process data prior to modeling.\n",
      "- `mlflow_logging` - boolean, default=None | Whether to log the training results to mlflow.\n",
      "  Default value is None, which means the logging decision is made based on\n",
      "  AutoML.__init__'s mlflow_logging argument.\n",
      "  This requires mlflow to be installed and to have an active mlflow run.\n",
      "  FLAML will create nested runs.\n",
      "- `fit_kwargs_by_estimator` - dict, default=None | The user specified keywords arguments, grouped by estimator name.\n",
      "  For TransformersEstimator, available fit_kwargs can be found from\n",
      "  [TrainingArgumentsForAuto](nlp/huggingface/training_args).\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "fit_kwargs_by_estimator = {\n",
      "    \"transformer\": {\n",
      "        \"output_dir\": \"test/data/output/\",\n",
      "        \"fp16\": False,\n",
      "    },\n",
      "    \"tft\": {\n",
      "        \"max_encoder_length\": 1,\n",
      "        \"min_encoder_length\": 1,\n",
      "        \"static_categoricals\": [],\n",
      "        \"static_reals\": [],\n",
      "        \"time_varying_known_categoricals\": [],\n",
      "        \"time_varying_known_reals\": [],\n",
      "        \"time_varying_unknown_categoricals\": [],\n",
      "        \"time_varying_unknown_reals\": [],\n",
      "        \"variable_groups\": {},\n",
      "        \"lags\": {},\n",
      "    }\n",
      "}\n",
      "```\n",
      "  \n",
      "- `**fit_kwargs` - Other key word arguments to pass to fit() function of\n",
      "  the searched learners, such as sample_weight. Below are a few examples of\n",
      "  estimator-specific parameters:\n",
      "- `period` - int | forecast horizon for all time series forecast tasks.\n",
      "- `gpu_per_trial` - float, default = 0 | A float of the number of gpus per trial,\n",
      "  only used by TransformersEstimator, XGBoostSklearnEstimator, and\n",
      "  TemporalFusionTransformerEstimator.\n",
      "- `group_ids` - list of strings of column names identifying a time series, only\n",
      "  used by TemporalFusionTransformerEstimator, required for\n",
      "  'ts_forecast_panel' task. `group_ids` is a parameter for TimeSeriesDataSet object\n",
      "  from PyTorchForecasting.\n",
      "  For other parameters to describe your dataset, refer to\n",
      "  [TimeSeriesDataSet PyTorchForecasting](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.data.timeseries.TimeSeriesDataSet.html).\n",
      "  To specify your variables, use `static_categoricals`, `static_reals`,\n",
      "  `time_varying_known_categoricals`, `time_varying_known_reals`,\n",
      "  `time_varying_unknown_categoricals`, `time_varying_unknown_reals`,\n",
      "  `variable_groups`. To provide more information on your data, use\n",
      "  `max_encoder_length`, `min_encoder_length`, `lags`.\n",
      "- `log_dir` - str, default = \"lightning_logs\" | Folder into which to log results\n",
      "  for tensorboard, only used by TemporalFusionTransformerEstimator.\n",
      "- `max_epochs` - int, default = 20 | Maximum number of epochs to run training,\n",
      "  only used by TemporalFusionTransformerEstimator.\n",
      "- `batch_size` - int, default = 64 | Batch size for training model, only\n",
      "  used by TemporalFusionTransformerEstimator.\n",
      "\n",
      "\n",
      "from flaml import BlendSearch\n",
      "algo = BlendSearch(metric='val_loss', mode='min',\n",
      "        space=search_space,\n",
      "        low_cost_partial_config=low_cost_partial_config)\n",
      "for i in range(10):\n",
      "    analysis = tune.run(compute_with_config,\n",
      "        search_alg=algo, use_ray=False)\n",
      "    print(analysis.trials[-1].last_result)\n",
      "```\n",
      "  \n",
      "- `verbose` - 0, 1, 2, or 3. If ray or spark backend is used, their verbosity will be\n",
      "  affected by this argument. 0 = silent, 1 = only status updates,\n",
      "  2 = status and brief trial results, 3 = status and detailed trial results.\n",
      "  Defaults to 2.\n",
      "- `local_dir` - A string of the local dir to save ray logs if ray backend is\n",
      "  used; or a local dir to save the tuning log.\n",
      "- `num_samples` - An integer of the number of configs to try. Defaults to 1.\n",
      "- `resources_per_trial` - A dictionary of the hardware resources to allocate\n",
      "  per trial, e.g., `{'cpu': 1}`. It is only valid when using ray backend\n",
      "  (by setting 'use_ray = True'). It shall be used when you need to do\n",
      "  [parallel tuning](../../Use-Cases/Tune-User-Defined-Function#parallel-tuning).\n",
      "- `config_constraints` - A list of config constraints to be satisfied.\n",
      "  e.g., ```config_constraints = [(mem_size, '<=', 1024**3)]```\n",
      "  \n",
      "  mem_size is a function which produces a float number for the bytes\n",
      "  needed for a config.\n",
      "  It is used to skip configs which do not fit in memory.\n",
      "- `metric_constraints` - A list of metric constraints to be satisfied.\n",
      "  e.g., `['precision', '>=', 0.9]`. The sign can be \">=\" or \"<=\".\n",
      "- `max_failure` - int | the maximal consecutive number of failures to sample\n",
      "  a trial before the tuning is terminated.\n",
      "- `use_ray` - A boolean of whether to use ray as the backend.\n",
      "- `use_spark` - A boolean of whether to use spark as the backend.\n",
      "- `log_file_name` - A string of the log file name. Default to None.\n",
      "  When set to None:\n",
      "  if local_dir is not given, no log file is created;\n",
      "  if local_dir is given, the log file name will be autogenerated under local_dir.\n",
      "  Only valid when verbose > 0 or use_ray is True.\n",
      "- `lexico_objectives` - dict, default=None | It specifics information needed to perform multi-objective\n",
      "  optimization with lexicographic preferences. When lexico_objectives is not None, the arguments metric,\n",
      "  mode, will be invalid, and flaml's tune uses CFO\n",
      "  as the `search_alg`, which makes the input (if provided) `search_alg' invalid.\n",
      "  This dictionary shall contain the following fields of key-value pairs:\n",
      "  - \"metrics\":  a list of optimization objectives with the orders reflecting the priorities/preferences of the\n",
      "  objectives.\n",
      "  - \"modes\" (optional): a list of optimization modes (each mode either \"min\" or \"max\") corresponding to the\n",
      "  objectives in the metric list. If not provided, we use \"min\" as the default mode for all the objectives.\n",
      "  - \"targets\" (optional): a dictionary to specify the optimization targets on the objectives. The keys are the\n",
      "  metric names (provided in \"metric\"), and the values are the numerical target values.\n",
      "  - \"tolerances\" (optional): a dictionary to specify the optimality tolerances on objectives. The keys are the metric names (provided in \"metrics\"), and the values are the absolute/percentage tolerance in the form of numeric/string.\n",
      "  E.g.,\n",
      "```python\n",
      "lexico_objectives = {\n",
      "    \"metrics\": [\"error_rate\", \"pred_time\"],\n",
      "    \"modes\": [\"min\", \"min\"],\n",
      "    \"tolerances\": {\"error_rate\": 0.01, \"pred_time\": 0.0},\n",
      "    \"targets\": {\"error_rate\": 0.0},\n",
      "}\n",
      "```\n",
      "  We also support percentage tolerance.\n",
      "  E.g.,\n",
      "```python\n",
      "lexico_objectives = {\n",
      "    \"metrics\": [\"error_rate\", \"pred_time\"],\n",
      "    \"modes\": [\"min\", \"min\"],\n",
      "    \"tolerances\": {\"error_rate\": \"5%\", \"pred_time\": \"0%\"},\n",
      "    \"targets\": {\"error_rate\": 0.0},\n",
      "}\n",
      "```\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel the PySpark job if overtime.\n",
      "- `n_concurrent_trials` - int, default=0 | The number of concurrent trials when perform hyperparameter\n",
      "  tuning with Spark. Only valid when use_spark=True and spark is required:\n",
      "  `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark. When tune.run() is called from AutoML, it will be\n",
      "  overwritten by the value of `n_concurrent_trials` in AutoML. When <= 0, the concurrent trials\n",
      "  will be set to the number of executors.\n",
      "- `**ray_args` - keyword arguments to pass to ray.tune.run().\n",
      "  Only valid when use_ray=True.\n",
      "\n",
      "## Tuner Objects\n",
      "\n",
      "```python\n",
      "class Tuner()\n",
      "```\n",
      "\n",
      "Tuner is the class-based way of launching hyperparameter tuning jobs compatible with Ray Tune 2.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `trainable` - A user-defined evaluation function.\n",
      "  It takes a configuration as input, outputs a evaluation\n",
      "  result (can be a numerical value or a dictionary of string\n",
      "  and numerical value pairs) for the input configuration.\n",
      "  For machine learning tasks, it usually involves training and\n",
      "  scoring a machine learning model, e.g., through validation loss.\n",
      "- `param_space` - Search space of the tuning job.\n",
      "  One thing to note is that both preprocessor and dataset can be tuned here.\n",
      "- `tune_config` - Tuning algorithm specific configs.\n",
      "  Refer to ray.tune.tune_config.TuneConfig for more info.\n",
      "- `run_config` - Runtime configuration that is specific to individual trials.\n",
      "  If passed, this will overwrite the run config passed to the Trainer,\n",
      "  if applicable. Refer to ray.air.config.RunConfig for more info.\n",
      "  \n",
      "  Usage pattern:\n",
      "  \n",
      "  .. code-block:: python\n",
      "  \n",
      "  from sklearn.datasets import load_breast_cancer\n",
      "  \n",
      "  from ray import tune\n",
      "  from ray.data import from_pandas\n",
      "  from ray.air.config import RunConfig, ScalingConfig\n",
      "  from ray.train.xgboost import XGBoostTrainer\n",
      "  from ray.tune.tuner import Tuner\n",
      "  \n",
      "  def get_dataset():\n",
      "  data_raw = load_breast_cancer(as_frame=True)\n",
      "  dataset_df = data_raw[\"data\"]\n",
      "  dataset_df[\"target\"] = data_raw[\"target\"]\n",
      "  dataset = from_pandas(dataset_df)\n",
      "  return dataset\n",
      "  \n",
      "  trainer = XGBoostTrainer(\n",
      "  label_column=\"target\",\n",
      "  params={},\n",
      "- `datasets={\"train\"` - get_dataset()},\n",
      "  )\n",
      "  \n",
      "  param_space = {\n",
      "- `\"scaling_config\"` - ScalingConfig(\n",
      "  num_workers=tune.grid_search([2, 4]),\n",
      "  resources_per_worker={\n",
      "- `\"CPU\"` - tune.grid_search([1, 2]),\n",
      "  },\n",
      "  ),\n",
      "  # You can even grid search various datasets in Tune.\n",
      "  # \"datasets\": {\n",
      "  #     \"train\": tune.grid_search(\n",
      "  #         [ds1, ds2]\n",
      "  #     ),\n",
      "  # },\n",
      "- `\"params\"` - {\n",
      "- `\"objective\"` - \"binary:logistic\",\n",
      "- `\"tree_method\"` - \"approx\",\n",
      "- `\"eval_metric\"` - [\"logloss\", \"error\"],\n",
      "- `\"eta\"` - tune.loguniform(1e-4, 1e-1),\n",
      "- `\"subsample\"` - tune.uniform(0.5, 1.0),\n",
      "- `\"max_depth\"` - tune.randint(1, 9),\n",
      "  },\n",
      "  }\n",
      "  tuner = Tuner(trainable=trainer, param_space=param_space,\n",
      "  run_config=RunConfig(name=\"my_tune_run\"))\n",
      "  analysis = tuner.fit()\n",
      "  \n",
      "  To retry a failed tune run, you can then do\n",
      "  \n",
      "  .. code-block:: python\n",
      "  \n",
      "  tuner = Tuner.restore(experiment_checkpoint_dir)\n",
      "  tuner.fit()\n",
      "  \n",
      "  ``experiment_checkpoint_dir`` can be easily located near the end of the\n",
      "  console output of your first failed run.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "To perform a classification task using FLAML and using Spark for parallel training, you can follow these steps:\n",
      "\n",
      "1. Install FLAML with Spark support by running `pip install flaml[spark]`.\n",
      "2. Set up the AutoML instance with the required settings, like specifying `time_budget` for 30 seconds, `use_spark` to True, and `force_cancel` to True for cancelling Spark jobs if the time limit is reached.\n",
      "3. Use the `fit` method to train the model.\n",
      "\n",
      "Here is the code example:\n",
      "\n",
      "```python\n",
      "from flaml import AutoML\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "# Load the iris dataset\n",
      "data = load_iris()\n",
      "X_train, y_train = data.data, data.target\n",
      "\n",
      "# Create AutoML instance\n",
      "automl = AutoML()\n",
      "\n",
      "# Configure AutoML for classification task with Spark and a 30 seconds training time\n",
      "automl_settings = {\n",
      "    \"time_budget\": 30,  # Train for 30 seconds\n",
      "    \"task\": \"classification\",\n",
      "    \"n_concurrent_trials\": 4,  # Number of concurrent trials\n",
      "    \"use_spark\": True,  # To use spark for parallel training\n",
      "    \"force_cancel\": True,  # To cancel Spark jobs if time limit is reached\n",
      "}\n",
      "\n",
      "# Train the model\n",
      "automl.fit(X_train, y_train, **automl_settings)\n",
      "```\n",
      "\n",
      "This will train an AutoML model using FLAML and Spark for parallel training, with a time budget of 30 seconds and force cancel jobs if the time limit is reached.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/03 22:00:19 WARN Utils: Your hostname, DESKTOP-TTKT4BA resolves to a loopback address: 127.0.1.1; using 172.31.215.15 instead (on interface eth0)\n",
      "23/08/03 22:00:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/03 22:00:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[flaml.automl.logger: 08-03 22:00:23] {1679} INFO - task = classification\n",
      "[flaml.automl.logger: 08-03 22:00:23] {1690} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 08-03 22:00:23] {1788} INFO - Minimizing error metric: log_loss\n",
      "[flaml.automl.logger: 08-03 22:00:23] {1900} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-03 22:00:23,676]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "\u001b[32m[I 2023-08-03 22:00:23,927]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/03 22:00:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[flaml.tune.tune: 08-03 22:00:25] {729} INFO - Number of trials: 1/1000000, 1 RUNNING, 0 TERMINATED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-03 22:00:34] {749} INFO - Brief result: {'pred_time': 6.8442026774088526e-06, 'wall_clock_time': 17.609724760055542, 'metric_for_logging': {'pred_time': 6.8442026774088526e-06}, 'val_loss': 0.6499914632462661, 'trained_estimator': <flaml.automl.model.LGBMEstimator object at 0x7f260cb084f0>}\n",
      "[flaml.tune.tune: 08-03 22:00:34] {729} INFO - Number of trials: 2/1000000, 1 RUNNING, 1 TERMINATED\n",
      "[flaml.tune.tune: 08-03 22:00:34] {749} INFO - Brief result: {'pred_time': 1.7034212748209633e-05, 'wall_clock_time': 17.8204128742218, 'metric_for_logging': {'pred_time': 1.7034212748209633e-05}, 'val_loss': 0.13431251746799158, 'trained_estimator': <flaml.automl.model.RandomForestEstimator object at 0x7f260cb08d30>}\n",
      "[flaml.tune.tune: 08-03 22:00:34] {729} INFO - Number of trials: 3/1000000, 1 RUNNING, 2 TERMINATED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-03 22:00:46] {749} INFO - Brief result: {'pred_time': 2.990245819091797e-05, 'wall_clock_time': 29.605072498321533, 'metric_for_logging': {'pred_time': 2.990245819091797e-05}, 'val_loss': 0.17314595016097636, 'trained_estimator': <flaml.automl.model.CatBoostEstimator object at 0x7f260cb08dc0>}\n",
      "[flaml.tune.tune: 08-03 22:00:46] {729} INFO - Number of trials: 4/1000000, 1 RUNNING, 3 TERMINATED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "Time exceeded, canceled jobs                                        (0 + 1) / 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[flaml.automl.logger: 08-03 22:00:47] {2493} INFO - selected model: None\n",
      "[flaml.automl.logger: 08-03 22:00:47] {2627} INFO - retrain rf for 0.0s\n",
      "[flaml.automl.logger: 08-03 22:00:47] {2630} INFO - retrained model: RandomForestClassifier(criterion='entropy', max_features=0.5000000000000001,\n",
      "                       max_leaf_nodes=4, n_estimators=4, n_jobs=-1,\n",
      "                       random_state=12032022)\n",
      "[flaml.automl.logger: 08-03 22:00:47] {1930} INFO - fit succeeded\n",
      "[flaml.automl.logger: 08-03 22:00:47] {1931} INFO - Time taken to find the best model: 17.8204128742218\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "None\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "# given a problem, we use the ragproxyagent to generate a prompt to be sent to the assistant as the initial message.\n",
    "# the assistant receives the message and generates a response. The response will be sent back to the ragproxyagent for processing.\n",
    "# The conversation continues until the termination condition is met, in RetrieveChat, the termination condition when no human-in-loop is no code block detected.\n",
    "# With human-in-loop, the conversation will continue until the user says \"exit\".\n",
    "code_problem = \"How can I use FLAML to perform a classification task and use spark to do parallel training. Train 30 seconds and force cancel jobs if time limit is reached.\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=code_problem, search_string=\"spark\")  # search_string is used as an extra filter for the embeddings search, in this case, we only want to search documents that contain \"spark\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Use RetrieveChat to answer a question that is not related to code generation.\n",
    "\n",
    "Problem: Who is the author of FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_58', 'doc_51', 'doc_35', 'doc_3', 'doc_22', 'doc_40', 'doc_14', 'doc_13', 'doc_59', 'doc_52', 'doc_1', 'doc_6', 'doc_28', 'doc_56', 'doc_29', 'doc_2', 'doc_55', 'doc_44', 'doc_19', 'doc_32']]\n",
      "Adding doc_id doc_58 to context.\n",
      "Adding doc_id doc_51 to context.\n",
      "Adding doc_id doc_35 to context.\n",
      "Adding doc_id doc_3 to context.\n",
      "Adding doc_id doc_22 to context.\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user. You should follow the following steps to answer a question:\n",
      "Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
      "a QA task.\n",
      "Step 2, you generate code or answer the question based on the intent.\n",
      "You should leverage the context provided by the user as much as possible. If you think the context is not enough, you\n",
      "can reply exactly \"UPDATE CONTEXT\" to ask the user to provide more contexts.\n",
      "For code generation, you must obey the following rules:\n",
      "You MUST NOT install any packages because all the packages needed are already installed.\n",
      "The code will be executed in IPython, you must follow the formats below to write your code:\n",
      "```python\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Who is the author of FLAML?\n",
      "\n",
      "Context is: ---\n",
      "sidebar_label: estimator\n",
      "title: default.estimator\n",
      "---\n",
      "\n",
      "#### flamlize\\_estimator\n",
      "\n",
      "```python\n",
      "def flamlize_estimator(super_class, name: str, task: str, alternatives=None)\n",
      "```\n",
      "\n",
      "Enhance an estimator class with flaml's data-dependent default hyperparameter settings.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "import sklearn.ensemble as ensemble\n",
      "RandomForestRegressor = flamlize_estimator(\n",
      "    ensemble.RandomForestRegressor, \"rf\", \"regression\"\n",
      ")\n",
      "```\n",
      "  \n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `super_class` - an scikit-learn compatible estimator class.\n",
      "- `name` - a str of the estimator's name.\n",
      "- `task` - a str of the task type.\n",
      "- `alternatives` - (Optional) a list for alternative estimator names. For example,\n",
      "  ```[(\"max_depth\", 0, \"xgboost\")]``` means if the \"max_depth\" is set to 0\n",
      "  in the constructor, then look for the learned defaults for estimator \"xgboost\".\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: task\n",
      "title: automl.task.task\n",
      "---\n",
      "\n",
      "## Task Objects\n",
      "\n",
      "```python\n",
      "class Task(ABC)\n",
      "```\n",
      "\n",
      "Abstract base class for a machine learning task.\n",
      "\n",
      "Class definitions should implement abstract methods and provide a non-empty dictionary of estimator classes.\n",
      "A Task can be suitable to be used for multiple machine-learning tasks (e.g. classification or regression) or be\n",
      "implemented specifically for a single one depending on the generality of data validation and model evaluation methods\n",
      "implemented. The implementation of a Task may optionally use the training data and labels to determine data and task\n",
      "specific details, such as in determining if a problem is single-label or multi-label.\n",
      "\n",
      "FLAML evaluates at runtime how to behave exactly, relying on the task instance to provide implementations of\n",
      "operations which vary between tasks.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(task_name: str, X_train: Optional[Union[np.ndarray, DataFrame, psDataFrame]] = None, y_train: Optional[Union[np.ndarray, DataFrame, Series, psSeries]] = None)\n",
      "```\n",
      "\n",
      "Constructor.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `task_name` - String name for this type of task. Used when the Task can be generic and implement a number of\n",
      "  types of sub-task.\n",
      "- `X_train` - Optional. Some Task types may use the data shape or features to determine details of their usage,\n",
      "  such as in binary vs multilabel classification.\n",
      "- `y_train` - Optional. Some Task types may use the data shape or features to determine details of their usage,\n",
      "  such as in binary vs multilabel classification.\n",
      "\n",
      "#### \\_\\_str\\_\\_\n",
      "\n",
      "```python\n",
      "def __str__() -> str\n",
      "```\n",
      "\n",
      "Name of this task type.\n",
      "\n",
      "#### evaluate\\_model\\_CV\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def evaluate_model_CV(config: dict, estimator: \"flaml.automl.ml.BaseEstimator\", X_train_all: Union[np.ndarray, DataFrame, psDataFrame], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries], budget: int, kf, eval_metric: str, best_val_loss: float, log_training_metric: bool = False, fit_kwargs: Optional[dict] = {}) -> Tuple[float, float, float, float]\n",
      "```\n",
      "\n",
      "Evaluate the model using cross-validation.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `config` - configuration used in the evaluation of the metric.\n",
      "- `estimator` - Estimator class of the model.\n",
      "- `X_train_all` - Complete training feature data.\n",
      "- `y_train_all` - Complete training target data.\n",
      "- `budget` - Training time budget.\n",
      "- `kf` - Cross-validation index generator.\n",
      "- `eval_metric` - Metric name to be used for evaluation.\n",
      "- `best_val_loss` - Best current validation-set loss.\n",
      "- `log_training_metric` - Bool defaults False. Enables logging of the training metric.\n",
      "- `fit_kwargs` - Additional kwargs passed to the estimator's fit method.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  validation loss, metric value, train time, prediction time\n",
      "\n",
      "#### validate\\_data\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def validate_data(automl: \"flaml.automl.automl.AutoML\", state: \"flaml.automl.state.AutoMLState\", X_train_all: Union[np.ndarray, DataFrame, psDataFrame, None], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], dataframe: Union[DataFrame, None], label: str, X_val: Optional[Union[np.ndarray, DataFrame, psDataFrame]] = None, y_val: Optional[Union[np.ndarray, DataFrame, Series, psSeries]] = None, groups_val: Optional[List[str]] = None, groups: Optional[List[str]] = None)\n",
      "```\n",
      "\n",
      "Validate that the data is suitable for this task type.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `automl` - The AutoML instance from which this task has been constructed.\n",
      "- `state` - The AutoMLState instance for this run.\n",
      "- `X_train_all` - The complete data set or None if dataframe is supplied.\n",
      "- `y_train_all` - The complete target set or None if dataframe is supplied.\n",
      "- `dataframe` - A dataframe constaining the complete data set with targets.\n",
      "- `label` - The name of the target column in dataframe.\n",
      "- `X_val` - Optional. A data set for validation.\n",
      "- `y_val` - Optional. A target vector corresponding to X_val for validation.\n",
      "- `groups_val` - Group labels (with matching length to y_val) or group counts (with sum equal to length of y_val)\n",
      "  for validation data. Need to be consistent with groups.\n",
      "- `groups` - Group labels (with matching length to y_train) or groups counts (with sum equal to length of y_train)\n",
      "  for training data.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `AssertionError` - The data provided is invalid for this task type and configuration.\n",
      "\n",
      "#### prepare\\_data\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def prepare_data(state: \"flaml.automl.state.AutoMLState\", X_train_all: Union[np.ndarray, DataFrame, psDataFrame], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], auto_augment: bool, eval_method: str, split_type: str, split_ratio: float, n_splits: int, data_is_df: bool, sample_weight_full: Optional[List[float]] = None)\n",
      "```\n",
      "\n",
      "Prepare the data for fitting or inference.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `automl` - The AutoML instance from which this task has been constructed.\n",
      "- `state` - The AutoMLState instance for this run.\n",
      "- `X_train_all` - The complete data set or None if dataframe is supplied. Must\n",
      "  contain the target if y_train_all is None\n",
      "- `y_train_all` - The complete target set or None if supplied in X_train_all.\n",
      "- `auto_augment` - If true, task-specific data augmentations will be applied.\n",
      "- `eval_method` - A string of resampling strategy, one of ['auto', 'cv', 'holdout'].\n",
      "- `split_type` - str or splitter object, default=\"auto\" | the data split type.\n",
      "  * A valid splitter object is an instance of a derived class of scikit-learn\n",
      "  [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
      "  and have ``split`` and ``get_n_splits`` methods with the same signatures.\n",
      "  Set eval_method to \"cv\" to use the splitter object.\n",
      "  * Valid str options depend on different tasks.\n",
      "  For classification tasks, valid choices are\n",
      "  [\"auto\", 'stratified', 'uniform', 'time', 'group']. \"auto\" -> stratified.\n",
      "  For regression tasks, valid choices are [\"auto\", 'uniform', 'time'].\n",
      "  \"auto\" -> uniform.\n",
      "  For time series forecast tasks, must be \"auto\" or 'time'.\n",
      "  For ranking task, must be \"auto\" or 'group'.\n",
      "- `split_ratio` - A float of the valiation data percentage for holdout.\n",
      "- `n_splits` - An integer of the number of folds for cross - validation.\n",
      "- `data_is_df` - True if the data was provided as a DataFrame else False.\n",
      "- `sample_weight_full` - A 1d arraylike of the sample weight.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `AssertionError` - The configuration provided is invalid for this task type and data.\n",
      "\n",
      "#### decide\\_split\\_type\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def decide_split_type(split_type: str, y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], fit_kwargs: dict, groups: Optional[List[str]] = None) -> str\n",
      "```\n",
      "\n",
      "Choose an appropriate data split type for this data and task.\n",
      "\n",
      "If split_type is 'auto' then this is determined based on the task type and data.\n",
      "If a specific split_type is requested then the choice is validated to be appropriate.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `split_type` - Either 'auto' or a task appropriate split type.\n",
      "- `y_train_all` - The complete set of targets.\n",
      "- `fit_kwargs` - Additional kwargs passed to the estimator's fit method.\n",
      "- `groups` - Optional. Group labels (with matching length to y_train) or groups counts (with sum equal to length\n",
      "  of y_train) for training data.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  The determined appropriate split type.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `AssertionError` - The requested split_type is invalid for this task, configuration and data.\n",
      "\n",
      "#### preprocess\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def preprocess(X: Union[np.ndarray, DataFrame, psDataFrame], transformer: Optional[\"flaml.automl.data.DataTransformer\"] = None) -> Union[np.ndarray, DataFrame]\n",
      "```\n",
      "\n",
      "Preprocess the data ready for fitting or inference with this task type.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `X` - The data set to process.\n",
      "- `transformer` - A DataTransformer instance to be used in processing.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  The preprocessed data set having the same type as the input.\n",
      "\n",
      "#### default\\_estimator\\_list\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def default_estimator_list(estimator_list: Union[List[str], str] = \"auto\", is_spark_dataframe: bool = False) -> List[str]\n",
      "```\n",
      "\n",
      "Return the list of default estimators registered for this task type.\n",
      "\n",
      "If 'auto' is provided then the default list is returned, else the provided list will be validated given this task\n",
      "type.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "new_automl = AutoML()\n",
      "new_automl.fit(X_train, y_train, starting_points=starting_points)\n",
      "```\n",
      "  \n",
      "- `seed` - int or None, default=None | The random seed for hpo.\n",
      "- `n_concurrent_trials` - [Experimental] int, default=1 | The number of\n",
      "  concurrent trials. When n_concurrent_trials > 1, flaml performes\n",
      "  [parallel tuning](../../Use-Cases/Task-Oriented-AutoML#parallel-tuning)\n",
      "  and installation of ray or spark is required: `pip install flaml[ray]`\n",
      "  or `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark.\n",
      "- `keep_search_state` - boolean, default=False | Whether to keep data needed\n",
      "  for model search after fit(). By default the state is deleted for\n",
      "  space saving.\n",
      "- `preserve_checkpoint` - boolean, default=True | Whether to preserve the saved checkpoint\n",
      "  on disk when deleting automl. By default the checkpoint is preserved.\n",
      "- `early_stop` - boolean, default=False | Whether to stop early if the\n",
      "  search is considered to converge.\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel Spark jobs if the\n",
      "  search time exceeded the time budget.\n",
      "- `append_log` - boolean, default=False | Whetehr to directly append the log\n",
      "  records to the input log file if it exists.\n",
      "- `auto_augment` - boolean, default=True | Whether to automatically\n",
      "  augment rare classes.\n",
      "- `min_sample_size` - int, default=MIN_SAMPLE_TRAIN | the minimal sample\n",
      "  size when sample=True.\n",
      "- `use_ray` - boolean or dict.\n",
      "  If boolean: default=False | Whether to use ray to run the training\n",
      "  in separate processes. This can be used to prevent OOM for large\n",
      "  datasets, but will incur more overhead in time.\n",
      "  If dict: the dict contains the keywords arguments to be passed to\n",
      "  [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).\n",
      "- `use_spark` - boolean, default=False | Whether to use spark to run the training\n",
      "  in parallel spark jobs. This can be used to accelerate training on large models\n",
      "  and large datasets, but will incur more overhead in time and thus slow down\n",
      "  training in some cases. GPU training is not supported yet when use_spark is True.\n",
      "  For Spark clusters, by default, we will launch one trial per executor. However,\n",
      "  sometimes we want to launch more trials than the number of executors (e.g., local mode).\n",
      "  In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override\n",
      "  the detected `num_executors`. The final number of concurrent trials will be the minimum\n",
      "  of `n_concurrent_trials` and `num_executors`.\n",
      "- `free_mem_ratio` - float between 0 and 1, default=0. The free memory ratio to keep during training.\n",
      "- `metric_constraints` - list, default=[] | The list of metric constraints.\n",
      "  Each element in this list is a 3-tuple, which shall be expressed\n",
      "  in the following format: the first element of the 3-tuple is the name of the\n",
      "  metric, the second element is the inequality sign chosen from \">=\" and \"<=\",\n",
      "  and the third element is the constraint value. E.g., `('val_loss', '<=', 0.1)`.\n",
      "  Note that all the metric names in metric_constraints need to be reported via\n",
      "  the metrics_to_log dictionary returned by a customized metric function.\n",
      "  The customized metric function shall be provided via the `metric` key word\n",
      "  argument of the fit() function or the automl constructor.\n",
      "  Find an example in the 4th constraint type in this [doc](../../Use-Cases/Task-Oriented-AutoML#constraint).\n",
      "  If `pred_time_limit` is provided as one of keyword arguments to fit() function or\n",
      "  the automl constructor, flaml will automatically (and under the hood)\n",
      "  add it as an additional element in the metric_constraints. Essentially 'pred_time_limit'\n",
      "  specifies a constraint about the prediction latency constraint in seconds.\n",
      "- `custom_hp` - dict, default=None | The custom search space specified by user.\n",
      "  It is a nested dict with keys being the estimator names, and values being dicts\n",
      "  per estimator search space. In the per estimator search space dict,\n",
      "  the keys are the hyperparameter names, and values are dicts of info (\"domain\",\n",
      "  \"init_value\", and \"low_cost_init_value\") about the search space associated with\n",
      "  the hyperparameter (i.e., per hyperparameter search space dict). When custom_hp\n",
      "  is provided, the built-in search space which is also a nested dict of per estimator\n",
      "  search space dict, will be updated with custom_hp. Note that during this nested dict update,\n",
      "  the per hyperparameter search space dicts will be replaced (instead of updated) by the ones\n",
      "  provided in custom_hp. Note that the value for \"domain\" can either be a constant\n",
      "  or a sample.Domain object.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "custom_hp = {\n",
      "     \"transformer_ms\": {\n",
      "         \"model_path\": {\n",
      "             \"domain\": \"albert-base-v2\",\n",
      "         },\n",
      "         \"learning_rate\": {\n",
      "             \"domain\": tune.choice([1e-4, 1e-5]),\n",
      "         }\n",
      "     }\n",
      " }\n",
      "```\n",
      "- `skip_transform` - boolean, default=False | Whether to pre-process data prior to modeling.\n",
      "- `fit_kwargs_by_estimator` - dict, default=None | The user specified keywords arguments, grouped by estimator name.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "fit_kwargs_by_estimator = {\n",
      "    \"transformer\": {\n",
      "        \"output_dir\": \"test/data/output/\",\n",
      "        \"fp16\": False,\n",
      "    }\n",
      "}\n",
      "```\n",
      "- `mlflow_logging` - boolean, default=True | Whether to log the training results to mlflow.\n",
      "  This requires mlflow to be installed and to have an active mlflow run.\n",
      "  FLAML will create nested runs.\n",
      "\n",
      "#### config\\_history\n",
      "\n",
      "```python\n",
      "@property\n",
      "def config_history() -> dict\n",
      "```\n",
      "\n",
      "A dictionary of iter->(estimator, config, time),\n",
      "storing the best estimator, config, and the time when the best\n",
      "model is updated each time.\n",
      "\n",
      "#### model\n",
      "\n",
      "```python\n",
      "@property\n",
      "def model()\n",
      "```\n",
      "\n",
      "An object with `predict()` and `predict_proba()` method (for\n",
      "classification), storing the best trained model.\n",
      "\n",
      "#### best\\_model\\_for\\_estimator\n",
      "\n",
      "```python\n",
      "def best_model_for_estimator(estimator_name: str)\n",
      "```\n",
      "\n",
      "Return the best model found for a particular estimator.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `estimator_name` - a str of the estimator's name.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  An object storing the best model for estimator_name.\n",
      "  If `model_history` was set to False during fit(), then the returned model\n",
      "  is untrained unless estimator_name is the best estimator.\n",
      "  If `model_history` was set to True, then the returned model is trained.\n",
      "\n",
      "#### best\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_estimator()\n",
      "```\n",
      "\n",
      "A string indicating the best estimator found.\n",
      "\n",
      "#### best\\_iteration\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_iteration()\n",
      "```\n",
      "\n",
      "An integer of the iteration number where the best\n",
      "config is found.\n",
      "\n",
      "#### best\\_config\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config()\n",
      "```\n",
      "\n",
      "A dictionary of the best configuration.\n",
      "\n",
      "#### best\\_config\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best configuration.\n",
      "\n",
      "#### best\\_loss\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best loss.\n",
      "\n",
      "#### best\\_loss\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss()\n",
      "```\n",
      "\n",
      "A float of the best loss found.\n",
      "\n",
      "#### best\\_result\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_result()\n",
      "```\n",
      "\n",
      "Result dictionary for model trained with the best config.\n",
      "\n",
      "#### metrics\\_for\\_best\\_config\n",
      "---\n",
      "sidebar_label: user_proxy_agent\n",
      "title: autogen.agent.user_proxy_agent\n",
      "---\n",
      "\n",
      "## UserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class UserProxyAgent(Agent)\n",
      "```\n",
      "\n",
      "(Experimental) A proxy agent for the user, that can execute code and provide feedback to the other agents.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name: str, system_message: Optional[str] = \"\", work_dir: Optional[str] = None, human_input_mode: Optional[str] = \"ALWAYS\", function_map: Optional[Dict[str, Callable]] = {}, max_consecutive_auto_reply: Optional[int] = None, is_termination_msg: Optional[Callable[[Dict], bool]] = None, use_docker: Optional[Union[List[str], str, bool]] = True, timeout: Optional[int] = 600, **config, ,)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `system_message` _str_ - system message for the agent.\n",
      "- `work_dir` _Optional, str_ - The working directory for the code execution.\n",
      "  If None, a default working directory will be used.\n",
      "  The default working directory is the \"extensions\" directory under\n",
      "  \"path_to_flaml/autogen\".\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `function_map` _dict[str, callable]_ - Mapping function names (passed to openai) to callable functions.\n",
      "- `max_consecutive_auto_reply` _int_ - the maximum number of consecutive auto replies.\n",
      "  default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
      "  The limit only plays a role when human_input_mode is not \"ALWAYS\".\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `use_docker` _Optional, list, str or bool_ - The docker image to use for code execution.\n",
      "  If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
      "  with the first image successfully pulled.\n",
      "  If None, False or empty, the code will be executed in the current environment.\n",
      "  Default is True, which will be converted into a list.\n",
      "  If the code is executed in the current environment,\n",
      "  the code must be trusted.\n",
      "- `timeout` _Optional, int_ - The maximum execution time in seconds.\n",
      "- `**config` _dict_ - other configurations.\n",
      "\n",
      "#### use\\_docker\n",
      "\n",
      "```python\n",
      "@property\n",
      "def use_docker() -> Union[bool, str]\n",
      "```\n",
      "\n",
      "bool value of whether to use docker to execute the code,\n",
      "or str value of the docker image name to use.\n",
      "\n",
      "#### execute\\_code\n",
      "\n",
      "```python\n",
      "def execute_code(code_blocks)\n",
      "```\n",
      "\n",
      "Execute the code and return the result.\n",
      "\n",
      "#### auto\\_reply\n",
      "\n",
      "```python\n",
      "def auto_reply(message: dict, sender, default_reply=\"\")\n",
      "```\n",
      "\n",
      "Generate an auto reply.\n",
      "\n",
      "#### receive\n",
      "\n",
      "```python\n",
      "def receive(message: Union[Dict, str], sender)\n",
      "```\n",
      "\n",
      "Receive a message from the sender agent.\n",
      "Once a message is received, this function sends a reply to the sender or simply stop.\n",
      "The reply can be generated automatically or entered manually by a human.\n",
      "\n",
      "#### generate\\_init\\_prompt\n",
      "\n",
      "```python\n",
      "def generate_init_prompt(*args, **kwargs) -> Union[str, Dict]\n",
      "```\n",
      "\n",
      "Generate the initial prompt for the agent.\n",
      "\n",
      "Override this function to customize the initial prompt based on user's request.\n",
      "\n",
      "#### initiate\\_chat\n",
      "\n",
      "```python\n",
      "def initiate_chat(recipient, *args, **kwargs)\n",
      "```\n",
      "\n",
      "Initiate a chat with the receiver agent.\n",
      "\n",
      "`generate_init_prompt` is called to generate the initial prompt for the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `receiver` - the receiver agent.\n",
      "- `*args` - any additional arguments.\n",
      "- `**kwargs` - any additional keyword arguments.\n",
      "\n",
      "#### register\\_function\n",
      "\n",
      "```python\n",
      "def register_function(function_map: Dict[str, Callable])\n",
      "```\n",
      "\n",
      "Register functions to the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `function_map` - a dictionary mapping function names to functions.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: cfo_cat\n",
      "title: tune.searcher.cfo_cat\n",
      "---\n",
      "\n",
      "## FLOW2Cat Objects\n",
      "\n",
      "```python\n",
      "class FLOW2Cat(FLOW2)\n",
      "```\n",
      "\n",
      "Local search algorithm optimized for categorical variables.\n",
      "\n",
      "## CFOCat Objects\n",
      "\n",
      "```python\n",
      "class CFOCat(CFO)\n",
      "```\n",
      "\n",
      "CFO optimized for categorical variables.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user. You should follow the following steps to answer a question:\n",
      "Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
      "a QA task.\n",
      "Step 2, you generate code or answer the question based on the intent.\n",
      "You should leverage the context provided by the user as much as possible. If you think the context is not enough, you\n",
      "can reply exactly \"UPDATE CONTEXT\" to ask the user to provide more contexts.\n",
      "For code generation, you must obey the following rules:\n",
      "You MUST NOT install any packages because all the packages needed are already installed.\n",
      "The code will be executed in IPython, you must follow the formats below to write your code:\n",
      "```python\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Who is the author of FLAML?\n",
      "\n",
      "Context is: ---\n",
      "sidebar_label: estimator\n",
      "title: default.estimator\n",
      "---\n",
      "\n",
      "#### flamlize\\_estimator\n",
      "\n",
      "```python\n",
      "def flamlize_estimator(super_class, name: str, task: str, alternatives=None)\n",
      "```\n",
      "\n",
      "Enhance an estimator class with flaml's data-dependent default hyperparameter settings.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "import sklearn.ensemble as ensemble\n",
      "RandomForestRegressor = flamlize_estimator(\n",
      "    ensemble.RandomForestRegressor, \"rf\", \"regression\"\n",
      ")\n",
      "```\n",
      "  \n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `super_class` - an scikit-learn compatible estimator class.\n",
      "- `name` - a str of the estimator's name.\n",
      "- `task` - a str of the task type.\n",
      "- `alternatives` - (Optional) a list for alternative estimator names. For example,\n",
      "  ```[(\"max_depth\", 0, \"xgboost\")]``` means if the \"max_depth\" is set to 0\n",
      "  in the constructor, then look for the learned defaults for estimator \"xgboost\".\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: task\n",
      "title: automl.task.task\n",
      "---\n",
      "\n",
      "## Task Objects\n",
      "\n",
      "```python\n",
      "class Task(ABC)\n",
      "```\n",
      "\n",
      "Abstract base class for a machine learning task.\n",
      "\n",
      "Class definitions should implement abstract methods and provide a non-empty dictionary of estimator classes.\n",
      "A Task can be suitable to be used for multiple machine-learning tasks (e.g. classification or regression) or be\n",
      "implemented specifically for a single one depending on the generality of data validation and model evaluation methods\n",
      "implemented. The implementation of a Task may optionally use the training data and labels to determine data and task\n",
      "specific details, such as in determining if a problem is single-label or multi-label.\n",
      "\n",
      "FLAML evaluates at runtime how to behave exactly, relying on the task instance to provide implementations of\n",
      "operations which vary between tasks.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(task_name: str, X_train: Optional[Union[np.ndarray, DataFrame, psDataFrame]] = None, y_train: Optional[Union[np.ndarray, DataFrame, Series, psSeries]] = None)\n",
      "```\n",
      "\n",
      "Constructor.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `task_name` - String name for this type of task. Used when the Task can be generic and implement a number of\n",
      "  types of sub-task.\n",
      "- `X_train` - Optional. Some Task types may use the data shape or features to determine details of their usage,\n",
      "  such as in binary vs multilabel classification.\n",
      "- `y_train` - Optional. Some Task types may use the data shape or features to determine details of their usage,\n",
      "  such as in binary vs multilabel classification.\n",
      "\n",
      "#### \\_\\_str\\_\\_\n",
      "\n",
      "```python\n",
      "def __str__() -> str\n",
      "```\n",
      "\n",
      "Name of this task type.\n",
      "\n",
      "#### evaluate\\_model\\_CV\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def evaluate_model_CV(config: dict, estimator: \"flaml.automl.ml.BaseEstimator\", X_train_all: Union[np.ndarray, DataFrame, psDataFrame], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries], budget: int, kf, eval_metric: str, best_val_loss: float, log_training_metric: bool = False, fit_kwargs: Optional[dict] = {}) -> Tuple[float, float, float, float]\n",
      "```\n",
      "\n",
      "Evaluate the model using cross-validation.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `config` - configuration used in the evaluation of the metric.\n",
      "- `estimator` - Estimator class of the model.\n",
      "- `X_train_all` - Complete training feature data.\n",
      "- `y_train_all` - Complete training target data.\n",
      "- `budget` - Training time budget.\n",
      "- `kf` - Cross-validation index generator.\n",
      "- `eval_metric` - Metric name to be used for evaluation.\n",
      "- `best_val_loss` - Best current validation-set loss.\n",
      "- `log_training_metric` - Bool defaults False. Enables logging of the training metric.\n",
      "- `fit_kwargs` - Additional kwargs passed to the estimator's fit method.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  validation loss, metric value, train time, prediction time\n",
      "\n",
      "#### validate\\_data\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def validate_data(automl: \"flaml.automl.automl.AutoML\", state: \"flaml.automl.state.AutoMLState\", X_train_all: Union[np.ndarray, DataFrame, psDataFrame, None], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], dataframe: Union[DataFrame, None], label: str, X_val: Optional[Union[np.ndarray, DataFrame, psDataFrame]] = None, y_val: Optional[Union[np.ndarray, DataFrame, Series, psSeries]] = None, groups_val: Optional[List[str]] = None, groups: Optional[List[str]] = None)\n",
      "```\n",
      "\n",
      "Validate that the data is suitable for this task type.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `automl` - The AutoML instance from which this task has been constructed.\n",
      "- `state` - The AutoMLState instance for this run.\n",
      "- `X_train_all` - The complete data set or None if dataframe is supplied.\n",
      "- `y_train_all` - The complete target set or None if dataframe is supplied.\n",
      "- `dataframe` - A dataframe constaining the complete data set with targets.\n",
      "- `label` - The name of the target column in dataframe.\n",
      "- `X_val` - Optional. A data set for validation.\n",
      "- `y_val` - Optional. A target vector corresponding to X_val for validation.\n",
      "- `groups_val` - Group labels (with matching length to y_val) or group counts (with sum equal to length of y_val)\n",
      "  for validation data. Need to be consistent with groups.\n",
      "- `groups` - Group labels (with matching length to y_train) or groups counts (with sum equal to length of y_train)\n",
      "  for training data.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `AssertionError` - The data provided is invalid for this task type and configuration.\n",
      "\n",
      "#### prepare\\_data\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def prepare_data(state: \"flaml.automl.state.AutoMLState\", X_train_all: Union[np.ndarray, DataFrame, psDataFrame], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], auto_augment: bool, eval_method: str, split_type: str, split_ratio: float, n_splits: int, data_is_df: bool, sample_weight_full: Optional[List[float]] = None)\n",
      "```\n",
      "\n",
      "Prepare the data for fitting or inference.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `automl` - The AutoML instance from which this task has been constructed.\n",
      "- `state` - The AutoMLState instance for this run.\n",
      "- `X_train_all` - The complete data set or None if dataframe is supplied. Must\n",
      "  contain the target if y_train_all is None\n",
      "- `y_train_all` - The complete target set or None if supplied in X_train_all.\n",
      "- `auto_augment` - If true, task-specific data augmentations will be applied.\n",
      "- `eval_method` - A string of resampling strategy, one of ['auto', 'cv', 'holdout'].\n",
      "- `split_type` - str or splitter object, default=\"auto\" | the data split type.\n",
      "  * A valid splitter object is an instance of a derived class of scikit-learn\n",
      "  [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
      "  and have ``split`` and ``get_n_splits`` methods with the same signatures.\n",
      "  Set eval_method to \"cv\" to use the splitter object.\n",
      "  * Valid str options depend on different tasks.\n",
      "  For classification tasks, valid choices are\n",
      "  [\"auto\", 'stratified', 'uniform', 'time', 'group']. \"auto\" -> stratified.\n",
      "  For regression tasks, valid choices are [\"auto\", 'uniform', 'time'].\n",
      "  \"auto\" -> uniform.\n",
      "  For time series forecast tasks, must be \"auto\" or 'time'.\n",
      "  For ranking task, must be \"auto\" or 'group'.\n",
      "- `split_ratio` - A float of the valiation data percentage for holdout.\n",
      "- `n_splits` - An integer of the number of folds for cross - validation.\n",
      "- `data_is_df` - True if the data was provided as a DataFrame else False.\n",
      "- `sample_weight_full` - A 1d arraylike of the sample weight.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `AssertionError` - The configuration provided is invalid for this task type and data.\n",
      "\n",
      "#### decide\\_split\\_type\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def decide_split_type(split_type: str, y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], fit_kwargs: dict, groups: Optional[List[str]] = None) -> str\n",
      "```\n",
      "\n",
      "Choose an appropriate data split type for this data and task.\n",
      "\n",
      "If split_type is 'auto' then this is determined based on the task type and data.\n",
      "If a specific split_type is requested then the choice is validated to be appropriate.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `split_type` - Either 'auto' or a task appropriate split type.\n",
      "- `y_train_all` - The complete set of targets.\n",
      "- `fit_kwargs` - Additional kwargs passed to the estimator's fit method.\n",
      "- `groups` - Optional. Group labels (with matching length to y_train) or groups counts (with sum equal to length\n",
      "  of y_train) for training data.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  The determined appropriate split type.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `AssertionError` - The requested split_type is invalid for this task, configuration and data.\n",
      "\n",
      "#### preprocess\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def preprocess(X: Union[np.ndarray, DataFrame, psDataFrame], transformer: Optional[\"flaml.automl.data.DataTransformer\"] = None) -> Union[np.ndarray, DataFrame]\n",
      "```\n",
      "\n",
      "Preprocess the data ready for fitting or inference with this task type.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `X` - The data set to process.\n",
      "- `transformer` - A DataTransformer instance to be used in processing.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  The preprocessed data set having the same type as the input.\n",
      "\n",
      "#### default\\_estimator\\_list\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def default_estimator_list(estimator_list: Union[List[str], str] = \"auto\", is_spark_dataframe: bool = False) -> List[str]\n",
      "```\n",
      "\n",
      "Return the list of default estimators registered for this task type.\n",
      "\n",
      "If 'auto' is provided then the default list is returned, else the provided list will be validated given this task\n",
      "type.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "new_automl = AutoML()\n",
      "new_automl.fit(X_train, y_train, starting_points=starting_points)\n",
      "```\n",
      "  \n",
      "- `seed` - int or None, default=None | The random seed for hpo.\n",
      "- `n_concurrent_trials` - [Experimental] int, default=1 | The number of\n",
      "  concurrent trials. When n_concurrent_trials > 1, flaml performes\n",
      "  [parallel tuning](../../Use-Cases/Task-Oriented-AutoML#parallel-tuning)\n",
      "  and installation of ray or spark is required: `pip install flaml[ray]`\n",
      "  or `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark.\n",
      "- `keep_search_state` - boolean, default=False | Whether to keep data needed\n",
      "  for model search after fit(). By default the state is deleted for\n",
      "  space saving.\n",
      "- `preserve_checkpoint` - boolean, default=True | Whether to preserve the saved checkpoint\n",
      "  on disk when deleting automl. By default the checkpoint is preserved.\n",
      "- `early_stop` - boolean, default=False | Whether to stop early if the\n",
      "  search is considered to converge.\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel Spark jobs if the\n",
      "  search time exceeded the time budget.\n",
      "- `append_log` - boolean, default=False | Whetehr to directly append the log\n",
      "  records to the input log file if it exists.\n",
      "- `auto_augment` - boolean, default=True | Whether to automatically\n",
      "  augment rare classes.\n",
      "- `min_sample_size` - int, default=MIN_SAMPLE_TRAIN | the minimal sample\n",
      "  size when sample=True.\n",
      "- `use_ray` - boolean or dict.\n",
      "  If boolean: default=False | Whether to use ray to run the training\n",
      "  in separate processes. This can be used to prevent OOM for large\n",
      "  datasets, but will incur more overhead in time.\n",
      "  If dict: the dict contains the keywords arguments to be passed to\n",
      "  [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).\n",
      "- `use_spark` - boolean, default=False | Whether to use spark to run the training\n",
      "  in parallel spark jobs. This can be used to accelerate training on large models\n",
      "  and large datasets, but will incur more overhead in time and thus slow down\n",
      "  training in some cases. GPU training is not supported yet when use_spark is True.\n",
      "  For Spark clusters, by default, we will launch one trial per executor. However,\n",
      "  sometimes we want to launch more trials than the number of executors (e.g., local mode).\n",
      "  In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override\n",
      "  the detected `num_executors`. The final number of concurrent trials will be the minimum\n",
      "  of `n_concurrent_trials` and `num_executors`.\n",
      "- `free_mem_ratio` - float between 0 and 1, default=0. The free memory ratio to keep during training.\n",
      "- `metric_constraints` - list, default=[] | The list of metric constraints.\n",
      "  Each element in this list is a 3-tuple, which shall be expressed\n",
      "  in the following format: the first element of the 3-tuple is the name of the\n",
      "  metric, the second element is the inequality sign chosen from \">=\" and \"<=\",\n",
      "  and the third element is the constraint value. E.g., `('val_loss', '<=', 0.1)`.\n",
      "  Note that all the metric names in metric_constraints need to be reported via\n",
      "  the metrics_to_log dictionary returned by a customized metric function.\n",
      "  The customized metric function shall be provided via the `metric` key word\n",
      "  argument of the fit() function or the automl constructor.\n",
      "  Find an example in the 4th constraint type in this [doc](../../Use-Cases/Task-Oriented-AutoML#constraint).\n",
      "  If `pred_time_limit` is provided as one of keyword arguments to fit() function or\n",
      "  the automl constructor, flaml will automatically (and under the hood)\n",
      "  add it as an additional element in the metric_constraints. Essentially 'pred_time_limit'\n",
      "  specifies a constraint about the prediction latency constraint in seconds.\n",
      "- `custom_hp` - dict, default=None | The custom search space specified by user.\n",
      "  It is a nested dict with keys being the estimator names, and values being dicts\n",
      "  per estimator search space. In the per estimator search space dict,\n",
      "  the keys are the hyperparameter names, and values are dicts of info (\"domain\",\n",
      "  \"init_value\", and \"low_cost_init_value\") about the search space associated with\n",
      "  the hyperparameter (i.e., per hyperparameter search space dict). When custom_hp\n",
      "  is provided, the built-in search space which is also a nested dict of per estimator\n",
      "  search space dict, will be updated with custom_hp. Note that during this nested dict update,\n",
      "  the per hyperparameter search space dicts will be replaced (instead of updated) by the ones\n",
      "  provided in custom_hp. Note that the value for \"domain\" can either be a constant\n",
      "  or a sample.Domain object.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "custom_hp = {\n",
      "     \"transformer_ms\": {\n",
      "         \"model_path\": {\n",
      "             \"domain\": \"albert-base-v2\",\n",
      "         },\n",
      "         \"learning_rate\": {\n",
      "             \"domain\": tune.choice([1e-4, 1e-5]),\n",
      "         }\n",
      "     }\n",
      " }\n",
      "```\n",
      "- `skip_transform` - boolean, default=False | Whether to pre-process data prior to modeling.\n",
      "- `fit_kwargs_by_estimator` - dict, default=None | The user specified keywords arguments, grouped by estimator name.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "fit_kwargs_by_estimator = {\n",
      "    \"transformer\": {\n",
      "        \"output_dir\": \"test/data/output/\",\n",
      "        \"fp16\": False,\n",
      "    }\n",
      "}\n",
      "```\n",
      "- `mlflow_logging` - boolean, default=True | Whether to log the training results to mlflow.\n",
      "  This requires mlflow to be installed and to have an active mlflow run.\n",
      "  FLAML will create nested runs.\n",
      "\n",
      "#### config\\_history\n",
      "\n",
      "```python\n",
      "@property\n",
      "def config_history() -> dict\n",
      "```\n",
      "\n",
      "A dictionary of iter->(estimator, config, time),\n",
      "storing the best estimator, config, and the time when the best\n",
      "model is updated each time.\n",
      "\n",
      "#### model\n",
      "\n",
      "```python\n",
      "@property\n",
      "def model()\n",
      "```\n",
      "\n",
      "An object with `predict()` and `predict_proba()` method (for\n",
      "classification), storing the best trained model.\n",
      "\n",
      "#### best\\_model\\_for\\_estimator\n",
      "\n",
      "```python\n",
      "def best_model_for_estimator(estimator_name: str)\n",
      "```\n",
      "\n",
      "Return the best model found for a particular estimator.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `estimator_name` - a str of the estimator's name.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  An object storing the best model for estimator_name.\n",
      "  If `model_history` was set to False during fit(), then the returned model\n",
      "  is untrained unless estimator_name is the best estimator.\n",
      "  If `model_history` was set to True, then the returned model is trained.\n",
      "\n",
      "#### best\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_estimator()\n",
      "```\n",
      "\n",
      "A string indicating the best estimator found.\n",
      "\n",
      "#### best\\_iteration\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_iteration()\n",
      "```\n",
      "\n",
      "An integer of the iteration number where the best\n",
      "config is found.\n",
      "\n",
      "#### best\\_config\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config()\n",
      "```\n",
      "\n",
      "A dictionary of the best configuration.\n",
      "\n",
      "#### best\\_config\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best configuration.\n",
      "\n",
      "#### best\\_loss\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best loss.\n",
      "\n",
      "#### best\\_loss\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss()\n",
      "```\n",
      "\n",
      "A float of the best loss found.\n",
      "\n",
      "#### best\\_result\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_result()\n",
      "```\n",
      "\n",
      "Result dictionary for model trained with the best config.\n",
      "\n",
      "#### metrics\\_for\\_best\\_config\n",
      "---\n",
      "sidebar_label: user_proxy_agent\n",
      "title: autogen.agent.user_proxy_agent\n",
      "---\n",
      "\n",
      "## UserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class UserProxyAgent(Agent)\n",
      "```\n",
      "\n",
      "(Experimental) A proxy agent for the user, that can execute code and provide feedback to the other agents.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name: str, system_message: Optional[str] = \"\", work_dir: Optional[str] = None, human_input_mode: Optional[str] = \"ALWAYS\", function_map: Optional[Dict[str, Callable]] = {}, max_consecutive_auto_reply: Optional[int] = None, is_termination_msg: Optional[Callable[[Dict], bool]] = None, use_docker: Optional[Union[List[str], str, bool]] = True, timeout: Optional[int] = 600, **config, ,)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `system_message` _str_ - system message for the agent.\n",
      "- `work_dir` _Optional, str_ - The working directory for the code execution.\n",
      "  If None, a default working directory will be used.\n",
      "  The default working directory is the \"extensions\" directory under\n",
      "  \"path_to_flaml/autogen\".\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `function_map` _dict[str, callable]_ - Mapping function names (passed to openai) to callable functions.\n",
      "- `max_consecutive_auto_reply` _int_ - the maximum number of consecutive auto replies.\n",
      "  default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
      "  The limit only plays a role when human_input_mode is not \"ALWAYS\".\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `use_docker` _Optional, list, str or bool_ - The docker image to use for code execution.\n",
      "  If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
      "  with the first image successfully pulled.\n",
      "  If None, False or empty, the code will be executed in the current environment.\n",
      "  Default is True, which will be converted into a list.\n",
      "  If the code is executed in the current environment,\n",
      "  the code must be trusted.\n",
      "- `timeout` _Optional, int_ - The maximum execution time in seconds.\n",
      "- `**config` _dict_ - other configurations.\n",
      "\n",
      "#### use\\_docker\n",
      "\n",
      "```python\n",
      "@property\n",
      "def use_docker() -> Union[bool, str]\n",
      "```\n",
      "\n",
      "bool value of whether to use docker to execute the code,\n",
      "or str value of the docker image name to use.\n",
      "\n",
      "#### execute\\_code\n",
      "\n",
      "```python\n",
      "def execute_code(code_blocks)\n",
      "```\n",
      "\n",
      "Execute the code and return the result.\n",
      "\n",
      "#### auto\\_reply\n",
      "\n",
      "```python\n",
      "def auto_reply(message: dict, sender, default_reply=\"\")\n",
      "```\n",
      "\n",
      "Generate an auto reply.\n",
      "\n",
      "#### receive\n",
      "\n",
      "```python\n",
      "def receive(message: Union[Dict, str], sender)\n",
      "```\n",
      "\n",
      "Receive a message from the sender agent.\n",
      "Once a message is received, this function sends a reply to the sender or simply stop.\n",
      "The reply can be generated automatically or entered manually by a human.\n",
      "\n",
      "#### generate\\_init\\_prompt\n",
      "\n",
      "```python\n",
      "def generate_init_prompt(*args, **kwargs) -> Union[str, Dict]\n",
      "```\n",
      "\n",
      "Generate the initial prompt for the agent.\n",
      "\n",
      "Override this function to customize the initial prompt based on user's request.\n",
      "\n",
      "#### initiate\\_chat\n",
      "\n",
      "```python\n",
      "def initiate_chat(recipient, *args, **kwargs)\n",
      "```\n",
      "\n",
      "Initiate a chat with the receiver agent.\n",
      "\n",
      "`generate_init_prompt` is called to generate the initial prompt for the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `receiver` - the receiver agent.\n",
      "- `*args` - any additional arguments.\n",
      "- `**kwargs` - any additional keyword arguments.\n",
      "\n",
      "#### register\\_function\n",
      "\n",
      "```python\n",
      "def register_function(function_map: Dict[str, Callable])\n",
      "```\n",
      "\n",
      "Register functions to the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `function_map` - a dictionary mapping function names to functions.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: cfo_cat\n",
      "title: tune.searcher.cfo_cat\n",
      "---\n",
      "\n",
      "## FLOW2Cat Objects\n",
      "\n",
      "```python\n",
      "class FLOW2Cat(FLOW2)\n",
      "```\n",
      "\n",
      "Local search algorithm optimized for categorical variables.\n",
      "\n",
      "## CFOCat Objects\n",
      "\n",
      "```python\n",
      "class CFOCat(CFO)\n",
      "```\n",
      "\n",
      "CFO optimized for categorical variables.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "FLAML (Fast Lightweight AutoML) is an open-source Python package created by Chi Wang, Qingyun Wu, Qiang Dai, Yu Wang, and Jinrong Wu from Microsoft Corporation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "qa_problem = \"Who is the author of FLAML?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=qa_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "Use RetrieveChat to help generate sample code and ask for human-in-loop feedbacks.\n",
    "\n",
    "Problem: how to build a time series forecasting model for stock price using FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_37', 'doc_35', 'doc_46', 'doc_40', 'doc_49', 'doc_51', 'doc_58', 'doc_48', 'doc_14', 'doc_39', 'doc_47', 'doc_41', 'doc_13', 'doc_60', 'doc_52', 'doc_59', 'doc_43', 'doc_10', 'doc_34', 'doc_33']]\n",
      "Adding doc_id doc_37 to context.\n",
      "Adding doc_id doc_35 to context.\n",
      "Adding doc_id doc_46 to context.\n",
      "Adding doc_id doc_40 to context.\n",
      "Adding doc_id doc_49 to context.\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user. You should follow the following steps to answer a question:\n",
      "Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
      "a QA task.\n",
      "Step 2, you generate code or answer the question based on the intent.\n",
      "You should leverage the context provided by the user as much as possible. If you think the context is not enough, you\n",
      "can reply exactly \"UPDATE CONTEXT\" to ask the user to provide more contexts.\n",
      "For code generation, you must obey the following rules:\n",
      "You MUST NOT install any packages because all the packages needed are already installed.\n",
      "The code will be executed in IPython, you must follow the formats below to write your code:\n",
      "```python\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: how to build a time series forecasting model for stock price using FLAML?\n",
      "\n",
      "Context is: \n",
      "- `log_file_name` - A string of the log file name.\n",
      "- `X_train` - A numpy array or dataframe of training data in shape n*m.\n",
      "  For time series forecast tasks, the first column of X_train must be the timestamp column (datetime type). Other columns in the dataframe are assumed to be exogenous variables (categorical or numeric).\n",
      "- `y_train` - A numpy array or series of labels in shape n*1.\n",
      "- `dataframe` - A dataframe of training data including label column.\n",
      "  For time series forecast tasks, dataframe must be specified and should\n",
      "  have at least two columns: timestamp and label, where the first\n",
      "  column is the timestamp column (datetime type). Other columns\n",
      "  in the dataframe are assumed to be exogenous variables\n",
      "  (categorical or numeric).\n",
      "- `label` - A str of the label column name, e.g., 'label';\n",
      "- `Note` - If X_train and y_train are provided,\n",
      "  dataframe and label are ignored;\n",
      "  If not, dataframe and label must be provided.\n",
      "- `time_budget` - A float number of the time budget in seconds.\n",
      "- `task` - A string of the task type, e.g.,\n",
      "  'classification', 'regression', 'ts_forecast', 'rank',\n",
      "  'seq-classification', 'seq-regression', 'summarization',\n",
      "  or an instance of Task class.\n",
      "- `eval_method` - A string of resampling strategy, one of\n",
      "  ['auto', 'cv', 'holdout'].\n",
      "- `split_ratio` - A float of the validation data percentage for holdout.\n",
      "- `n_splits` - An integer of the number of folds for cross-validation.\n",
      "- `split_type` - str or splitter object, default=\"auto\" | the data split type.\n",
      "  * A valid splitter object is an instance of a derived class of scikit-learn\n",
      "  [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
      "  and have ``split`` and ``get_n_splits`` methods with the same signatures.\n",
      "  Set eval_method to \"cv\" to use the splitter object.\n",
      "  * Valid str options depend on different tasks.\n",
      "  For classification tasks, valid choices are\n",
      "  [\"auto\", 'stratified', 'uniform', 'time', 'group']. \"auto\" -> stratified.\n",
      "  For regression tasks, valid choices are [\"auto\", 'uniform', 'time'].\n",
      "  \"auto\" -> uniform.\n",
      "  For time series forecast tasks, must be \"auto\" or 'time'.\n",
      "  For ranking task, must be \"auto\" or 'group'.\n",
      "- `groups` - None or array-like | Group labels (with matching length to\n",
      "  y_train) or groups counts (with sum equal to length of y_train)\n",
      "  for training data.\n",
      "- `n_jobs` - An integer of the number of threads for training | default=-1.\n",
      "  Use all available resources when n_jobs == -1.\n",
      "- `train_best` - A boolean of whether to train the best config in the\n",
      "  time budget; if false, train the last config in the budget.\n",
      "- `train_full` - A boolean of whether to train on the full data. If true,\n",
      "  eval_method and sample_size in the log file will be ignored.\n",
      "- `record_id` - the ID of the training log record from which the model will\n",
      "  be retrained. By default `record_id = -1` which means this will be\n",
      "  ignored. `record_id = 0` corresponds to the first trial, and\n",
      "  when `record_id >= 0`, `time_budget` will be ignored.\n",
      "- `auto_augment` - boolean, default=True | Whether to automatically\n",
      "  augment rare classes.\n",
      "- `custom_hp` - dict, default=None | The custom search space specified by user\n",
      "  Each key is the estimator name, each value is a dict of the custom search space for that estimator. Notice the\n",
      "  domain of the custom search space can either be a value or a sample.Domain object.\n",
      "  \n",
      "```python\n",
      "custom_hp = {\n",
      "    \"transformer_ms\": {\n",
      "        \"model_path\": {\n",
      "            \"domain\": \"albert-base-v2\",\n",
      "        },\n",
      "        \"learning_rate\": {\n",
      "            \"domain\": tune.choice([1e-4, 1e-5]),\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "- `fit_kwargs_by_estimator` - dict, default=None | The user specified keywords arguments, grouped by estimator name.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "fit_kwargs_by_estimator = {\n",
      "    \"transformer\": {\n",
      "        \"output_dir\": \"test/data/output/\",\n",
      "        \"fp16\": False,\n",
      "    }\n",
      "}\n",
      "```\n",
      "  \n",
      "- `**fit_kwargs` - Other key word arguments to pass to fit() function of\n",
      "  the searched learners, such as sample_weight. Below are a few examples of\n",
      "  estimator-specific parameters:\n",
      "- `period` - int | forecast horizon for all time series forecast tasks.\n",
      "- `gpu_per_trial` - float, default = 0 | A float of the number of gpus per trial,\n",
      "  only used by TransformersEstimator, XGBoostSklearnEstimator, and\n",
      "  TemporalFusionTransformerEstimator.\n",
      "- `group_ids` - list of strings of column names identifying a time series, only\n",
      "  used by TemporalFusionTransformerEstimator, required for\n",
      "  'ts_forecast_panel' task. `group_ids` is a parameter for TimeSeriesDataSet object\n",
      "  from PyTorchForecasting.\n",
      "  For other parameters to describe your dataset, refer to\n",
      "  [TimeSeriesDataSet PyTorchForecasting](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.data.timeseries.TimeSeriesDataSet.html).\n",
      "  To specify your variables, use `static_categoricals`, `static_reals`,\n",
      "  `time_varying_known_categoricals`, `time_varying_known_reals`,\n",
      "  `time_varying_unknown_categoricals`, `time_varying_unknown_reals`,\n",
      "  `variable_groups`. To provide more information on your data, use\n",
      "  `max_encoder_length`, `min_encoder_length`, `lags`.\n",
      "- `log_dir` - str, default = \"lightning_logs\" | Folder into which to log results\n",
      "  for tensorboard, only used by TemporalFusionTransformerEstimator.\n",
      "- `max_epochs` - int, default = 20 | Maximum number of epochs to run training,\n",
      "  only used by TemporalFusionTransformerEstimator.\n",
      "- `batch_size` - int, default = 64 | Batch size for training model, only\n",
      "  used by TemporalFusionTransformerEstimator.\n",
      "\n",
      "#### search\\_space\n",
      "\n",
      "```python\n",
      "@property\n",
      "def search_space() -> dict\n",
      "```\n",
      "\n",
      "Search space.\n",
      "\n",
      "Must be called after fit(...)\n",
      "(use max_iter=0 and retrain_final=False to prevent actual fitting).\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  A dict of the search space.\n",
      "\n",
      "#### low\\_cost\\_partial\\_config\n",
      "\n",
      "```python\n",
      "@property\n",
      "def low_cost_partial_config() -> dict\n",
      "```\n",
      "\n",
      "Low cost partial config.\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  A dict.\n",
      "  (a) if there is only one estimator in estimator_list, each key is a\n",
      "  hyperparameter name.\n",
      "  (b) otherwise, it is a nested dict with 'ml' as the key, and\n",
      "  a list of the low_cost_partial_configs as the value, corresponding\n",
      "  to each learner's low_cost_partial_config; the estimator index as\n",
      "  an integer corresponding to the cheapest learner is appended to the\n",
      "  list at the end.\n",
      "\n",
      "#### cat\\_hp\\_cost\n",
      "\n",
      "```python\n",
      "@property\n",
      "def cat_hp_cost() -> dict\n",
      "```\n",
      "\n",
      "Categorical hyperparameter cost\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  A dict.\n",
      "  (a) if there is only one estimator in estimator_list, each key is a\n",
      "  hyperparameter name.\n",
      "  (b) otherwise, it is a nested dict with 'ml' as the key, and\n",
      "  a list of the cat_hp_cost's as the value, corresponding\n",
      "  to each learner's cat_hp_cost; the cost relative to lgbm for each\n",
      "  learner (as a list itself) is appended to the list at the end.\n",
      "\n",
      "#### points\\_to\\_evaluate\n",
      "\n",
      "```python\n",
      "@property\n",
      "def points_to_evaluate() -> dict\n",
      "```\n",
      "\n",
      "Initial points to evaluate.\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  A list of dicts. Each dict is the initial point for each learner.\n",
      "\n",
      "#### resource\\_attr\n",
      "\n",
      "```python\n",
      "@property\n",
      "def resource_attr() -> Optional[str]\n",
      "```\n",
      "\n",
      "Attribute of the resource dimension.\n",
      "\n",
      "new_automl = AutoML()\n",
      "new_automl.fit(X_train, y_train, starting_points=starting_points)\n",
      "```\n",
      "  \n",
      "- `seed` - int or None, default=None | The random seed for hpo.\n",
      "- `n_concurrent_trials` - [Experimental] int, default=1 | The number of\n",
      "  concurrent trials. When n_concurrent_trials > 1, flaml performes\n",
      "  [parallel tuning](../../Use-Cases/Task-Oriented-AutoML#parallel-tuning)\n",
      "  and installation of ray or spark is required: `pip install flaml[ray]`\n",
      "  or `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark.\n",
      "- `keep_search_state` - boolean, default=False | Whether to keep data needed\n",
      "  for model search after fit(). By default the state is deleted for\n",
      "  space saving.\n",
      "- `preserve_checkpoint` - boolean, default=True | Whether to preserve the saved checkpoint\n",
      "  on disk when deleting automl. By default the checkpoint is preserved.\n",
      "- `early_stop` - boolean, default=False | Whether to stop early if the\n",
      "  search is considered to converge.\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel Spark jobs if the\n",
      "  search time exceeded the time budget.\n",
      "- `append_log` - boolean, default=False | Whetehr to directly append the log\n",
      "  records to the input log file if it exists.\n",
      "- `auto_augment` - boolean, default=True | Whether to automatically\n",
      "  augment rare classes.\n",
      "- `min_sample_size` - int, default=MIN_SAMPLE_TRAIN | the minimal sample\n",
      "  size when sample=True.\n",
      "- `use_ray` - boolean or dict.\n",
      "  If boolean: default=False | Whether to use ray to run the training\n",
      "  in separate processes. This can be used to prevent OOM for large\n",
      "  datasets, but will incur more overhead in time.\n",
      "  If dict: the dict contains the keywords arguments to be passed to\n",
      "  [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).\n",
      "- `use_spark` - boolean, default=False | Whether to use spark to run the training\n",
      "  in parallel spark jobs. This can be used to accelerate training on large models\n",
      "  and large datasets, but will incur more overhead in time and thus slow down\n",
      "  training in some cases. GPU training is not supported yet when use_spark is True.\n",
      "  For Spark clusters, by default, we will launch one trial per executor. However,\n",
      "  sometimes we want to launch more trials than the number of executors (e.g., local mode).\n",
      "  In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override\n",
      "  the detected `num_executors`. The final number of concurrent trials will be the minimum\n",
      "  of `n_concurrent_trials` and `num_executors`.\n",
      "- `free_mem_ratio` - float between 0 and 1, default=0. The free memory ratio to keep during training.\n",
      "- `metric_constraints` - list, default=[] | The list of metric constraints.\n",
      "  Each element in this list is a 3-tuple, which shall be expressed\n",
      "  in the following format: the first element of the 3-tuple is the name of the\n",
      "  metric, the second element is the inequality sign chosen from \">=\" and \"<=\",\n",
      "  and the third element is the constraint value. E.g., `('val_loss', '<=', 0.1)`.\n",
      "  Note that all the metric names in metric_constraints need to be reported via\n",
      "  the metrics_to_log dictionary returned by a customized metric function.\n",
      "  The customized metric function shall be provided via the `metric` key word\n",
      "  argument of the fit() function or the automl constructor.\n",
      "  Find an example in the 4th constraint type in this [doc](../../Use-Cases/Task-Oriented-AutoML#constraint).\n",
      "  If `pred_time_limit` is provided as one of keyword arguments to fit() function or\n",
      "  the automl constructor, flaml will automatically (and under the hood)\n",
      "  add it as an additional element in the metric_constraints. Essentially 'pred_time_limit'\n",
      "  specifies a constraint about the prediction latency constraint in seconds.\n",
      "- `custom_hp` - dict, default=None | The custom search space specified by user.\n",
      "  It is a nested dict with keys being the estimator names, and values being dicts\n",
      "  per estimator search space. In the per estimator search space dict,\n",
      "  the keys are the hyperparameter names, and values are dicts of info (\"domain\",\n",
      "  \"init_value\", and \"low_cost_init_value\") about the search space associated with\n",
      "  the hyperparameter (i.e., per hyperparameter search space dict). When custom_hp\n",
      "  is provided, the built-in search space which is also a nested dict of per estimator\n",
      "  search space dict, will be updated with custom_hp. Note that during this nested dict update,\n",
      "  the per hyperparameter search space dicts will be replaced (instead of updated) by the ones\n",
      "  provided in custom_hp. Note that the value for \"domain\" can either be a constant\n",
      "  or a sample.Domain object.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "custom_hp = {\n",
      "     \"transformer_ms\": {\n",
      "         \"model_path\": {\n",
      "             \"domain\": \"albert-base-v2\",\n",
      "         },\n",
      "         \"learning_rate\": {\n",
      "             \"domain\": tune.choice([1e-4, 1e-5]),\n",
      "         }\n",
      "     }\n",
      " }\n",
      "```\n",
      "- `skip_transform` - boolean, default=False | Whether to pre-process data prior to modeling.\n",
      "- `fit_kwargs_by_estimator` - dict, default=None | The user specified keywords arguments, grouped by estimator name.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "fit_kwargs_by_estimator = {\n",
      "    \"transformer\": {\n",
      "        \"output_dir\": \"test/data/output/\",\n",
      "        \"fp16\": False,\n",
      "    }\n",
      "}\n",
      "```\n",
      "- `mlflow_logging` - boolean, default=True | Whether to log the training results to mlflow.\n",
      "  This requires mlflow to be installed and to have an active mlflow run.\n",
      "  FLAML will create nested runs.\n",
      "\n",
      "#### config\\_history\n",
      "\n",
      "```python\n",
      "@property\n",
      "def config_history() -> dict\n",
      "```\n",
      "\n",
      "A dictionary of iter->(estimator, config, time),\n",
      "storing the best estimator, config, and the time when the best\n",
      "model is updated each time.\n",
      "\n",
      "#### model\n",
      "\n",
      "```python\n",
      "@property\n",
      "def model()\n",
      "```\n",
      "\n",
      "An object with `predict()` and `predict_proba()` method (for\n",
      "classification), storing the best trained model.\n",
      "\n",
      "#### best\\_model\\_for\\_estimator\n",
      "\n",
      "```python\n",
      "def best_model_for_estimator(estimator_name: str)\n",
      "```\n",
      "\n",
      "Return the best model found for a particular estimator.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `estimator_name` - a str of the estimator's name.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  An object storing the best model for estimator_name.\n",
      "  If `model_history` was set to False during fit(), then the returned model\n",
      "  is untrained unless estimator_name is the best estimator.\n",
      "  If `model_history` was set to True, then the returned model is trained.\n",
      "\n",
      "#### best\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_estimator()\n",
      "```\n",
      "\n",
      "A string indicating the best estimator found.\n",
      "\n",
      "#### best\\_iteration\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_iteration()\n",
      "```\n",
      "\n",
      "An integer of the iteration number where the best\n",
      "config is found.\n",
      "\n",
      "#### best\\_config\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config()\n",
      "```\n",
      "\n",
      "A dictionary of the best configuration.\n",
      "\n",
      "#### best\\_config\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best configuration.\n",
      "\n",
      "#### best\\_loss\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best loss.\n",
      "\n",
      "#### best\\_loss\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss()\n",
      "```\n",
      "\n",
      "A float of the best loss found.\n",
      "\n",
      "#### best\\_result\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_result()\n",
      "```\n",
      "\n",
      "Result dictionary for model trained with the best config.\n",
      "\n",
      "#### metrics\\_for\\_best\\_config\n",
      "---\n",
      "sidebar_label: ts_model\n",
      "title: automl.time_series.ts_model\n",
      "---\n",
      "\n",
      "## Prophet Objects\n",
      "\n",
      "```python\n",
      "class Prophet(TimeSeriesEstimator)\n",
      "```\n",
      "\n",
      "The class for tuning Prophet.\n",
      "\n",
      "## ARIMA Objects\n",
      "\n",
      "```python\n",
      "class ARIMA(StatsModelsEstimator)\n",
      "```\n",
      "\n",
      "The class for tuning ARIMA.\n",
      "\n",
      "## SARIMAX Objects\n",
      "\n",
      "```python\n",
      "class SARIMAX(StatsModelsEstimator)\n",
      "```\n",
      "\n",
      "The class for tuning SARIMA.\n",
      "\n",
      "## HoltWinters Objects\n",
      "\n",
      "```python\n",
      "class HoltWinters(StatsModelsEstimator)\n",
      "```\n",
      "\n",
      "The class for tuning Holt Winters model, aka 'Triple Exponential Smoothing'.\n",
      "\n",
      "## TS\\_SKLearn Objects\n",
      "\n",
      "```python\n",
      "class TS_SKLearn(TimeSeriesEstimator)\n",
      "```\n",
      "\n",
      "The class for tuning SKLearn Regressors for time-series forecasting\n",
      "\n",
      "## LGBM\\_TS Objects\n",
      "\n",
      "```python\n",
      "class LGBM_TS(TS_SKLearn)\n",
      "```\n",
      "\n",
      "The class for tuning LGBM Regressor for time-series forecasting\n",
      "\n",
      "## XGBoost\\_TS Objects\n",
      "\n",
      "```python\n",
      "class XGBoost_TS(TS_SKLearn)\n",
      "```\n",
      "\n",
      "The class for tuning XGBoost Regressor for time-series forecasting\n",
      "\n",
      "## RF\\_TS Objects\n",
      "\n",
      "```python\n",
      "class RF_TS(TS_SKLearn)\n",
      "```\n",
      "\n",
      "The class for tuning Random Forest Regressor for time-series forecasting\n",
      "\n",
      "## ExtraTrees\\_TS Objects\n",
      "\n",
      "```python\n",
      "class ExtraTrees_TS(TS_SKLearn)\n",
      "```\n",
      "\n",
      "The class for tuning Extra Trees Regressor for time-series forecasting\n",
      "\n",
      "## XGBoostLimitDepth\\_TS Objects\n",
      "\n",
      "```python\n",
      "class XGBoostLimitDepth_TS(TS_SKLearn)\n",
      "```\n",
      "\n",
      "The class for tuning XGBoost Regressor with unlimited depth for time-series forecasting\n",
      "\n",
      "\n",
      "\n",
      "new_automl = AutoML()\n",
      "new_automl.fit(X_train, y_train, starting_points=starting_points)\n",
      "```\n",
      "  \n",
      "- `seed` - int or None, default=None | The random seed for hpo.\n",
      "- `n_concurrent_trials` - [Experimental] int, default=1 | The number of\n",
      "  concurrent trials. When n_concurrent_trials > 1, flaml performes\n",
      "  [parallel tuning](../../Use-Cases/Task-Oriented-AutoML#parallel-tuning)\n",
      "  and installation of ray or spark is required: `pip install flaml[ray]`\n",
      "  or `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark.\n",
      "- `keep_search_state` - boolean, default=False | Whether to keep data needed\n",
      "  for model search after fit(). By default the state is deleted for\n",
      "  space saving.\n",
      "- `preserve_checkpoint` - boolean, default=True | Whether to preserve the saved checkpoint\n",
      "  on disk when deleting automl. By default the checkpoint is preserved.\n",
      "- `early_stop` - boolean, default=False | Whether to stop early if the\n",
      "  search is considered to converge.\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel the PySpark job if overtime.\n",
      "- `append_log` - boolean, default=False | Whetehr to directly append the log\n",
      "  records to the input log file if it exists.\n",
      "- `auto_augment` - boolean, default=True | Whether to automatically\n",
      "  augment rare classes.\n",
      "- `min_sample_size` - int, default=MIN_SAMPLE_TRAIN | the minimal sample\n",
      "  size when sample=True.\n",
      "- `use_ray` - boolean or dict.\n",
      "  If boolean: default=False | Whether to use ray to run the training\n",
      "  in separate processes. This can be used to prevent OOM for large\n",
      "  datasets, but will incur more overhead in time.\n",
      "  If dict: the dict contains the keywords arguments to be passed to\n",
      "  [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).\n",
      "- `use_spark` - boolean, default=False | Whether to use spark to run the training\n",
      "  in parallel spark jobs. This can be used to accelerate training on large models\n",
      "  and large datasets, but will incur more overhead in time and thus slow down\n",
      "  training in some cases.\n",
      "- `free_mem_ratio` - float between 0 and 1, default=0. The free memory ratio to keep during training.\n",
      "- `metric_constraints` - list, default=[] | The list of metric constraints.\n",
      "  Each element in this list is a 3-tuple, which shall be expressed\n",
      "  in the following format: the first element of the 3-tuple is the name of the\n",
      "  metric, the second element is the inequality sign chosen from \">=\" and \"<=\",\n",
      "  and the third element is the constraint value. E.g., `('precision', '>=', 0.9)`.\n",
      "  Note that all the metric names in metric_constraints need to be reported via\n",
      "  the metrics_to_log dictionary returned by a customized metric function.\n",
      "  The customized metric function shall be provided via the `metric` key word argument\n",
      "  of the fit() function or the automl constructor.\n",
      "  Find examples in this [test](https://github.com/microsoft/FLAML/tree/main/test/automl/test_constraints.py).\n",
      "  If `pred_time_limit` is provided as one of keyword arguments to fit() function or\n",
      "  the automl constructor, flaml will automatically (and under the hood)\n",
      "  add it as an additional element in the metric_constraints. Essentially 'pred_time_limit'\n",
      "  specifies a constraint about the prediction latency constraint in seconds.\n",
      "- `custom_hp` - dict, default=None | The custom search space specified by user\n",
      "  Each key is the estimator name, each value is a dict of the custom search space for that estimator. Notice the\n",
      "  domain of the custom search space can either be a value of a sample.Domain object.\n",
      "  \n",
      "  \n",
      "  \n",
      "```python\n",
      "custom_hp = {\n",
      "    \"transformer_ms\": {\n",
      "        \"model_path\": {\n",
      "            \"domain\": \"albert-base-v2\",\n",
      "        },\n",
      "        \"learning_rate\": {\n",
      "            \"domain\": tune.choice([1e-4, 1e-5]),\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "- `time_col` - for a time series task, name of the column containing the timestamps. If not\n",
      "  provided, defaults to the first column of X_train/X_val\n",
      "  \n",
      "- `cv_score_agg_func` - customized cross-validation scores aggregate function. Default to average metrics across folds. If specificed, this function needs to\n",
      "  have the following input arguments:\n",
      "  \n",
      "  * val_loss_folds: list of floats, the loss scores of each fold;\n",
      "  * log_metrics_folds: list of dicts/floats, the metrics of each fold to log.\n",
      "  \n",
      "  This function should return the final aggregate result of all folds. A float number of the minimization objective, and a dictionary as the metrics to log or None.\n",
      "  E.g.,\n",
      "  \n",
      "```python\n",
      "def cv_score_agg_func(val_loss_folds, log_metrics_folds):\n",
      "    metric_to_minimize = sum(val_loss_folds)/len(val_loss_folds)\n",
      "    metrics_to_log = None\n",
      "    for single_fold in log_metrics_folds:\n",
      "        if metrics_to_log is None:\n",
      "            metrics_to_log = single_fold\n",
      "        elif isinstance(metrics_to_log, dict):\n",
      "            metrics_to_log = {k: metrics_to_log[k] + v for k, v in single_fold.items()}\n",
      "        else:\n",
      "            metrics_to_log += single_fold\n",
      "    if metrics_to_log:\n",
      "        n = len(val_loss_folds)\n",
      "        metrics_to_log = (\n",
      "            {k: v / n for k, v in metrics_to_log.items()}\n",
      "            if isinstance(metrics_to_log, dict)\n",
      "            else metrics_to_log / n\n",
      "        )\n",
      "    return metric_to_minimize, metrics_to_log\n",
      "```\n",
      "  \n",
      "- `skip_transform` - boolean, default=False | Whether to pre-process data prior to modeling.\n",
      "- `mlflow_logging` - boolean, default=None | Whether to log the training results to mlflow.\n",
      "  Default value is None, which means the logging decision is made based on\n",
      "  AutoML.__init__'s mlflow_logging argument.\n",
      "  This requires mlflow to be installed and to have an active mlflow run.\n",
      "  FLAML will create nested runs.\n",
      "- `fit_kwargs_by_estimator` - dict, default=None | The user specified keywords arguments, grouped by estimator name.\n",
      "  For TransformersEstimator, available fit_kwargs can be found from\n",
      "  [TrainingArgumentsForAuto](nlp/huggingface/training_args).\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "fit_kwargs_by_estimator = {\n",
      "    \"transformer\": {\n",
      "        \"output_dir\": \"test/data/output/\",\n",
      "        \"fp16\": False,\n",
      "    },\n",
      "    \"tft\": {\n",
      "        \"max_encoder_length\": 1,\n",
      "        \"min_encoder_length\": 1,\n",
      "        \"static_categoricals\": [],\n",
      "        \"static_reals\": [],\n",
      "        \"time_varying_known_categoricals\": [],\n",
      "        \"time_varying_known_reals\": [],\n",
      "        \"time_varying_unknown_categoricals\": [],\n",
      "        \"time_varying_unknown_reals\": [],\n",
      "        \"variable_groups\": {},\n",
      "        \"lags\": {},\n",
      "    }\n",
      "}\n",
      "```\n",
      "  \n",
      "- `**fit_kwargs` - Other key word arguments to pass to fit() function of\n",
      "  the searched learners, such as sample_weight. Below are a few examples of\n",
      "  estimator-specific parameters:\n",
      "- `period` - int | forecast horizon for all time series forecast tasks.\n",
      "- `gpu_per_trial` - float, default = 0 | A float of the number of gpus per trial,\n",
      "  only used by TransformersEstimator, XGBoostSklearnEstimator, and\n",
      "  TemporalFusionTransformerEstimator.\n",
      "- `group_ids` - list of strings of column names identifying a time series, only\n",
      "  used by TemporalFusionTransformerEstimator, required for\n",
      "  'ts_forecast_panel' task. `group_ids` is a parameter for TimeSeriesDataSet object\n",
      "  from PyTorchForecasting.\n",
      "  For other parameters to describe your dataset, refer to\n",
      "  [TimeSeriesDataSet PyTorchForecasting](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.data.timeseries.TimeSeriesDataSet.html).\n",
      "  To specify your variables, use `static_categoricals`, `static_reals`,\n",
      "  `time_varying_known_categoricals`, `time_varying_known_reals`,\n",
      "  `time_varying_unknown_categoricals`, `time_varying_unknown_reals`,\n",
      "  `variable_groups`. To provide more information on your data, use\n",
      "  `max_encoder_length`, `min_encoder_length`, `lags`.\n",
      "- `log_dir` - str, default = \"lightning_logs\" | Folder into which to log results\n",
      "  for tensorboard, only used by TemporalFusionTransformerEstimator.\n",
      "- `max_epochs` - int, default = 20 | Maximum number of epochs to run training,\n",
      "  only used by TemporalFusionTransformerEstimator.\n",
      "- `batch_size` - int, default = 64 | Batch size for training model, only\n",
      "  used by TemporalFusionTransformerEstimator.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: ts_data\n",
      "title: automl.time_series.ts_data\n",
      "---\n",
      "\n",
      "## TimeSeriesDataset Objects\n",
      "\n",
      "```python\n",
      "@dataclass\n",
      "class TimeSeriesDataset()\n",
      "```\n",
      "\n",
      "#### to\\_univariate\n",
      "\n",
      "```python\n",
      "def to_univariate() -> Dict[str, \"TimeSeriesDataset\"]\n",
      "```\n",
      "\n",
      "Convert a multivariate TrainingData  to a dict of univariate ones\n",
      "@param df:\n",
      "@return:\n",
      "\n",
      "#### fourier\\_series\n",
      "\n",
      "```python\n",
      "def fourier_series(feature: pd.Series, name: str)\n",
      "```\n",
      "\n",
      "Assume feature goes from 0 to 1 cyclically, transform that into Fourier\n",
      "@param feature: input feature\n",
      "@return: sin(2pi*feature), cos(2pi*feature)\n",
      "\n",
      "## DataTransformerTS Objects\n",
      "\n",
      "```python\n",
      "class DataTransformerTS()\n",
      "```\n",
      "\n",
      "Transform input time series training data.\n",
      "\n",
      "#### fit\n",
      "\n",
      "```python\n",
      "def fit(X: Union[DataFrame, np.array], y)\n",
      "```\n",
      "\n",
      "Fit transformer.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `X` - A numpy array or a pandas dataframe of training data.\n",
      "- `y` - A numpy array or a pandas series of labels.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `X` - Processed numpy array or pandas dataframe of training data.\n",
      "- `y` - Processed numpy array or pandas series of labels.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "To build a time series forecasting model for stock price using FLAML, you can follow these steps:\n",
      "\n",
      "1. Install the FLAML library if you haven't already by running `pip install flaml`.\n",
      "\n",
      "2. Import the necessary packages:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from flaml import AutoML\n",
      "```\n",
      "\n",
      "3. Load your stock price data into a pandas DataFrame. The data should have at least two columns: timestamp and target variable (stock price). Make sure the timestamp column has datetime data type.\n",
      "\n",
      "```python\n",
      "data = pd.read_csv(\"your_stock_price_data.csv\")\n",
      "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
      "```\n",
      "\n",
      "4. Split the data into training and validation sets:\n",
      "\n",
      "```python\n",
      "train_data = data[data[\"timestamp\"] < \"cutoff_date\"]\n",
      "val_data = data[data[\"timestamp\"] >= \"cutoff_date\"]\n",
      "```\n",
      "\n",
      "Replace \"cutoff_date\" with the desired date in the format \"YYYY-MM-DD\".\n",
      "\n",
      "5. Initialize the FLAML AutoML instance with the necessary settings:\n",
      "\n",
      "```python\n",
      "automl = AutoML()\n",
      "```\n",
      "\n",
      "6. Fit the model to your data. FLAML supports multiple time-series forecasting models, including the following: 'fbprophet', 'arima', 'sarimax', 'holtwinters', 'lgbm_ts', 'xgboost_ts', 'rf_ts', 'extratrees_ts', and 'xgboost_limitdepth_ts'. In this example, I will use the default estimator_list, which includes 'fbprophet', 'arima', 'sarimax', and 'holtwinters':\n",
      "\n",
      "```python\n",
      "automl.fit(\n",
      "    X_train=train_data,\n",
      "    y_train=None,\n",
      "    dataframe=train_data,\n",
      "    label=\"stock_price\",  # replace this with the name of your target variable\n",
      "    task=\"ts_forecast\",\n",
      "    time_budget=600,  # set your desired time budget in seconds\n",
      ")\n",
      "```\n",
      "\n",
      "7. Make predictions using the best estimator found by FLAML:\n",
      "\n",
      "```python\n",
      "predictions = automl.predict(val_data)\n",
      "```\n",
      "\n",
      "The predictions will be in the form of a NumPy array.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/home/lijiang1/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/home/lijiang1/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/home/lijiang1/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_stock_price_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39myour_stock_price_data.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m data[\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(data[\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_stock_price_data.csv'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "None\n",
      "None\n",
      "[Errno 2] No such file or directory: 'your_stock_price_data.csv'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "None\n",
      "None\n",
      "[Errno 2] No such file or directory: 'your_stock_price_data.csv'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "It seems I cannot execute the code since I don't have access to your stock price data file. However, you can follow the same steps I provided above to build a time series forecasting model for stock price using FLAML on your local machine or your preferred environment.\n",
      "\n",
      "Please ensure you have the stock price data saved as a CSV file (\"your_stock_price_data.csv\") on your local machine. Change this file name in the code if needed and also adjust the \"cutoff_date\" to the desired date.\n",
      "\n",
      "If you have any questions or issues, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "# set `human_input_mode` to be `ALWAYS`, so the agent will ask for human input at every step.\n",
    "ragproxyagent.human_input_mode = \"ALWAYS\"\n",
    "code_problem = \"how to build a time series forecasting model for stock price using FLAML?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=code_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4\n",
    "\n",
    "Use RetrieveChat to answer a question and ask for human-in-loop feedbacks.\n",
    "\n",
    "Problem: Is there a function named `tune_automl` in FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_35', 'doc_14', 'doc_40', 'doc_13', 'doc_51', 'doc_58', 'doc_20', 'doc_26', 'doc_34', 'doc_22', 'doc_11', 'doc_59', 'doc_3', 'doc_56', 'doc_47', 'doc_53', 'doc_19', 'doc_28', 'doc_32', 'doc_46']]\n",
      "Adding doc_id doc_35 to context.\n",
      "Adding doc_id doc_14 to context.\n",
      "Adding doc_id doc_40 to context.\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user. You should follow the following steps to answer a question:\n",
      "Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
      "a QA task.\n",
      "Step 2, you generate code or answer the question based on the intent.\n",
      "You should leverage the context provided by the user as much as possible. If you think the context is not enough, you\n",
      "can reply exactly \"UPDATE CONTEXT\" to ask the user to provide more contexts.\n",
      "For code generation, you must obey the following rules:\n",
      "You MUST NOT install any packages because all the packages needed are already installed.\n",
      "The code will be executed in IPython, you must follow the formats below to write your code:\n",
      "```python\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Is there a function named `tune_automl` in FLAML?\n",
      "\n",
      "Context is: \n",
      "new_automl = AutoML()\n",
      "new_automl.fit(X_train, y_train, starting_points=starting_points)\n",
      "```\n",
      "  \n",
      "- `seed` - int or None, default=None | The random seed for hpo.\n",
      "- `n_concurrent_trials` - [Experimental] int, default=1 | The number of\n",
      "  concurrent trials. When n_concurrent_trials > 1, flaml performes\n",
      "  [parallel tuning](../../Use-Cases/Task-Oriented-AutoML#parallel-tuning)\n",
      "  and installation of ray or spark is required: `pip install flaml[ray]`\n",
      "  or `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark.\n",
      "- `keep_search_state` - boolean, default=False | Whether to keep data needed\n",
      "  for model search after fit(). By default the state is deleted for\n",
      "  space saving.\n",
      "- `preserve_checkpoint` - boolean, default=True | Whether to preserve the saved checkpoint\n",
      "  on disk when deleting automl. By default the checkpoint is preserved.\n",
      "- `early_stop` - boolean, default=False | Whether to stop early if the\n",
      "  search is considered to converge.\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel Spark jobs if the\n",
      "  search time exceeded the time budget.\n",
      "- `append_log` - boolean, default=False | Whetehr to directly append the log\n",
      "  records to the input log file if it exists.\n",
      "- `auto_augment` - boolean, default=True | Whether to automatically\n",
      "  augment rare classes.\n",
      "- `min_sample_size` - int, default=MIN_SAMPLE_TRAIN | the minimal sample\n",
      "  size when sample=True.\n",
      "- `use_ray` - boolean or dict.\n",
      "  If boolean: default=False | Whether to use ray to run the training\n",
      "  in separate processes. This can be used to prevent OOM for large\n",
      "  datasets, but will incur more overhead in time.\n",
      "  If dict: the dict contains the keywords arguments to be passed to\n",
      "  [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).\n",
      "- `use_spark` - boolean, default=False | Whether to use spark to run the training\n",
      "  in parallel spark jobs. This can be used to accelerate training on large models\n",
      "  and large datasets, but will incur more overhead in time and thus slow down\n",
      "  training in some cases. GPU training is not supported yet when use_spark is True.\n",
      "  For Spark clusters, by default, we will launch one trial per executor. However,\n",
      "  sometimes we want to launch more trials than the number of executors (e.g., local mode).\n",
      "  In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override\n",
      "  the detected `num_executors`. The final number of concurrent trials will be the minimum\n",
      "  of `n_concurrent_trials` and `num_executors`.\n",
      "- `free_mem_ratio` - float between 0 and 1, default=0. The free memory ratio to keep during training.\n",
      "- `metric_constraints` - list, default=[] | The list of metric constraints.\n",
      "  Each element in this list is a 3-tuple, which shall be expressed\n",
      "  in the following format: the first element of the 3-tuple is the name of the\n",
      "  metric, the second element is the inequality sign chosen from \">=\" and \"<=\",\n",
      "  and the third element is the constraint value. E.g., `('val_loss', '<=', 0.1)`.\n",
      "  Note that all the metric names in metric_constraints need to be reported via\n",
      "  the metrics_to_log dictionary returned by a customized metric function.\n",
      "  The customized metric function shall be provided via the `metric` key word\n",
      "  argument of the fit() function or the automl constructor.\n",
      "  Find an example in the 4th constraint type in this [doc](../../Use-Cases/Task-Oriented-AutoML#constraint).\n",
      "  If `pred_time_limit` is provided as one of keyword arguments to fit() function or\n",
      "  the automl constructor, flaml will automatically (and under the hood)\n",
      "  add it as an additional element in the metric_constraints. Essentially 'pred_time_limit'\n",
      "  specifies a constraint about the prediction latency constraint in seconds.\n",
      "- `custom_hp` - dict, default=None | The custom search space specified by user.\n",
      "  It is a nested dict with keys being the estimator names, and values being dicts\n",
      "  per estimator search space. In the per estimator search space dict,\n",
      "  the keys are the hyperparameter names, and values are dicts of info (\"domain\",\n",
      "  \"init_value\", and \"low_cost_init_value\") about the search space associated with\n",
      "  the hyperparameter (i.e., per hyperparameter search space dict). When custom_hp\n",
      "  is provided, the built-in search space which is also a nested dict of per estimator\n",
      "  search space dict, will be updated with custom_hp. Note that during this nested dict update,\n",
      "  the per hyperparameter search space dicts will be replaced (instead of updated) by the ones\n",
      "  provided in custom_hp. Note that the value for \"domain\" can either be a constant\n",
      "  or a sample.Domain object.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "custom_hp = {\n",
      "     \"transformer_ms\": {\n",
      "         \"model_path\": {\n",
      "             \"domain\": \"albert-base-v2\",\n",
      "         },\n",
      "         \"learning_rate\": {\n",
      "             \"domain\": tune.choice([1e-4, 1e-5]),\n",
      "         }\n",
      "     }\n",
      " }\n",
      "```\n",
      "- `skip_transform` - boolean, default=False | Whether to pre-process data prior to modeling.\n",
      "- `fit_kwargs_by_estimator` - dict, default=None | The user specified keywords arguments, grouped by estimator name.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "fit_kwargs_by_estimator = {\n",
      "    \"transformer\": {\n",
      "        \"output_dir\": \"test/data/output/\",\n",
      "        \"fp16\": False,\n",
      "    }\n",
      "}\n",
      "```\n",
      "- `mlflow_logging` - boolean, default=True | Whether to log the training results to mlflow.\n",
      "  This requires mlflow to be installed and to have an active mlflow run.\n",
      "  FLAML will create nested runs.\n",
      "\n",
      "#### config\\_history\n",
      "\n",
      "```python\n",
      "@property\n",
      "def config_history() -> dict\n",
      "```\n",
      "\n",
      "A dictionary of iter->(estimator, config, time),\n",
      "storing the best estimator, config, and the time when the best\n",
      "model is updated each time.\n",
      "\n",
      "#### model\n",
      "\n",
      "```python\n",
      "@property\n",
      "def model()\n",
      "```\n",
      "\n",
      "An object with `predict()` and `predict_proba()` method (for\n",
      "classification), storing the best trained model.\n",
      "\n",
      "#### best\\_model\\_for\\_estimator\n",
      "\n",
      "```python\n",
      "def best_model_for_estimator(estimator_name: str)\n",
      "```\n",
      "\n",
      "Return the best model found for a particular estimator.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `estimator_name` - a str of the estimator's name.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  An object storing the best model for estimator_name.\n",
      "  If `model_history` was set to False during fit(), then the returned model\n",
      "  is untrained unless estimator_name is the best estimator.\n",
      "  If `model_history` was set to True, then the returned model is trained.\n",
      "\n",
      "#### best\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_estimator()\n",
      "```\n",
      "\n",
      "A string indicating the best estimator found.\n",
      "\n",
      "#### best\\_iteration\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_iteration()\n",
      "```\n",
      "\n",
      "An integer of the iteration number where the best\n",
      "config is found.\n",
      "\n",
      "#### best\\_config\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config()\n",
      "```\n",
      "\n",
      "A dictionary of the best configuration.\n",
      "\n",
      "#### best\\_config\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_config_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best configuration.\n",
      "\n",
      "#### best\\_loss\\_per\\_estimator\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss_per_estimator()\n",
      "```\n",
      "\n",
      "A dictionary of all estimators' best loss.\n",
      "\n",
      "#### best\\_loss\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_loss()\n",
      "```\n",
      "\n",
      "A float of the best loss found.\n",
      "\n",
      "#### best\\_result\n",
      "\n",
      "```python\n",
      "@property\n",
      "def best_result()\n",
      "```\n",
      "\n",
      "Result dictionary for model trained with the best config.\n",
      "\n",
      "#### metrics\\_for\\_best\\_config\n",
      "from flaml import BlendSearch\n",
      "algo = BlendSearch(metric='val_loss', mode='min',\n",
      "        space=search_space,\n",
      "        low_cost_partial_config=low_cost_partial_config)\n",
      "for i in range(10):\n",
      "    analysis = tune.run(compute_with_config,\n",
      "        search_alg=algo, use_ray=False)\n",
      "    print(analysis.trials[-1].last_result)\n",
      "```\n",
      "  \n",
      "- `verbose` - 0, 1, 2, or 3. If ray or spark backend is used, their verbosity will be\n",
      "  affected by this argument. 0 = silent, 1 = only status updates,\n",
      "  2 = status and brief trial results, 3 = status and detailed trial results.\n",
      "  Defaults to 2.\n",
      "- `local_dir` - A string of the local dir to save ray logs if ray backend is\n",
      "  used; or a local dir to save the tuning log.\n",
      "- `num_samples` - An integer of the number of configs to try. Defaults to 1.\n",
      "- `resources_per_trial` - A dictionary of the hardware resources to allocate\n",
      "  per trial, e.g., `{'cpu': 1}`. It is only valid when using ray backend\n",
      "  (by setting 'use_ray = True'). It shall be used when you need to do\n",
      "  [parallel tuning](../../Use-Cases/Tune-User-Defined-Function#parallel-tuning).\n",
      "- `config_constraints` - A list of config constraints to be satisfied.\n",
      "  e.g., ```config_constraints = [(mem_size, '<=', 1024**3)]```\n",
      "  \n",
      "  mem_size is a function which produces a float number for the bytes\n",
      "  needed for a config.\n",
      "  It is used to skip configs which do not fit in memory.\n",
      "- `metric_constraints` - A list of metric constraints to be satisfied.\n",
      "  e.g., `['precision', '>=', 0.9]`. The sign can be \">=\" or \"<=\".\n",
      "- `max_failure` - int | the maximal consecutive number of failures to sample\n",
      "  a trial before the tuning is terminated.\n",
      "- `use_ray` - A boolean of whether to use ray as the backend.\n",
      "- `use_spark` - A boolean of whether to use spark as the backend.\n",
      "- `log_file_name` - A string of the log file name. Default to None.\n",
      "  When set to None:\n",
      "  if local_dir is not given, no log file is created;\n",
      "  if local_dir is given, the log file name will be autogenerated under local_dir.\n",
      "  Only valid when verbose > 0 or use_ray is True.\n",
      "- `lexico_objectives` - dict, default=None | It specifics information needed to perform multi-objective\n",
      "  optimization with lexicographic preferences. When lexico_objectives is not None, the arguments metric,\n",
      "  mode, will be invalid, and flaml's tune uses CFO\n",
      "  as the `search_alg`, which makes the input (if provided) `search_alg' invalid.\n",
      "  This dictionary shall contain the following fields of key-value pairs:\n",
      "  - \"metrics\":  a list of optimization objectives with the orders reflecting the priorities/preferences of the\n",
      "  objectives.\n",
      "  - \"modes\" (optional): a list of optimization modes (each mode either \"min\" or \"max\") corresponding to the\n",
      "  objectives in the metric list. If not provided, we use \"min\" as the default mode for all the objectives.\n",
      "  - \"targets\" (optional): a dictionary to specify the optimization targets on the objectives. The keys are the\n",
      "  metric names (provided in \"metric\"), and the values are the numerical target values.\n",
      "  - \"tolerances\" (optional): a dictionary to specify the optimality tolerances on objectives. The keys are the metric names (provided in \"metrics\"), and the values are the absolute/percentage tolerance in the form of numeric/string.\n",
      "  E.g.,\n",
      "```python\n",
      "lexico_objectives = {\n",
      "    \"metrics\": [\"error_rate\", \"pred_time\"],\n",
      "    \"modes\": [\"min\", \"min\"],\n",
      "    \"tolerances\": {\"error_rate\": 0.01, \"pred_time\": 0.0},\n",
      "    \"targets\": {\"error_rate\": 0.0},\n",
      "}\n",
      "```\n",
      "  We also support percentage tolerance.\n",
      "  E.g.,\n",
      "```python\n",
      "lexico_objectives = {\n",
      "    \"metrics\": [\"error_rate\", \"pred_time\"],\n",
      "    \"modes\": [\"min\", \"min\"],\n",
      "    \"tolerances\": {\"error_rate\": \"5%\", \"pred_time\": \"0%\"},\n",
      "    \"targets\": {\"error_rate\": 0.0},\n",
      "}\n",
      "```\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel the PySpark job if overtime.\n",
      "- `n_concurrent_trials` - int, default=0 | The number of concurrent trials when perform hyperparameter\n",
      "  tuning with Spark. Only valid when use_spark=True and spark is required:\n",
      "  `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark. When tune.run() is called from AutoML, it will be\n",
      "  overwritten by the value of `n_concurrent_trials` in AutoML. When <= 0, the concurrent trials\n",
      "  will be set to the number of executors.\n",
      "- `**ray_args` - keyword arguments to pass to ray.tune.run().\n",
      "  Only valid when use_ray=True.\n",
      "\n",
      "## Tuner Objects\n",
      "\n",
      "```python\n",
      "class Tuner()\n",
      "```\n",
      "\n",
      "Tuner is the class-based way of launching hyperparameter tuning jobs compatible with Ray Tune 2.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `trainable` - A user-defined evaluation function.\n",
      "  It takes a configuration as input, outputs a evaluation\n",
      "  result (can be a numerical value or a dictionary of string\n",
      "  and numerical value pairs) for the input configuration.\n",
      "  For machine learning tasks, it usually involves training and\n",
      "  scoring a machine learning model, e.g., through validation loss.\n",
      "- `param_space` - Search space of the tuning job.\n",
      "  One thing to note is that both preprocessor and dataset can be tuned here.\n",
      "- `tune_config` - Tuning algorithm specific configs.\n",
      "  Refer to ray.tune.tune_config.TuneConfig for more info.\n",
      "- `run_config` - Runtime configuration that is specific to individual trials.\n",
      "  If passed, this will overwrite the run config passed to the Trainer,\n",
      "  if applicable. Refer to ray.air.config.RunConfig for more info.\n",
      "  \n",
      "  Usage pattern:\n",
      "  \n",
      "  .. code-block:: python\n",
      "  \n",
      "  from sklearn.datasets import load_breast_cancer\n",
      "  \n",
      "  from ray import tune\n",
      "  from ray.data import from_pandas\n",
      "  from ray.air.config import RunConfig, ScalingConfig\n",
      "  from ray.train.xgboost import XGBoostTrainer\n",
      "  from ray.tune.tuner import Tuner\n",
      "  \n",
      "  def get_dataset():\n",
      "  data_raw = load_breast_cancer(as_frame=True)\n",
      "  dataset_df = data_raw[\"data\"]\n",
      "  dataset_df[\"target\"] = data_raw[\"target\"]\n",
      "  dataset = from_pandas(dataset_df)\n",
      "  return dataset\n",
      "  \n",
      "  trainer = XGBoostTrainer(\n",
      "  label_column=\"target\",\n",
      "  params={},\n",
      "- `datasets={\"train\"` - get_dataset()},\n",
      "  )\n",
      "  \n",
      "  param_space = {\n",
      "- `\"scaling_config\"` - ScalingConfig(\n",
      "  num_workers=tune.grid_search([2, 4]),\n",
      "  resources_per_worker={\n",
      "- `\"CPU\"` - tune.grid_search([1, 2]),\n",
      "  },\n",
      "  ),\n",
      "  # You can even grid search various datasets in Tune.\n",
      "  # \"datasets\": {\n",
      "  #     \"train\": tune.grid_search(\n",
      "  #         [ds1, ds2]\n",
      "  #     ),\n",
      "  # },\n",
      "- `\"params\"` - {\n",
      "- `\"objective\"` - \"binary:logistic\",\n",
      "- `\"tree_method\"` - \"approx\",\n",
      "- `\"eval_metric\"` - [\"logloss\", \"error\"],\n",
      "- `\"eta\"` - tune.loguniform(1e-4, 1e-1),\n",
      "- `\"subsample\"` - tune.uniform(0.5, 1.0),\n",
      "- `\"max_depth\"` - tune.randint(1, 9),\n",
      "  },\n",
      "  }\n",
      "  tuner = Tuner(trainable=trainer, param_space=param_space,\n",
      "  run_config=RunConfig(name=\"my_tune_run\"))\n",
      "  analysis = tuner.fit()\n",
      "  \n",
      "  To retry a failed tune run, you can then do\n",
      "  \n",
      "  .. code-block:: python\n",
      "  \n",
      "  tuner = Tuner.restore(experiment_checkpoint_dir)\n",
      "  tuner.fit()\n",
      "  \n",
      "  ``experiment_checkpoint_dir`` can be easily located near the end of the\n",
      "  console output of your first failed run.\n",
      "\n",
      "\n",
      "\n",
      "new_automl = AutoML()\n",
      "new_automl.fit(X_train, y_train, starting_points=starting_points)\n",
      "```\n",
      "  \n",
      "- `seed` - int or None, default=None | The random seed for hpo.\n",
      "- `n_concurrent_trials` - [Experimental] int, default=1 | The number of\n",
      "  concurrent trials. When n_concurrent_trials > 1, flaml performes\n",
      "  [parallel tuning](../../Use-Cases/Task-Oriented-AutoML#parallel-tuning)\n",
      "  and installation of ray or spark is required: `pip install flaml[ray]`\n",
      "  or `pip install flaml[spark]`. Please check\n",
      "  [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
      "  for more details about installing Spark.\n",
      "- `keep_search_state` - boolean, default=False | Whether to keep data needed\n",
      "  for model search after fit(). By default the state is deleted for\n",
      "  space saving.\n",
      "- `preserve_checkpoint` - boolean, default=True | Whether to preserve the saved checkpoint\n",
      "  on disk when deleting automl. By default the checkpoint is preserved.\n",
      "- `early_stop` - boolean, default=False | Whether to stop early if the\n",
      "  search is considered to converge.\n",
      "- `force_cancel` - boolean, default=False | Whether to forcely cancel the PySpark job if overtime.\n",
      "- `append_log` - boolean, default=False | Whetehr to directly append the log\n",
      "  records to the input log file if it exists.\n",
      "- `auto_augment` - boolean, default=True | Whether to automatically\n",
      "  augment rare classes.\n",
      "- `min_sample_size` - int, default=MIN_SAMPLE_TRAIN | the minimal sample\n",
      "  size when sample=True.\n",
      "- `use_ray` - boolean or dict.\n",
      "  If boolean: default=False | Whether to use ray to run the training\n",
      "  in separate processes. This can be used to prevent OOM for large\n",
      "  datasets, but will incur more overhead in time.\n",
      "  If dict: the dict contains the keywords arguments to be passed to\n",
      "  [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).\n",
      "- `use_spark` - boolean, default=False | Whether to use spark to run the training\n",
      "  in parallel spark jobs. This can be used to accelerate training on large models\n",
      "  and large datasets, but will incur more overhead in time and thus slow down\n",
      "  training in some cases.\n",
      "- `free_mem_ratio` - float between 0 and 1, default=0. The free memory ratio to keep during training.\n",
      "- `metric_constraints` - list, default=[] | The list of metric constraints.\n",
      "  Each element in this list is a 3-tuple, which shall be expressed\n",
      "  in the following format: the first element of the 3-tuple is the name of the\n",
      "  metric, the second element is the inequality sign chosen from \">=\" and \"<=\",\n",
      "  and the third element is the constraint value. E.g., `('precision', '>=', 0.9)`.\n",
      "  Note that all the metric names in metric_constraints need to be reported via\n",
      "  the metrics_to_log dictionary returned by a customized metric function.\n",
      "  The customized metric function shall be provided via the `metric` key word argument\n",
      "  of the fit() function or the automl constructor.\n",
      "  Find examples in this [test](https://github.com/microsoft/FLAML/tree/main/test/automl/test_constraints.py).\n",
      "  If `pred_time_limit` is provided as one of keyword arguments to fit() function or\n",
      "  the automl constructor, flaml will automatically (and under the hood)\n",
      "  add it as an additional element in the metric_constraints. Essentially 'pred_time_limit'\n",
      "  specifies a constraint about the prediction latency constraint in seconds.\n",
      "- `custom_hp` - dict, default=None | The custom search space specified by user\n",
      "  Each key is the estimator name, each value is a dict of the custom search space for that estimator. Notice the\n",
      "  domain of the custom search space can either be a value of a sample.Domain object.\n",
      "  \n",
      "  \n",
      "  \n",
      "```python\n",
      "custom_hp = {\n",
      "    \"transformer_ms\": {\n",
      "        \"model_path\": {\n",
      "            \"domain\": \"albert-base-v2\",\n",
      "        },\n",
      "        \"learning_rate\": {\n",
      "            \"domain\": tune.choice([1e-4, 1e-5]),\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "- `time_col` - for a time series task, name of the column containing the timestamps. If not\n",
      "  provided, defaults to the first column of X_train/X_val\n",
      "  \n",
      "- `cv_score_agg_func` - customized cross-validation scores aggregate function. Default to average metrics across folds. If specificed, this function needs to\n",
      "  have the following input arguments:\n",
      "  \n",
      "  * val_loss_folds: list of floats, the loss scores of each fold;\n",
      "  * log_metrics_folds: list of dicts/floats, the metrics of each fold to log.\n",
      "  \n",
      "  This function should return the final aggregate result of all folds. A float number of the minimization objective, and a dictionary as the metrics to log or None.\n",
      "  E.g.,\n",
      "  \n",
      "```python\n",
      "def cv_score_agg_func(val_loss_folds, log_metrics_folds):\n",
      "    metric_to_minimize = sum(val_loss_folds)/len(val_loss_folds)\n",
      "    metrics_to_log = None\n",
      "    for single_fold in log_metrics_folds:\n",
      "        if metrics_to_log is None:\n",
      "            metrics_to_log = single_fold\n",
      "        elif isinstance(metrics_to_log, dict):\n",
      "            metrics_to_log = {k: metrics_to_log[k] + v for k, v in single_fold.items()}\n",
      "        else:\n",
      "            metrics_to_log += single_fold\n",
      "    if metrics_to_log:\n",
      "        n = len(val_loss_folds)\n",
      "        metrics_to_log = (\n",
      "            {k: v / n for k, v in metrics_to_log.items()}\n",
      "            if isinstance(metrics_to_log, dict)\n",
      "            else metrics_to_log / n\n",
      "        )\n",
      "    return metric_to_minimize, metrics_to_log\n",
      "```\n",
      "  \n",
      "- `skip_transform` - boolean, default=False | Whether to pre-process data prior to modeling.\n",
      "- `mlflow_logging` - boolean, default=None | Whether to log the training results to mlflow.\n",
      "  Default value is None, which means the logging decision is made based on\n",
      "  AutoML.__init__'s mlflow_logging argument.\n",
      "  This requires mlflow to be installed and to have an active mlflow run.\n",
      "  FLAML will create nested runs.\n",
      "- `fit_kwargs_by_estimator` - dict, default=None | The user specified keywords arguments, grouped by estimator name.\n",
      "  For TransformersEstimator, available fit_kwargs can be found from\n",
      "  [TrainingArgumentsForAuto](nlp/huggingface/training_args).\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "fit_kwargs_by_estimator = {\n",
      "    \"transformer\": {\n",
      "        \"output_dir\": \"test/data/output/\",\n",
      "        \"fp16\": False,\n",
      "    },\n",
      "    \"tft\": {\n",
      "        \"max_encoder_length\": 1,\n",
      "        \"min_encoder_length\": 1,\n",
      "        \"static_categoricals\": [],\n",
      "        \"static_reals\": [],\n",
      "        \"time_varying_known_categoricals\": [],\n",
      "        \"time_varying_known_reals\": [],\n",
      "        \"time_varying_unknown_categoricals\": [],\n",
      "        \"time_varying_unknown_reals\": [],\n",
      "        \"variable_groups\": {},\n",
      "        \"lags\": {},\n",
      "    }\n",
      "}\n",
      "```\n",
      "  \n",
      "- `**fit_kwargs` - Other key word arguments to pass to fit() function of\n",
      "  the searched learners, such as sample_weight. Below are a few examples of\n",
      "  estimator-specific parameters:\n",
      "- `period` - int | forecast horizon for all time series forecast tasks.\n",
      "- `gpu_per_trial` - float, default = 0 | A float of the number of gpus per trial,\n",
      "  only used by TransformersEstimator, XGBoostSklearnEstimator, and\n",
      "  TemporalFusionTransformerEstimator.\n",
      "- `group_ids` - list of strings of column names identifying a time series, only\n",
      "  used by TemporalFusionTransformerEstimator, required for\n",
      "  'ts_forecast_panel' task. `group_ids` is a parameter for TimeSeriesDataSet object\n",
      "  from PyTorchForecasting.\n",
      "  For other parameters to describe your dataset, refer to\n",
      "  [TimeSeriesDataSet PyTorchForecasting](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.data.timeseries.TimeSeriesDataSet.html).\n",
      "  To specify your variables, use `static_categoricals`, `static_reals`,\n",
      "  `time_varying_known_categoricals`, `time_varying_known_reals`,\n",
      "  `time_varying_unknown_categoricals`, `time_varying_unknown_reals`,\n",
      "  `variable_groups`. To provide more information on your data, use\n",
      "  `max_encoder_length`, `min_encoder_length`, `lags`.\n",
      "- `log_dir` - str, default = \"lightning_logs\" | Folder into which to log results\n",
      "  for tensorboard, only used by TemporalFusionTransformerEstimator.\n",
      "- `max_epochs` - int, default = 20 | Maximum number of epochs to run training,\n",
      "  only used by TemporalFusionTransformerEstimator.\n",
      "- `batch_size` - int, default = 64 | Batch size for training model, only\n",
      "  used by TemporalFusionTransformerEstimator.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "The function you're asking for is called `fit()` in the AutoML class, not `tune_automl`. You can use the `fit()` function to train the FLAML's AutoML model with your defined parameters.\n",
      "\n",
      "Here's an example snippet:\n",
      "\n",
      "```python\n",
      "from flaml import AutoML\n",
      "automl = AutoML()\n",
      "automl.fit(X_train, y_train, task=\"classification\", time_budget=100)\n",
      "```\n",
      "\n",
      "Replace `X_train` and `y_train` with your training data, and customize `task` and `time_budget` as necessary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "update context\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Updating context and resetting conversation.\n",
      "Adding doc_id doc_13 to context.\n",
      "Adding doc_id doc_51 to context.\n",
      "Adding doc_id doc_58 to context.\n",
      "Adding doc_id doc_20 to context.\n",
      "Adding doc_id doc_26 to context.\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user. You should follow the following steps to answer a question:\n",
      "Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
      "a QA task.\n",
      "Step 2, you generate code or answer the question based on the intent.\n",
      "You should leverage the context provided by the user as much as possible. If you think the context is not enough, you\n",
      "can reply exactly \"UPDATE CONTEXT\" to ask the user to provide more contexts.\n",
      "For code generation, you must obey the following rules:\n",
      "You MUST NOT install any packages because all the packages needed are already installed.\n",
      "The code will be executed in IPython, you must follow the formats below to write your code:\n",
      "```python\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Is there a function named `tune_automl` in FLAML?\n",
      "\n",
      "Context is: ---\n",
      "sidebar_label: tune\n",
      "title: tune.tune\n",
      "---\n",
      "\n",
      "## ExperimentAnalysis Objects\n",
      "\n",
      "```python\n",
      "class ExperimentAnalysis(EA)\n",
      "```\n",
      "\n",
      "Class for storing the experiment results.\n",
      "\n",
      "#### report\n",
      "\n",
      "```python\n",
      "def report(_metric=None, **kwargs)\n",
      "```\n",
      "\n",
      "A function called by the HPO application to report final or intermediate\n",
      "results.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "import time\n",
      "from flaml import tune\n",
      "\n",
      "def compute_with_config(config):\n",
      "    current_time = time.time()\n",
      "    metric2minimize = (round(config['x'])-95000)**2\n",
      "    time2eval = time.time() - current_time\n",
      "    tune.report(metric2minimize=metric2minimize, time2eval=time2eval)\n",
      "\n",
      "analysis = tune.run(\n",
      "    compute_with_config,\n",
      "    config={\n",
      "        'x': tune.lograndint(lower=1, upper=1000000),\n",
      "        'y': tune.randint(lower=1, upper=1000000)\n",
      "    },\n",
      "    metric='metric2minimize', mode='min',\n",
      "    num_samples=1000000, time_budget_s=60, use_ray=False)\n",
      "\n",
      "print(analysis.trials[-1].last_result)\n",
      "```\n",
      "  \n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `_metric` - Optional default anonymous metric for ``tune.report(value)``.\n",
      "  (For compatibility with ray.tune.report)\n",
      "- `**kwargs` - Any key value pair to be reported.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "  StopIteration (when not using ray, i.e., _use_ray=False):\n",
      "  A StopIteration exception is raised if the trial has been signaled to stop.\n",
      "  SystemExit (when using ray):\n",
      "  A SystemExit exception is raised if the trial has been signaled to stop by ray.\n",
      "\n",
      "#### run\n",
      "\n",
      "```python\n",
      "def run(evaluation_function, config: Optional[dict] = None, low_cost_partial_config: Optional[dict] = None, cat_hp_cost: Optional[dict] = None, metric: Optional[str] = None, mode: Optional[str] = None, time_budget_s: Union[int, float] = None, points_to_evaluate: Optional[List[dict]] = None, evaluated_rewards: Optional[List] = None, resource_attr: Optional[str] = None, min_resource: Optional[float] = None, max_resource: Optional[float] = None, reduction_factor: Optional[float] = None, scheduler=None, search_alg=None, verbose: Optional[int] = 2, local_dir: Optional[str] = None, num_samples: Optional[int] = 1, resources_per_trial: Optional[dict] = None, config_constraints: Optional[List[Tuple[Callable[[dict], float], str, float]]] = None, metric_constraints: Optional[List[Tuple[str, str, float]]] = None, max_failure: Optional[int] = 100, use_ray: Optional[bool] = False, use_spark: Optional[bool] = False, use_incumbent_result_in_evaluation: Optional[bool] = None, log_file_name: Optional[str] = None, lexico_objectives: Optional[dict] = None, force_cancel: Optional[bool] = False, n_concurrent_trials: Optional[int] = 0, **ray_args, ,)\n",
      "```\n",
      "\n",
      "The function-based way of performing HPO.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "import time\n",
      "from flaml import tune\n",
      "\n",
      "def compute_with_config(config):\n",
      "    current_time = time.time()\n",
      "    metric2minimize = (round(config['x'])-95000)**2\n",
      "    time2eval = time.time() - current_time\n",
      "    tune.report(metric2minimize=metric2minimize, time2eval=time2eval)\n",
      "    # if the evaluation fails unexpectedly and the exception is caught,\n",
      "    # and it doesn't inform the goodness of the config,\n",
      "    # return {}\n",
      "    # if the failure indicates a config is bad,\n",
      "    # report a bad metric value like np.inf or -np.inf\n",
      "    # depending on metric mode being min or max\n",
      "\n",
      "analysis = tune.run(\n",
      "    compute_with_config,\n",
      "    config={\n",
      "        'x': tune.lograndint(lower=1, upper=1000000),\n",
      "        'y': tune.randint(lower=1, upper=1000000)\n",
      "    },\n",
      "    metric='metric2minimize', mode='min',\n",
      "    num_samples=-1, time_budget_s=60, use_ray=False)\n",
      "\n",
      "print(analysis.trials[-1].last_result)\n",
      "```\n",
      "  \n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `evaluation_function` - A user-defined evaluation function.\n",
      "  It takes a configuration as input, outputs a evaluation\n",
      "  result (can be a numerical value or a dictionary of string\n",
      "  and numerical value pairs) for the input configuration.\n",
      "  For machine learning tasks, it usually involves training and\n",
      "  scoring a machine learning model, e.g., through validation loss.\n",
      "- `config` - A dictionary to specify the search space.\n",
      "- `low_cost_partial_config` - A dictionary from a subset of\n",
      "  controlled dimensions to the initial low-cost values.\n",
      "  e.g., ```{'n_estimators': 4, 'max_leaves': 4}```\n",
      "  \n",
      "- `cat_hp_cost` - A dictionary from a subset of categorical dimensions\n",
      "  to the relative cost of each choice.\n",
      "  e.g., ```{'tree_method': [1, 1, 2]}```\n",
      "  i.e., the relative cost of the\n",
      "  three choices of 'tree_method' is 1, 1 and 2 respectively\n",
      "- `metric` - A string of the metric name to optimize for.\n",
      "- `mode` - A string in ['min', 'max'] to specify the objective as\n",
      "  minimization or maximization.\n",
      "- `time_budget_s` - int or float | The time budget in seconds.\n",
      "- `points_to_evaluate` - A list of initial hyperparameter\n",
      "  configurations to run first.\n",
      "- `evaluated_rewards` _list_ - If you have previously evaluated the\n",
      "  parameters passed in as points_to_evaluate you can avoid\n",
      "  re-running those trials by passing in the reward attributes\n",
      "  as a list so the optimiser can be told the results without\n",
      "  needing to re-compute the trial. Must be the same or shorter length than\n",
      "  points_to_evaluate.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "points_to_evaluate = [\n",
      "    {\"b\": .99, \"cost_related\": {\"a\": 3}},\n",
      "    {\"b\": .99, \"cost_related\": {\"a\": 2}},\n",
      "]\n",
      "evaluated_rewards = [3.0]\n",
      "```\n",
      "  \n",
      "  means that you know the reward for the first config in\n",
      "  points_to_evaluate is 3.0 and want to inform run().\n",
      "  \n",
      "- `resource_attr` - A string to specify the resource dimension used by\n",
      "  the scheduler via \"scheduler\".\n",
      "- `min_resource` - A float of the minimal resource to use for the resource_attr.\n",
      "- `max_resource` - A float of the maximal resource to use for the resource_attr.\n",
      "- `reduction_factor` - A float of the reduction factor used for incremental\n",
      "  pruning.\n",
      "- `scheduler` - A scheduler for executing the experiment. Can be None, 'flaml',\n",
      "  'asha' (or  'async_hyperband', 'asynchyperband') or a custom instance of the TrialScheduler class. Default is None:\n",
      "  in this case when resource_attr is provided, the 'flaml' scheduler will be\n",
      "  used, otherwise no scheduler will be used. When set 'flaml', an\n",
      "  authentic scheduler implemented in FLAML will be used. It does not\n",
      "  require users to report intermediate results in evaluation_function.\n",
      "  Find more details about this scheduler in this paper\n",
      "  https://arxiv.org/pdf/1911.04706.pdf).\n",
      "  When set 'asha', the input for arguments \"resource_attr\",\n",
      "  \"min_resource\", \"max_resource\" and \"reduction_factor\" will be passed\n",
      "  to ASHA's \"time_attr\",  \"max_t\", \"grace_period\" and \"reduction_factor\"\n",
      "  respectively. You can also provide a self-defined scheduler instance\n",
      "  of the TrialScheduler class. When 'asha' or self-defined scheduler is\n",
      "  used, you usually need to report intermediate results in the evaluation\n",
      "  function via 'tune.report()'.\n",
      "  If you would like to do some cleanup opearation when the trial is stopped\n",
      "  by the scheduler, you can catch the `StopIteration` (when not using ray)\n",
      "  or `SystemExit` (when using ray) exception explicitly,\n",
      "  as shown in the following example.\n",
      "  Please find more examples using different types of schedulers\n",
      "  and how to set up the corresponding evaluation functions in\n",
      "  test/tune/test_scheduler.py, and test/tune/example_scheduler.py.\n",
      "```python\n",
      "def easy_objective(config):\n",
      "    width, height = config[\"width\"], config[\"height\"]\n",
      "    for step in range(config[\"steps\"]):\n",
      "        intermediate_score = evaluation_fn(step, width, height)\n",
      "        try:\n",
      "            tune.report(iterations=step, mean_loss=intermediate_score)\n",
      "        except (StopIteration, SystemExit):\n",
      "            # do cleanup operation here\n",
      "            return\n",
      "```\n",
      "- `search_alg` - An instance/string of the search algorithm\n",
      "  to be used. The same instance can be used for iterative tuning.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "---\n",
      "sidebar_label: task\n",
      "title: automl.task.task\n",
      "---\n",
      "\n",
      "## Task Objects\n",
      "\n",
      "```python\n",
      "class Task(ABC)\n",
      "```\n",
      "\n",
      "Abstract base class for a machine learning task.\n",
      "\n",
      "Class definitions should implement abstract methods and provide a non-empty dictionary of estimator classes.\n",
      "A Task can be suitable to be used for multiple machine-learning tasks (e.g. classification or regression) or be\n",
      "implemented specifically for a single one depending on the generality of data validation and model evaluation methods\n",
      "implemented. The implementation of a Task may optionally use the training data and labels to determine data and task\n",
      "specific details, such as in determining if a problem is single-label or multi-label.\n",
      "\n",
      "FLAML evaluates at runtime how to behave exactly, relying on the task instance to provide implementations of\n",
      "operations which vary between tasks.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(task_name: str, X_train: Optional[Union[np.ndarray, DataFrame, psDataFrame]] = None, y_train: Optional[Union[np.ndarray, DataFrame, Series, psSeries]] = None)\n",
      "```\n",
      "\n",
      "Constructor.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `task_name` - String name for this type of task. Used when the Task can be generic and implement a number of\n",
      "  types of sub-task.\n",
      "- `X_train` - Optional. Some Task types may use the data shape or features to determine details of their usage,\n",
      "  such as in binary vs multilabel classification.\n",
      "- `y_train` - Optional. Some Task types may use the data shape or features to determine details of their usage,\n",
      "  such as in binary vs multilabel classification.\n",
      "\n",
      "#### \\_\\_str\\_\\_\n",
      "\n",
      "```python\n",
      "def __str__() -> str\n",
      "```\n",
      "\n",
      "Name of this task type.\n",
      "\n",
      "#### evaluate\\_model\\_CV\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def evaluate_model_CV(config: dict, estimator: \"flaml.automl.ml.BaseEstimator\", X_train_all: Union[np.ndarray, DataFrame, psDataFrame], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries], budget: int, kf, eval_metric: str, best_val_loss: float, log_training_metric: bool = False, fit_kwargs: Optional[dict] = {}) -> Tuple[float, float, float, float]\n",
      "```\n",
      "\n",
      "Evaluate the model using cross-validation.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `config` - configuration used in the evaluation of the metric.\n",
      "- `estimator` - Estimator class of the model.\n",
      "- `X_train_all` - Complete training feature data.\n",
      "- `y_train_all` - Complete training target data.\n",
      "- `budget` - Training time budget.\n",
      "- `kf` - Cross-validation index generator.\n",
      "- `eval_metric` - Metric name to be used for evaluation.\n",
      "- `best_val_loss` - Best current validation-set loss.\n",
      "- `log_training_metric` - Bool defaults False. Enables logging of the training metric.\n",
      "- `fit_kwargs` - Additional kwargs passed to the estimator's fit method.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  validation loss, metric value, train time, prediction time\n",
      "\n",
      "#### validate\\_data\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def validate_data(automl: \"flaml.automl.automl.AutoML\", state: \"flaml.automl.state.AutoMLState\", X_train_all: Union[np.ndarray, DataFrame, psDataFrame, None], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], dataframe: Union[DataFrame, None], label: str, X_val: Optional[Union[np.ndarray, DataFrame, psDataFrame]] = None, y_val: Optional[Union[np.ndarray, DataFrame, Series, psSeries]] = None, groups_val: Optional[List[str]] = None, groups: Optional[List[str]] = None)\n",
      "```\n",
      "\n",
      "Validate that the data is suitable for this task type.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `automl` - The AutoML instance from which this task has been constructed.\n",
      "- `state` - The AutoMLState instance for this run.\n",
      "- `X_train_all` - The complete data set or None if dataframe is supplied.\n",
      "- `y_train_all` - The complete target set or None if dataframe is supplied.\n",
      "- `dataframe` - A dataframe constaining the complete data set with targets.\n",
      "- `label` - The name of the target column in dataframe.\n",
      "- `X_val` - Optional. A data set for validation.\n",
      "- `y_val` - Optional. A target vector corresponding to X_val for validation.\n",
      "- `groups_val` - Group labels (with matching length to y_val) or group counts (with sum equal to length of y_val)\n",
      "  for validation data. Need to be consistent with groups.\n",
      "- `groups` - Group labels (with matching length to y_train) or groups counts (with sum equal to length of y_train)\n",
      "  for training data.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `AssertionError` - The data provided is invalid for this task type and configuration.\n",
      "\n",
      "#### prepare\\_data\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def prepare_data(state: \"flaml.automl.state.AutoMLState\", X_train_all: Union[np.ndarray, DataFrame, psDataFrame], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], auto_augment: bool, eval_method: str, split_type: str, split_ratio: float, n_splits: int, data_is_df: bool, sample_weight_full: Optional[List[float]] = None)\n",
      "```\n",
      "\n",
      "Prepare the data for fitting or inference.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `automl` - The AutoML instance from which this task has been constructed.\n",
      "- `state` - The AutoMLState instance for this run.\n",
      "- `X_train_all` - The complete data set or None if dataframe is supplied. Must\n",
      "  contain the target if y_train_all is None\n",
      "- `y_train_all` - The complete target set or None if supplied in X_train_all.\n",
      "- `auto_augment` - If true, task-specific data augmentations will be applied.\n",
      "- `eval_method` - A string of resampling strategy, one of ['auto', 'cv', 'holdout'].\n",
      "- `split_type` - str or splitter object, default=\"auto\" | the data split type.\n",
      "  * A valid splitter object is an instance of a derived class of scikit-learn\n",
      "  [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
      "  and have ``split`` and ``get_n_splits`` methods with the same signatures.\n",
      "  Set eval_method to \"cv\" to use the splitter object.\n",
      "  * Valid str options depend on different tasks.\n",
      "  For classification tasks, valid choices are\n",
      "  [\"auto\", 'stratified', 'uniform', 'time', 'group']. \"auto\" -> stratified.\n",
      "  For regression tasks, valid choices are [\"auto\", 'uniform', 'time'].\n",
      "  \"auto\" -> uniform.\n",
      "  For time series forecast tasks, must be \"auto\" or 'time'.\n",
      "  For ranking task, must be \"auto\" or 'group'.\n",
      "- `split_ratio` - A float of the valiation data percentage for holdout.\n",
      "- `n_splits` - An integer of the number of folds for cross - validation.\n",
      "- `data_is_df` - True if the data was provided as a DataFrame else False.\n",
      "- `sample_weight_full` - A 1d arraylike of the sample weight.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `AssertionError` - The configuration provided is invalid for this task type and data.\n",
      "\n",
      "#### decide\\_split\\_type\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def decide_split_type(split_type: str, y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], fit_kwargs: dict, groups: Optional[List[str]] = None) -> str\n",
      "```\n",
      "\n",
      "Choose an appropriate data split type for this data and task.\n",
      "\n",
      "If split_type is 'auto' then this is determined based on the task type and data.\n",
      "If a specific split_type is requested then the choice is validated to be appropriate.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `split_type` - Either 'auto' or a task appropriate split type.\n",
      "- `y_train_all` - The complete set of targets.\n",
      "- `fit_kwargs` - Additional kwargs passed to the estimator's fit method.\n",
      "- `groups` - Optional. Group labels (with matching length to y_train) or groups counts (with sum equal to length\n",
      "  of y_train) for training data.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  The determined appropriate split type.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `AssertionError` - The requested split_type is invalid for this task, configuration and data.\n",
      "\n",
      "#### preprocess\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def preprocess(X: Union[np.ndarray, DataFrame, psDataFrame], transformer: Optional[\"flaml.automl.data.DataTransformer\"] = None) -> Union[np.ndarray, DataFrame]\n",
      "```\n",
      "\n",
      "Preprocess the data ready for fitting or inference with this task type.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `X` - The data set to process.\n",
      "- `transformer` - A DataTransformer instance to be used in processing.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  The preprocessed data set having the same type as the input.\n",
      "\n",
      "#### default\\_estimator\\_list\n",
      "\n",
      "```python\n",
      "@abstractmethod\n",
      "def default_estimator_list(estimator_list: Union[List[str], str] = \"auto\", is_spark_dataframe: bool = False) -> List[str]\n",
      "```\n",
      "\n",
      "Return the list of default estimators registered for this task type.\n",
      "\n",
      "If 'auto' is provided then the default list is returned, else the provided list will be validated given this task\n",
      "type.\n",
      "\n",
      "**Arguments**:\n",
      "---\n",
      "sidebar_label: estimator\n",
      "title: default.estimator\n",
      "---\n",
      "\n",
      "#### flamlize\\_estimator\n",
      "\n",
      "```python\n",
      "def flamlize_estimator(super_class, name: str, task: str, alternatives=None)\n",
      "```\n",
      "\n",
      "Enhance an estimator class with flaml's data-dependent default hyperparameter settings.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "import sklearn.ensemble as ensemble\n",
      "RandomForestRegressor = flamlize_estimator(\n",
      "    ensemble.RandomForestRegressor, \"rf\", \"regression\"\n",
      ")\n",
      "```\n",
      "  \n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `super_class` - an scikit-learn compatible estimator class.\n",
      "- `name` - a str of the estimator's name.\n",
      "- `task` - a str of the task type.\n",
      "- `alternatives` - (Optional) a list for alternative estimator names. For example,\n",
      "  ```[(\"max_depth\", 0, \"xgboost\")]``` means if the \"max_depth\" is set to 0\n",
      "  in the constructor, then look for the learned defaults for estimator \"xgboost\".\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: space\n",
      "title: tune.space\n",
      "---\n",
      "\n",
      "#### is\\_constant\n",
      "\n",
      "```python\n",
      "def is_constant(space: Union[Dict, List]) -> bool\n",
      "```\n",
      "\n",
      "Whether the search space is all constant.\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  A bool of whether the search space is all constant.\n",
      "\n",
      "#### define\\_by\\_run\\_func\n",
      "\n",
      "```python\n",
      "def define_by_run_func(trial, space: Dict, path: str = \"\") -> Optional[Dict[str, Any]]\n",
      "```\n",
      "\n",
      "Define-by-run function to create the search space.\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  A dict with constant values.\n",
      "\n",
      "#### unflatten\\_hierarchical\n",
      "\n",
      "```python\n",
      "def unflatten_hierarchical(config: Dict, space: Dict) -> Tuple[Dict, Dict]\n",
      "```\n",
      "\n",
      "Unflatten hierarchical config.\n",
      "\n",
      "#### add\\_cost\\_to\\_space\n",
      "\n",
      "```python\n",
      "def add_cost_to_space(space: Dict, low_cost_point: Dict, choice_cost: Dict)\n",
      "```\n",
      "\n",
      "Update the space in place by adding low_cost_point and choice_cost.\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  A dict with constant values.\n",
      "\n",
      "#### normalize\n",
      "\n",
      "```python\n",
      "def normalize(config: Dict, space: Dict, reference_config: Dict, normalized_reference_config: Dict, recursive: bool = False)\n",
      "```\n",
      "\n",
      "Normalize config in space according to reference_config.\n",
      "\n",
      "Normalize each dimension in config to [0,1].\n",
      "\n",
      "#### indexof\n",
      "\n",
      "```python\n",
      "def indexof(domain: Dict, config: Dict) -> int\n",
      "```\n",
      "\n",
      "Find the index of config in domain.categories.\n",
      "\n",
      "#### complete\\_config\n",
      "\n",
      "```python\n",
      "def complete_config(partial_config: Dict, space: Dict, flow2, disturb: bool = False, lower: Optional[Dict] = None, upper: Optional[Dict] = None) -> Tuple[Dict, Dict]\n",
      "```\n",
      "\n",
      "Complete partial config in space.\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  config, space.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: variant_generator\n",
      "title: tune.searcher.variant_generator\n",
      "---\n",
      "\n",
      "## TuneError Objects\n",
      "\n",
      "```python\n",
      "class TuneError(Exception)\n",
      "```\n",
      "\n",
      "General error class raised by ray.tune.\n",
      "\n",
      "#### generate\\_variants\n",
      "\n",
      "```python\n",
      "def generate_variants(unresolved_spec: Dict, constant_grid_search: bool = False, random_state: \"RandomState\" = None) -> Generator[Tuple[Dict, Dict], None, None]\n",
      "```\n",
      "\n",
      "Generates variants from a spec (dict) with unresolved values.\n",
      "There are two types of unresolved values:\n",
      "Grid search: These define a grid search over values. For example, the\n",
      "following grid search values in a spec will produce six distinct\n",
      "variants in combination:\n",
      "\"activation\": grid_search([\"relu\", \"tanh\"])\n",
      "\"learning_rate\": grid_search([1e-3, 1e-4, 1e-5])\n",
      "Lambda functions: These are evaluated to produce a concrete value, and\n",
      "can express dependencies or conditional distributions between values.\n",
      "They can also be used to express random search (e.g., by calling\n",
      "into the `random` or `np` module).\n",
      "\"cpu\": lambda spec: spec.config.num_workers\n",
      "\"batch_size\": lambda spec: random.uniform(1, 1000)\n",
      "Finally, to support defining specs in plain JSON / YAML, grid search\n",
      "and lambda functions can also be defined alternatively as follows:\n",
      "\"activation\": {\"grid_search\": [\"relu\", \"tanh\"]}\n",
      "\"cpu\": {\"eval\": \"spec.config.num_workers\"}\n",
      "Use `format_vars` to format the returned dict of hyperparameters.\n",
      "\n",
      "**Yields**:\n",
      "\n",
      "  (Dict of resolved variables, Spec object)\n",
      "\n",
      "#### grid\\_search\n",
      "\n",
      "```python\n",
      "def grid_search(values: List) -> Dict[str, List]\n",
      "```\n",
      "\n",
      "Convenience method for specifying grid search over a value.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `values` - An iterable whose parameters will be gridded.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[flaml.autogen.oai.completion: 08-03 22:03:15] {212} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 1348, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 316, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 277, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/requests/adapters.py\", line 489, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 1348, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 316, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 277, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/requests/sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/requests/sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/requests/adapters.py\", line 547, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/code/FLAML1/flaml/autogen/oai/completion.py\", line 204, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flaml.autogen.oai.completion:retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 1348, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 316, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 277, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/requests/adapters.py\", line 489, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 1348, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 316, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/http/client.py\", line 277, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/requests/sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/requests/sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/requests/adapters.py\", line 547, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/code/FLAML1/flaml/autogen/oai/completion.py\", line 204, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "Based on the provided context, FLAML has a module named `tune`, which provides functionality for hyperparameter tuning. However, there is no mention of a function named `tune_automl` in the given context. The primary function for performing hyperparameter optimization in this module is `tune.run`.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "update context\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Updating context and resetting conversation.\n",
      "Adding doc_id doc_34 to context.\n",
      "Adding doc_id doc_22 to context.\n",
      "Adding doc_id doc_11 to context.\n",
      "Adding doc_id doc_59 to context.\n",
      "Adding doc_id doc_3 to context.\n",
      "Adding doc_id doc_56 to context.\n",
      "Adding doc_id doc_47 to context.\n",
      "Adding doc_id doc_53 to context.\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user. You should follow the following steps to answer a question:\n",
      "Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
      "a QA task.\n",
      "Step 2, you generate code or answer the question based on the intent.\n",
      "You should leverage the context provided by the user as much as possible. If you think the context is not enough, you\n",
      "can reply exactly \"UPDATE CONTEXT\" to ask the user to provide more contexts.\n",
      "For code generation, you must obey the following rules:\n",
      "You MUST NOT install any packages because all the packages needed are already installed.\n",
      "The code will be executed in IPython, you must follow the formats below to write your code:\n",
      "```python\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Is there a function named `tune_automl` in FLAML?\n",
      "\n",
      "Context is: ---\n",
      "sidebar_label: automl\n",
      "title: automl.automl\n",
      "---\n",
      "\n",
      "#### size\n",
      "\n",
      "```python\n",
      "def size(learner_classes: dict, config: dict) -> float\n",
      "```\n",
      "\n",
      "Size function.\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  The mem size in bytes for a config.\n",
      "\n",
      "## AutoML Objects\n",
      "\n",
      "```python\n",
      "class AutoML(BaseEstimator)\n",
      "```\n",
      "\n",
      "The AutoML class.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "automl = AutoML()\n",
      "automl_settings = {\n",
      "    \"time_budget\": 60,\n",
      "    \"metric\": 'accuracy',\n",
      "    \"task\": 'classification',\n",
      "    \"log_file_name\": 'mylog.log',\n",
      "}\n",
      "automl.fit(X_train = X_train, y_train = y_train, **automl_settings)\n",
      "```\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(**settings)\n",
      "```\n",
      "\n",
      "Constructor.\n",
      "\n",
      "Many settings in fit() can be passed to the constructor too.\n",
      "If an argument in fit() is provided, it will override the setting passed to the constructor.\n",
      "If an argument in fit() is not provided but provided in the constructor, the value passed to the constructor will be used.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `metric` - A string of the metric name or a function,\n",
      "  e.g., 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_weighted',\n",
      "  'roc_auc_ovo_weighted', 'roc_auc_ovr_weighted', 'f1', 'micro_f1', 'macro_f1',\n",
      "  'log_loss', 'mae', 'mse', 'r2', 'mape'. Default is 'auto'.\n",
      "  If passing a customized metric function, the function needs to\n",
      "  have the following input arguments:\n",
      "  \n",
      "```python\n",
      "def custom_metric(\n",
      "    X_test, y_test, estimator, labels,\n",
      "    X_train, y_train, weight_test=None, weight_train=None,\n",
      "    config=None, groups_test=None, groups_train=None,\n",
      "):\n",
      "    return metric_to_minimize, metrics_to_log\n",
      "```\n",
      "  which returns a float number as the minimization objective,\n",
      "  and a dictionary as the metrics to log. E.g.,\n",
      "  \n",
      "```python\n",
      "def custom_metric(\n",
      "    X_val, y_val, estimator, labels,\n",
      "    X_train, y_train, weight_val=None, weight_train=None,\n",
      "    *args,\n",
      "):\n",
      "    from sklearn.metrics import log_loss\n",
      "    import time\n",
      "\n",
      "    start = time.time()\n",
      "    y_pred = estimator.predict_proba(X_val)\n",
      "    pred_time = (time.time() - start) / len(X_val)\n",
      "    val_loss = log_loss(y_val, y_pred, labels=labels, sample_weight=weight_val)\n",
      "    y_pred = estimator.predict_proba(X_train)\n",
      "    train_loss = log_loss(y_train, y_pred, labels=labels, sample_weight=weight_train)\n",
      "    alpha = 0.5\n",
      "    return val_loss * (1 + alpha) - alpha * train_loss, {\n",
      "        \"val_loss\": val_loss,\n",
      "        \"train_loss\": train_loss,\n",
      "        \"pred_time\": pred_time,\n",
      "    }\n",
      "```\n",
      "- `task` - A string of the task type, e.g.,\n",
      "  'classification', 'regression', 'ts_forecast', 'rank',\n",
      "  'seq-classification', 'seq-regression', 'summarization',\n",
      "  or an instance of the Task class.\n",
      "- `n_jobs` - An integer of the number of threads for training | default=-1.\n",
      "  Use all available resources when n_jobs == -1.\n",
      "- `log_file_name` - A string of the log file name | default=\"\". To disable logging,\n",
      "  set it to be an empty string \"\".\n",
      "- `estimator_list` - A list of strings for estimator names, or 'auto'.\n",
      "  e.g., ```['lgbm', 'xgboost', 'xgb_limitdepth', 'catboost', 'rf', 'extra_tree']```.\n",
      "- `time_budget` - A float number of the time budget in seconds.\n",
      "  Use -1 if no time limit.\n",
      "- `max_iter` - An integer of the maximal number of iterations.\n",
      "- `sample` - A boolean of whether to sample the training data during\n",
      "  search.\n",
      "- `ensemble` - boolean or dict | default=False. Whether to perform\n",
      "  ensemble after search. Can be a dict with keys 'passthrough'\n",
      "  and 'final_estimator' to specify the passthrough and\n",
      "  final_estimator in the stacker. The dict can also contain\n",
      "  'n_jobs' as the key to specify the number of jobs for the stacker.\n",
      "- `eval_method` - A string of resampling strategy, one of\n",
      "  ['auto', 'cv', 'holdout'].\n",
      "- `split_ratio` - A float of the valiation data percentage for holdout.\n",
      "- `n_splits` - An integer of the number of folds for cross - validation.\n",
      "- `log_type` - A string of the log type, one of\n",
      "  ['better', 'all'].\n",
      "  'better' only logs configs with better loss than previos iters\n",
      "  'all' logs all the tried configs.\n",
      "- `model_history` - A boolean of whether to keep the best\n",
      "  model per estimator. Make sure memory is large enough if setting to True.\n",
      "- `log_training_metric` - A boolean of whether to log the training\n",
      "  metric for each model.\n",
      "- `mem_thres` - A float of the memory size constraint in bytes.\n",
      "- `pred_time_limit` - A float of the prediction latency constraint in seconds.\n",
      "  It refers to the average prediction time per row in validation data.\n",
      "- `train_time_limit` - A float of the training time constraint in seconds.\n",
      "- `verbose` - int, default=3 | Controls the verbosity, higher means more\n",
      "  messages.\n",
      "- `retrain_full` - bool or str, default=True | whether to retrain the\n",
      "  selected model on the full training data when using holdout.\n",
      "  True - retrain only after search finishes; False - no retraining;\n",
      "  'budget' - do best effort to retrain without violating the time\n",
      "  budget.\n",
      "- `split_type` - str or splitter object, default=\"auto\" | the data split type.\n",
      "  * A valid splitter object is an instance of a derived class of scikit-learn\n",
      "  [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
      "  and have ``split`` and ``get_n_splits`` methods with the same signatures.\n",
      "  Set eval_method to \"cv\" to use the splitter object.\n",
      "  * Valid str options depend on different tasks.\n",
      "  For classification tasks, valid choices are\n",
      "  [\"auto\", 'stratified', 'uniform', 'time', 'group']. \"auto\" -> stratified.\n",
      "  For regression tasks, valid choices are [\"auto\", 'uniform', 'time'].\n",
      "  \"auto\" -> uniform.\n",
      "  For time series forecast tasks, must be \"auto\" or 'time'.\n",
      "  For ranking task, must be \"auto\" or 'group'.\n",
      "- `hpo_method` - str, default=\"auto\" | The hyperparameter\n",
      "  optimization method. By default, CFO is used for sequential\n",
      "  search and BlendSearch is used for parallel search.\n",
      "  No need to set when using flaml's default search space or using\n",
      "  a simple customized search space. When set to 'bs', BlendSearch\n",
      "  is used. BlendSearch can be tried when the search space is\n",
      "  complex, for example, containing multiple disjoint, discontinuous\n",
      "  subspaces. When set to 'random', random search is used.\n",
      "- `starting_points` - A dictionary or a str to specify the starting hyperparameter\n",
      "  config for the estimators | default=\"static\".\n",
      "  If str:\n",
      "  - if \"data\", use data-dependent defaults;\n",
      "  - if \"data:path\" use data-dependent defaults which are stored at path;\n",
      "  - if \"static\", use data-independent defaults.\n",
      "  If dict, keys are the name of the estimators, and values are the starting\n",
      "  hyperparamter configurations for the corresponding estimators.\n",
      "  The value can be a single hyperparamter configuration dict or a list\n",
      "  of hyperparamter configuration dicts.\n",
      "  In the following code example, we get starting_points from the\n",
      "  `automl` object and use them in the `new_automl` object.\n",
      "  e.g.,\n",
      "  \n",
      "```python\n",
      "from flaml import AutoML\n",
      "automl = AutoML()\n",
      "X_train, y_train = load_iris(return_X_y=True)\n",
      "automl.fit(X_train, y_train)\n",
      "starting_points = automl.best_config_per_estimator\n",
      "---\n",
      "sidebar_label: cfo_cat\n",
      "title: tune.searcher.cfo_cat\n",
      "---\n",
      "\n",
      "## FLOW2Cat Objects\n",
      "\n",
      "```python\n",
      "class FLOW2Cat(FLOW2)\n",
      "```\n",
      "\n",
      "Local search algorithm optimized for categorical variables.\n",
      "\n",
      "## CFOCat Objects\n",
      "\n",
      "```python\n",
      "class CFOCat(CFO)\n",
      "```\n",
      "\n",
      "CFO optimized for categorical variables.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: autovw\n",
      "title: onlineml.autovw\n",
      "---\n",
      "\n",
      "## AutoVW Objects\n",
      "\n",
      "```python\n",
      "class AutoVW()\n",
      "```\n",
      "\n",
      "Class for the AutoVW algorithm.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(max_live_model_num: int, search_space: dict, init_config: Optional[dict] = {}, min_resource_lease: Optional[Union[str, float]] = \"auto\", automl_runner_args: Optional[dict] = {}, scheduler_args: Optional[dict] = {}, model_select_policy: Optional[str] = \"threshold_loss_ucb\", metric: Optional[str] = \"mae_clipped\", random_seed: Optional[int] = None, model_selection_mode: Optional[str] = \"min\", cb_coef: Optional[float] = None)\n",
      "```\n",
      "\n",
      "Constructor.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `max_live_model_num` - An int to specify the maximum number of\n",
      "  'live' models, which, in other words, is the maximum number\n",
      "  of models allowed to update in each learning iteraction.\n",
      "- `search_space` - A dictionary of the search space. This search space\n",
      "  includes both hyperparameters we want to tune and fixed\n",
      "  hyperparameters. In the latter case, the value is a fixed value.\n",
      "- `init_config` - A dictionary of a partial or full initial config,\n",
      "  e.g. {'interactions': set(), 'learning_rate': 0.5}\n",
      "- `min_resource_lease` - string or float | The minimum resource lease\n",
      "  assigned to a particular model/trial. If set as 'auto', it will\n",
      "  be calculated automatically.\n",
      "- `automl_runner_args` - A dictionary of configuration for the OnlineTrialRunner.\n",
      "  If set {}, default values will be used, which is equivalent to using\n",
      "  the following configs.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "automl_runner_args = {\n",
      "    \"champion_test_policy\": 'loss_ucb', # the statistic test for a better champion\n",
      "    \"remove_worse\": False,              # whether to do worse than test\n",
      "}\n",
      "```\n",
      "  \n",
      "- `scheduler_args` - A dictionary of configuration for the scheduler.\n",
      "  If set {}, default values will be used, which is equivalent to using the\n",
      "  following config.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "scheduler_args = {\n",
      "    \"keep_challenger_metric\": 'ucb',  # what metric to use when deciding the top performing challengers\n",
      "    \"keep_challenger_ratio\": 0.5,     # denotes the ratio of top performing challengers to keep live\n",
      "    \"keep_champion\": True,            # specifcies whether to keep the champion always running\n",
      "}\n",
      "```\n",
      "  \n",
      "- `model_select_policy` - A string in ['threshold_loss_ucb',\n",
      "  'threshold_loss_lcb', 'threshold_loss_avg', 'loss_ucb', 'loss_lcb',\n",
      "  'loss_avg'] to specify how to select one model to do prediction from\n",
      "  the live model pool. Default value is 'threshold_loss_ucb'.\n",
      "- `metric` - A string in ['mae_clipped', 'mae', 'mse', 'absolute_clipped',\n",
      "  'absolute', 'squared'] to specify the name of the loss function used\n",
      "  for calculating the progressive validation loss in ChaCha.\n",
      "- `random_seed` - An integer of the random seed used in the searcher\n",
      "  (more specifically this the random seed for ConfigOracle).\n",
      "- `model_selection_mode` - A string in ['min', 'max'] to specify the objective as\n",
      "  minimization or maximization.\n",
      "- `cb_coef` - A float coefficient (optional) used in the sample complexity bound.\n",
      "\n",
      "#### predict\n",
      "\n",
      "```python\n",
      "def predict(data_sample)\n",
      "```\n",
      "\n",
      "Predict on the input data sample.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `data_sample` - one data example in vw format.\n",
      "\n",
      "#### learn\n",
      "\n",
      "```python\n",
      "def learn(data_sample)\n",
      "```\n",
      "\n",
      "Perform one online learning step with the given data sample.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `data_sample` - one data example in vw format. It will be used to\n",
      "  update the vw model.\n",
      "\n",
      "#### get\\_ns\\_feature\\_dim\\_from\\_vw\\_example\n",
      "\n",
      "```python\n",
      "@staticmethod\n",
      "def get_ns_feature_dim_from_vw_example(vw_example) -> dict\n",
      "```\n",
      "\n",
      "Get a dictionary of feature dimensionality for each namespace singleton.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: suggest\n",
      "title: default.suggest\n",
      "---\n",
      "\n",
      "#### suggest\\_config\n",
      "\n",
      "```python\n",
      "def suggest_config(task, X, y, estimator_or_predictor, location=None, k=None, meta_feature_fn=meta_feature)\n",
      "```\n",
      "\n",
      "Suggest a list of configs for the given task and training data.\n",
      "\n",
      "The returned configs can be used as starting points for AutoML.fit().\n",
      "`FLAML_sample_size` is removed from the configs.\n",
      "\n",
      "#### suggest\\_learner\n",
      "\n",
      "```python\n",
      "def suggest_learner(task, X, y, estimator_or_predictor=\"all\", estimator_list=None, location=None)\n",
      "```\n",
      "\n",
      "Suggest best learner within estimator_list.\n",
      "\n",
      "#### suggest\\_hyperparams\n",
      "\n",
      "```python\n",
      "def suggest_hyperparams(task, X, y, estimator_or_predictor, location=None)\n",
      "```\n",
      "\n",
      "Suggest hyperparameter configurations and an estimator class.\n",
      "\n",
      "The configurations can be used to initialize the estimator class like lightgbm.LGBMRegressor.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "hyperparams, estimator_class = suggest_hyperparams(\"regression\", X_train, y_train, \"lgbm\")\n",
      "model = estimator_class(**hyperparams)  # estimator_class is LGBMRegressor\n",
      "model.fit(X_train, y_train)\n",
      "```\n",
      "  \n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `task` - A string of the task type, e.g.,\n",
      "  'classification', 'regression', 'ts_forecast', 'rank',\n",
      "  'seq-classification', 'seq-regression'.\n",
      "- `X` - A dataframe of training data in shape n*m.\n",
      "  For 'ts_forecast' task, the first column of X_train\n",
      "  must be the timestamp column (datetime type). Other\n",
      "  columns in the dataframe are assumed to be exogenous\n",
      "  variables (categorical or numeric).\n",
      "- `y` - A series of labels in shape n*1.\n",
      "- `estimator_or_predictor` - A str of the learner name or a dict of the learned config predictor.\n",
      "  If a dict, it contains:\n",
      "  - \"version\": a str of the version number.\n",
      "  - \"preprocessing\": a dictionary containing:\n",
      "  * \"center\": a list of meta feature value offsets for normalization.\n",
      "  * \"scale\": a list of meta feature scales to normalize each dimension.\n",
      "  - \"neighbors\": a list of dictionaries. Each dictionary contains:\n",
      "  * \"features\": a list of the normalized meta features for a neighbor.\n",
      "  * \"choice\": an integer of the configuration id in the portfolio.\n",
      "  - \"portfolio\": a list of dictionaries, each corresponding to a configuration:\n",
      "  * \"class\": a str of the learner name.\n",
      "  * \"hyperparameters\": a dict of the config. The key \"FLAML_sample_size\" will be ignored.\n",
      "- `location` - (Optional) A str of the location containing mined portfolio file.\n",
      "  Only valid when the portfolio is a str, by default the location is flaml/default.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `hyperparams` - A dict of the hyperparameter configurations.\n",
      "- `estiamtor_class` - A class of the underlying estimator, e.g., lightgbm.LGBMClassifier.\n",
      "\n",
      "#### preprocess\\_and\\_suggest\\_hyperparams\n",
      "\n",
      "```python\n",
      "def preprocess_and_suggest_hyperparams(task, X, y, estimator_or_predictor, location=None)\n",
      "```\n",
      "\n",
      "Preprocess the data and suggest hyperparameters.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "```python\n",
      "hyperparams, estimator_class, X, y, feature_transformer, label_transformer = \\\n",
      "    preprocess_and_suggest_hyperparams(\"classification\", X_train, y_train, \"xgb_limitdepth\")\n",
      "model = estimator_class(**hyperparams)  # estimator_class is XGBClassifier\n",
      "model.fit(X, y)\n",
      "X_test = feature_transformer.transform(X_test)\n",
      "y_pred = label_transformer.inverse_transform(pd.Series(model.predict(X_test).astype(int)))\n",
      "```\n",
      "  \n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `task` - A string of the task type, e.g.,\n",
      "  'classification', 'regression', 'ts_forecast', 'rank',\n",
      "  'seq-classification', 'seq-regression'.\n",
      "- `X` - A dataframe of training data in shape n*m.\n",
      "  For 'ts_forecast' task, the first column of X_train\n",
      "  must be the timestamp column (datetime type). Other\n",
      "  columns in the dataframe are assumed to be exogenous\n",
      "  variables (categorical or numeric).\n",
      "- `y` - A series of labels in shape n*1.\n",
      "- `estimator_or_predictor` - A str of the learner name or a dict of the learned config predictor.\n",
      "  \"choose_xgb\" means choosing between xgb_limitdepth and xgboost.\n",
      "  If a dict, it contains:\n",
      "  - \"version\": a str of the version number.\n",
      "  - \"preprocessing\": a dictionary containing:\n",
      "  * \"center\": a list of meta feature value offsets for normalization.\n",
      "  * \"scale\": a list of meta feature scales to normalize each dimension.\n",
      "  - \"neighbors\": a list of dictionaries. Each dictionary contains:\n",
      "  * \"features\": a list of the normalized meta features for a neighbor.\n",
      "  * \"choice\": a integer of the configuration id in the portfolio.\n",
      "  - \"portfolio\": a list of dictionaries, each corresponding to a configuration:\n",
      "  * \"class\": a str of the learner name.\n",
      "  * \"hyperparameters\": a dict of the config. They key \"FLAML_sample_size\" will be ignored.\n",
      "- `location` - (Optional) A str of the location containing mined portfolio file.\n",
      "  Only valid when the portfolio is a str, by default the location is flaml/default.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `hyperparams` - A dict of the hyperparameter configurations.\n",
      "- `estiamtor_class` - A class of the underlying estimator, e.g., lightgbm.LGBMClassifier.\n",
      "- `X` - the preprocessed X.\n",
      "- `y` - the preprocessed y.\n",
      "- `feature_transformer` - a data transformer that can be applied to X_test.\n",
      "- `label_transformer` - a label transformer that can be applied to y_test.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: user_proxy_agent\n",
      "title: autogen.agent.user_proxy_agent\n",
      "---\n",
      "\n",
      "## UserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class UserProxyAgent(Agent)\n",
      "```\n",
      "\n",
      "(Experimental) A proxy agent for the user, that can execute code and provide feedback to the other agents.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name: str, system_message: Optional[str] = \"\", work_dir: Optional[str] = None, human_input_mode: Optional[str] = \"ALWAYS\", function_map: Optional[Dict[str, Callable]] = {}, max_consecutive_auto_reply: Optional[int] = None, is_termination_msg: Optional[Callable[[Dict], bool]] = None, use_docker: Optional[Union[List[str], str, bool]] = True, timeout: Optional[int] = 600, **config, ,)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `system_message` _str_ - system message for the agent.\n",
      "- `work_dir` _Optional, str_ - The working directory for the code execution.\n",
      "  If None, a default working directory will be used.\n",
      "  The default working directory is the \"extensions\" directory under\n",
      "  \"path_to_flaml/autogen\".\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `function_map` _dict[str, callable]_ - Mapping function names (passed to openai) to callable functions.\n",
      "- `max_consecutive_auto_reply` _int_ - the maximum number of consecutive auto replies.\n",
      "  default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
      "  The limit only plays a role when human_input_mode is not \"ALWAYS\".\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `use_docker` _Optional, list, str or bool_ - The docker image to use for code execution.\n",
      "  If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
      "  with the first image successfully pulled.\n",
      "  If None, False or empty, the code will be executed in the current environment.\n",
      "  Default is True, which will be converted into a list.\n",
      "  If the code is executed in the current environment,\n",
      "  the code must be trusted.\n",
      "- `timeout` _Optional, int_ - The maximum execution time in seconds.\n",
      "- `**config` _dict_ - other configurations.\n",
      "\n",
      "#### use\\_docker\n",
      "\n",
      "```python\n",
      "@property\n",
      "def use_docker() -> Union[bool, str]\n",
      "```\n",
      "\n",
      "bool value of whether to use docker to execute the code,\n",
      "or str value of the docker image name to use.\n",
      "\n",
      "#### execute\\_code\n",
      "\n",
      "```python\n",
      "def execute_code(code_blocks)\n",
      "```\n",
      "\n",
      "Execute the code and return the result.\n",
      "\n",
      "#### auto\\_reply\n",
      "\n",
      "```python\n",
      "def auto_reply(message: dict, sender, default_reply=\"\")\n",
      "```\n",
      "\n",
      "Generate an auto reply.\n",
      "\n",
      "#### receive\n",
      "\n",
      "```python\n",
      "def receive(message: Union[Dict, str], sender)\n",
      "```\n",
      "\n",
      "Receive a message from the sender agent.\n",
      "Once a message is received, this function sends a reply to the sender or simply stop.\n",
      "The reply can be generated automatically or entered manually by a human.\n",
      "\n",
      "#### generate\\_init\\_prompt\n",
      "\n",
      "```python\n",
      "def generate_init_prompt(*args, **kwargs) -> Union[str, Dict]\n",
      "```\n",
      "\n",
      "Generate the initial prompt for the agent.\n",
      "\n",
      "Override this function to customize the initial prompt based on user's request.\n",
      "\n",
      "#### initiate\\_chat\n",
      "\n",
      "```python\n",
      "def initiate_chat(recipient, *args, **kwargs)\n",
      "```\n",
      "\n",
      "Initiate a chat with the receiver agent.\n",
      "\n",
      "`generate_init_prompt` is called to generate the initial prompt for the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `receiver` - the receiver agent.\n",
      "- `*args` - any additional arguments.\n",
      "- `**kwargs` - any additional keyword arguments.\n",
      "\n",
      "#### register\\_function\n",
      "\n",
      "```python\n",
      "def register_function(function_map: Dict[str, Callable])\n",
      "```\n",
      "\n",
      "Register functions to the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `function_map` - a dictionary mapping function names to functions.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: training_args\n",
      "title: automl.nlp.huggingface.training_args\n",
      "---\n",
      "\n",
      "## TrainingArgumentsForAuto Objects\n",
      "\n",
      "```python\n",
      "@dataclass\n",
      "class TrainingArgumentsForAuto(TrainingArguments)\n",
      "```\n",
      "\n",
      "FLAML custom TrainingArguments.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `task` _str_ - the task name for NLP tasks, e.g., seq-classification, token-classification\n",
      "- `output_dir` _str_ - data root directory for outputing the log, etc.\n",
      "- `model_path` _str, optional, defaults to \"facebook/muppet-roberta-base\"_ - A string,\n",
      "  the path of the language model file, either a path from huggingface\n",
      "  model card huggingface.co/models, or a local path for the model.\n",
      "- `fp16` _bool, optional, defaults to \"False\"_ - A bool, whether to use FP16.\n",
      "- `max_seq_length` _int, optional, defaults to 128_ - An integer, the max length of the sequence.\n",
      "  For token classification task, this argument will be ineffective.\n",
      "  pad_to_max_length (bool, optional, defaults to \"False\"):\n",
      "  whether to pad all samples to model maximum sentence length.\n",
      "  If False, will pad the samples dynamically when batching to the maximum length in the batch.\n",
      "- `per_device_eval_batch_size` _int, optional, defaults to 1_ - An integer, the per gpu evaluation batch size.\n",
      "- `label_list` _List[str], optional, defaults to None_ - A list of string, the string list of the label names.\n",
      "  When the task is sequence labeling/token classification, there are two formats of the labels:\n",
      "  (1) The token labels, i.e., [B-PER, I-PER, B-LOC]; (2) Id labels. For (2), need to pass the label_list (e.g., [B-PER, I-PER, B-LOC])\n",
      "  to convert the Id to token labels when computing the metric with metric_loss_score.\n",
      "  See the example in [a simple token classification example](../../../../Examples/AutoML-NLP#a-simple-token-classification-example).\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: tft\n",
      "title: automl.time_series.tft\n",
      "---\n",
      "\n",
      "## TemporalFusionTransformerEstimator Objects\n",
      "\n",
      "```python\n",
      "class TemporalFusionTransformerEstimator(TimeSeriesEstimator)\n",
      "```\n",
      "\n",
      "The class for tuning Temporal Fusion Transformer\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: utils\n",
      "title: automl.nlp.utils\n",
      "---\n",
      "\n",
      "#### format\\_vars\n",
      "\n",
      "```python\n",
      "def format_vars(resolved_vars: Dict) -> str\n",
      "```\n",
      "\n",
      "Formats the resolved variable dict into a single string.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[flaml.autogen.oai.completion: 08-03 22:04:08] {212} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/code/FLAML1/flaml/autogen/oai/completion.py\", line 204, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 743, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flaml.autogen.oai.completion:retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lijiang1/code/FLAML1/flaml/autogen/oai/completion.py\", line 204, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/lijiang1/anaconda3/envs/flaml-oss/lib/python3.8/site-packages/openai/api_requestor.py\", line 743, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "Based on the provided context, I cannot find a function named `tune_automl` in FLAML. The context includes information about the AutoML class, AutoVW class and some other classes and functions, but no `tune_automl` function is mentioned.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Based on the provided context, I cannot find a function named `tune_automl` in FLAML. The context includes information about the AutoML class, AutoVW class and some other classes and functions, but no `tune_automl` function is mentioned.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "# set `human_input_mode` to be `ALWAYS`, so the agent will ask for human input at every step.\n",
    "ragproxyagent.human_input_mode = \"ALWAYS\"\n",
    "qa_problem = \"Is there a function named `tune_automl` in FLAML?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=qa_problem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
