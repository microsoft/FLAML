{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43f7-wG-Tjg_"
   },
   "source": [
    "# FineTuning NLP Models with FLAML Library\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "FLAML is a Python library (https://github.com/microsoft/FLAML) designed to automatically produce accurate machine learning models \n",
    "with low computational cost. It is fast and economical. The simple and lightweight design makes it easy to use and extend, such as adding new learners. FLAML can \n",
    "- serve as an economical AutoML engine,\n",
    "- be used as a fast hyperparameter tuning tool, or \n",
    "- be embedded in self-tuning software that requires low latency & resource in repetitive\n",
    "   tuning tasks.\n",
    "\n",
    "In this notebook, we demonstrate how to use the FLAML library to fine tune an NLP language model with hyperparameter search. We will use [flaml.tune](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function) with the built in GPU in colab for the tuning. However, if you have a machine with more than 1 GPU, you can also use FLAML's [parallel tuning](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) with the ray tune option. \n",
    "\n",
    "FLAML requires `Python>=3.7`. To run this notebook example, please install flaml with the `nlp,notebook` and `blendsearch` option:\n",
    "```bash\n",
    "pip install flaml[nlp,notebook,blendsearch]; \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8c3VMy6TjhC",
    "outputId": "52360084-895f-45b5-d800-ef72e32e9bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: flaml[blendsearch,hf,notebook,ray] in /usr/local/lib/python3.9/dist-packages (1.1.3)\n",
      "\u001b[33mWARNING: flaml 1.1.3 does not provide the extra 'hf'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: lightgbm>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (3.3.5)\n",
      "Requirement already satisfied: xgboost>=0.90 in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (1.7.4)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (1.2.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (1.3.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (1.10.1)\n",
      "Requirement already satisfied: openml==0.10.2 in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (0.10.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (3.5.3)\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (1.0.0)\n",
      "Requirement already satisfied: ray[tune]~=1.13 in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (1.13.0)\n",
      "Requirement already satisfied: optuna==2.8.0 in /usr/local/lib/python3.9/dist-packages (from flaml[blendsearch,hf,notebook,ray]) (2.8.0)\n",
      "Requirement already satisfied: xmltodict in /usr/local/lib/python3.9/dist-packages (from openml==0.10.2->flaml[blendsearch,hf,notebook,ray]) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from openml==0.10.2->flaml[blendsearch,hf,notebook,ray]) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from openml==0.10.2->flaml[blendsearch,hf,notebook,ray]) (2.25.1)\n",
      "Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from openml==0.10.2->flaml[blendsearch,hf,notebook,ray]) (2.5.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (1.4.46)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.9/dist-packages (from optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (1.10.2)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (0.9.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.9/dist-packages (from optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (6.7.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (23.0)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.9/dist-packages (from optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (4.2.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from lightgbm>=2.3.1->flaml[blendsearch,hf,notebook,ray]) (0.38.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.4->flaml[blendsearch,hf,notebook,ray]) (2022.7.1)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (1.43.0)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (3.19.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (3.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (6.0)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (1.3.3)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (1.3.1)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (8.0.4)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (22.2.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (4.3.3)\n",
      "Requirement already satisfied: virtualenv in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (20.20.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (1.0.4)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (0.8.10)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.9/dist-packages (from ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (2.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.24->flaml[blendsearch,hf,notebook,ray]) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.24->flaml[blendsearch,hf,notebook,ray]) (3.1.0)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.9/dist-packages (from jupyter->flaml[blendsearch,hf,notebook,ray]) (6.3.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from jupyter->flaml[blendsearch,hf,notebook,ray]) (6.5.4)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.9/dist-packages (from jupyter->flaml[blendsearch,hf,notebook,ray]) (6.1.0)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (from jupyter->flaml[blendsearch,hf,notebook,ray]) (7.7.1)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from jupyter->flaml[blendsearch,hf,notebook,ray]) (5.3.4)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.9/dist-packages (from jupyter->flaml[blendsearch,hf,notebook,ray]) (5.4.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flaml[blendsearch,hf,notebook,ray]) (4.39.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flaml[blendsearch,hf,notebook,ray]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flaml[blendsearch,hf,notebook,ray]) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flaml[blendsearch,hf,notebook,ray]) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flaml[blendsearch,hf,notebook,ray]) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.9/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (1.15.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.1.0->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (2.0.2)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.9/dist-packages (from alembic->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (4.5.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.9/dist-packages (from cliff->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from cliff->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (6.0.0)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from cliff->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (2.4.3)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from cliff->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (0.5.1)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from cliff->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (5.0.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (6.2)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (6.1.12)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (5.7.1)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (7.9.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter->flaml[blendsearch,hf,notebook,ray]) (3.6.2)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter->flaml[blendsearch,hf,notebook,ray]) (3.0.5)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (0.19.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter->flaml[blendsearch,hf,notebook,ray]) (2.0.10)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter->flaml[blendsearch,hf,notebook,ray]) (2.6.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (2.1.2)\n",
      "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (3.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (4.6.3)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.7.1)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (5.2.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (1.5.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.2.2)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (4.9.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (6.0.0)\n",
      "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (5.7.3)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (1.2.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.8.4)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->flaml[blendsearch,hf,notebook,ray]) (21.3.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.16.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->flaml[blendsearch,hf,notebook,ray]) (23.2.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->flaml[blendsearch,hf,notebook,ray]) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.17.1)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from qtconsole->jupyter->flaml[blendsearch,hf,notebook,ray]) (2.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->openml==0.10.2->flaml[blendsearch,hf,notebook,ray]) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->openml==0.10.2->flaml[blendsearch,hf,notebook,ray]) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->openml==0.10.2->flaml[blendsearch,hf,notebook,ray]) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->openml==0.10.2->flaml[blendsearch,hf,notebook,ray]) (2.10)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.9/dist-packages (from virtualenv->ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.9/dist-packages (from virtualenv->ray[tune]~=1.13->flaml[blendsearch,hf,notebook,ray]) (3.1.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.9/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (1.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (0.2.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->cliff->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (3.15.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.18.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (57.4.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (4.4.2)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (2.16.3)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from stevedore>=2.0.1->cliff->optuna==2.8.0->flaml[blendsearch,hf,notebook,ray]) (5.11.1)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.9/dist-packages (from terminado>=0.8.3->notebook->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook->jupyter->flaml[blendsearch,hf,notebook,ray]) (21.2.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->flaml[blendsearch,hf,notebook,ray]) (0.8.3)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->flaml[blendsearch,hf,notebook,ray]) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->flaml[blendsearch,hf,notebook,ray]) (2.21)\n"
     ]
    }
   ],
   "source": [
    "%pip install flaml[notebook,blendsearch,ray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lo1id59ntQX_",
    "outputId": "24eee553-1f29-4f6d-c2ad-5c4b1a90e4f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
      "Collecting charset-normalizer<4.0,>=2.0\n",
      "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: xxhash, multidict, dill, charset-normalizer, async-timeout, yarl, responses, multiprocess, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 async-timeout-4.0.2 charset-normalizer-3.1.0 datasets-2.10.1 dill-0.3.6 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.22.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (8.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (2022.6.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (4.65.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=cacebd03ac9a1d4da80fa2c3b056f50e1c8f83784aac52e45f9357f8f3da338f\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j24pfyQktbln",
    "outputId": "de84ceaf-50b0-4d20-c8a8-5a3ab1cb9c11"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'4.26.1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efPlAWTdTjhD"
   },
   "source": [
    "Let's run some examples. To use CoLab's built in GPU, you need to select Runtime -> Change runtime type and select GPU. Then you can print the device information using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kx9QbI7uaU8",
    "outputId": "a77fb70f-e380-4fc0-babe-97643339845e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.cuda.device object at 0x7f8d78618040>]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print([torch.cuda.device(i) for i in range(torch.cuda.device_count())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yEuLXoHua-f"
   },
   "source": [
    "Note: throughout this notebook, you may see a few ModuleNotFoundErrors. As long as the cell successfully executes, you can ignore that error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBr83DYlTjhD"
   },
   "source": [
    "## 2. Sentiment Classification Example\n",
    "### Load data and preprocess\n",
    "\n",
    "The Stanford Sentiment treebank (SST-2) dataset is a dataset for sentiment classification. First, let's load this dataset into pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGP2eqTBTjhD",
    "outputId": "2030ea8e-2c90-400f-8b52-bb399a5f1c80"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c34b457ca14ac4860d1d1cbc8aa26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/28.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d8b8b00c3843eb810ea2f199c5fe0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7727bd28f294d938e981b1fe1bc40bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/27.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2289c065a6424ad68df9c67cd5de1aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f2387c293d452e943a07225c1598be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3a7089ab8e4436bb21ed5914b7c78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56580e10012945d98054841b8f998b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"glue\", \"sst2\", split=\"train\").to_pandas()\n",
    "dev_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\").to_pandas()\n",
    "test_dataset = load_dataset(\"glue\", \"sst2\", split=\"test\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb7SAWVLTjhE"
   },
   "source": [
    "Take a look at the first 5 examples of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65mLkoJhTjhE",
    "outputId": "9e348ad8-b683-4699-b452-9d5cd533636b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e7110696-f2db-44cd-b3ff-3e17002cbe80\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7110696-f2db-44cd-b3ff-3e17002cbe80')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e7110696-f2db-44cd-b3ff-3e17002cbe80 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e7110696-f2db-44cd-b3ff-3e17002cbe80');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                            sentence  label  idx\n",
       "0       hide new secretions from the parental units       0    0\n",
       "1               contains no wit , only labored gags       0    1\n",
       "2  that loves its characters and communicates som...      1    2\n",
       "3  remains utterly satisfied to remain the same t...      0    3\n",
       "4  on the worst revenge-of-the-nerds clichés the ...      0    4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENcUQbOgTjhE"
   },
   "source": [
    "Separate the data into X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GA0VH9URTjhF"
   },
   "outputs": [],
   "source": [
    "custom_sent_keys = [\"sentence\"]          # specify the column names of the input sentences\n",
    "label_key = \"label\"                                    # specify the column name of the label\n",
    "\n",
    "X_train, y_train = train_dataset[custom_sent_keys], train_dataset[label_key]\n",
    "X_val, y_val = dev_dataset[custom_sent_keys], dev_dataset[label_key]\n",
    "X_test = test_dataset[custom_sent_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpRqB153TjhF"
   },
   "source": [
    "### Run FLAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kXabqxZuzQl"
   },
   "source": [
    "Now we can run AutoML with FLAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asYbkzrXTjhF"
   },
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "automl = AutoML()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XZmrBRru_A0"
   },
   "source": [
    "Let's run FLAML for 30 mins. Here we use Electra's [small model](https://huggingface.co/google/electra-small-discriminator) for the tuning. We set gpu_per_trial to 1, and n_concurrent_trials to 1 (the number of trials running at the same time). Make sure gpu_per_trial * n_concurrent_trials does not exceed the GPU number you have. While running you can observe the resource usage (including the GPU) on the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEvR2bZiTjhG"
   },
   "outputs": [],
   "source": [
    "MAX_ITER=20\n",
    "automl_settings = {\n",
    "    \"max_iter\": MAX_ITER,                  # setting the time budget\n",
    "    \"task\": \"seq-classification\",       # setting the task as seq-classification\n",
    "    \"fit_kwargs_by_estimator\": {\n",
    "        \"transformer\": {\n",
    "            \"output_dir\": \"data/output/\",   # setting the output directory\n",
    "            \"model_path\": \"google/electra-small-discriminator\",  # if model_path is not set, the default model is facebook/muppet-roberta-base: https://huggingface.co/facebook/muppet-roberta-base\n",
    "        }\n",
    "    },\n",
    "    \"gpu_per_trial\": 1,                 # using 1 GPU for each trial\n",
    "    \"log_file_name\": \"seqclass.log\",    # set the file to save the log for HPO\n",
    "    \"log_type\": \"all\",                  # the log type for trials: \"all\" if logging all the trials, \"better\" if only keeping the better trials\n",
    "    \"use_ray\": False,                   # If parallel tuning, set \"use_ray\" to {\"local_dir\": \"data/output/\"}\n",
    "    \"n_concurrent_trials\": 1,           # How many trials to run at the same time, n_concurrent_trials * gpu_per_trial must not exceed the total number of GPUs\n",
    "    \"keep_search_state\": True,          # keeping the search state\n",
    "  #  \"fp16\": False                       # whether to use fp16, this option is True by default. \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXjF65hOTjhG",
    "outputId": "acb609b4-e9f0-418e-c8ad-5c840087d457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:25:14] {2716} INFO - task = seq-classification\n",
      "[flaml.automl.automl: 02-13 00:25:14] {2718} INFO - Data split method: stratified\n",
      "[flaml.automl.automl: 02-13 00:25:14] {2721} INFO - Evaluation method: holdout\n",
      "[flaml.automl.automl: 02-13 00:25:14] {2848} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.automl: 02-13 00:25:14] {2994} INFO - List of ML learners in AutoML Run: ['transformer']\n",
      "[flaml.automl.automl: 02-13 00:25:14] {3323} INFO - iteration 0, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3641: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c647b6cb658147e88d2788d45a9548a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d5d915c3c94a3b80b9534afa7d0554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993f6c1a64cc40da98a85858c11e2011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1660829cada740aebc11c3e067ebb8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d12202659b741bcabfb41ce5120dfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5731, 'learning_rate': 4.6751863684771026e-06, 'epoch': 1.6}\n",
      "{'eval_loss': 0.43649888038635254, 'eval_automl_metric': 0.1754587155963303, 'eval_runtime': 12.0627, 'eval_samples_per_second': 72.289, 'eval_steps_per_second': 72.289, 'epoch': 2.0}\n",
      "{'eval_loss': 0.40600481629371643, 'eval_automl_metric': 0.16284403669724767, 'eval_runtime': 10.5259, 'eval_samples_per_second': 82.843, 'eval_steps_per_second': 82.843, 'epoch': 3.0}\n",
      "{'train_runtime': 98.6431, 'train_samples_per_second': 304.127, 'train_steps_per_second': 9.519, 'train_loss': 0.49010642217244077, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:27:36] {3461} INFO - Estimated sufficient time budget=67349s. Estimated necessary time budget=67s.\n",
      "[flaml.automl.automl: 02-13 00:27:36] {3508} INFO -  at 141.6s,\testimator transformer's best error=0.1628,\tbest estimator transformer's best error=0.1628\n",
      "[flaml.automl.automl: 02-13 00:27:36] {3323} INFO - iteration 1, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4873640537261963, 'eval_automl_metric': 0.18463302752293576, 'eval_runtime': 12.6354, 'eval_samples_per_second': 69.012, 'eval_steps_per_second': 69.012, 'epoch': 2.0}\n",
      "{'eval_loss': 0.4638785123825073, 'eval_automl_metric': 0.18119266055045868, 'eval_runtime': 12.7766, 'eval_samples_per_second': 68.25, 'eval_steps_per_second': 68.25, 'epoch': 3.0}\n",
      "{'train_runtime': 72.1158, 'train_samples_per_second': 415.997, 'train_steps_per_second': 6.531, 'train_loss': 0.5612566192691746, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:29:18] {3508} INFO -  at 243.9s,\testimator transformer's best error=0.1628,\tbest estimator transformer's best error=0.1628\n",
      "[flaml.automl.automl: 02-13 00:29:18] {3323} INFO - iteration 2, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5903, 'learning_rate': 7.550901222797876e-06, 'epoch': 0.8}\n",
      "{'loss': 0.3877, 'learning_rate': 4.805118959962285e-06, 'epoch': 1.6}\n",
      "{'eval_loss': 0.36965882778167725, 'eval_automl_metric': 0.1513761467889908, 'eval_runtime': 12.9701, 'eval_samples_per_second': 67.232, 'eval_steps_per_second': 67.232, 'epoch': 2.0}\n",
      "{'loss': 0.3432, 'learning_rate': 2.0593366971266936e-06, 'epoch': 2.4}\n",
      "{'eval_loss': 0.371982604265213, 'eval_automl_metric': 0.1513761467889908, 'eval_runtime': 11.1194, 'eval_samples_per_second': 78.422, 'eval_steps_per_second': 78.422, 'epoch': 3.0}\n",
      "{'train_runtime': 153.8165, 'train_samples_per_second': 195.038, 'train_steps_per_second': 12.19, 'train_loss': 0.41677740885416664, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_00-25-15/train_e2cbf9c4_1_s=9223372036854775807,e=1e-05,s=-1,s=3,e=32,d=20_2023-02-13_00-25-15/checkpoint-939 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:32:19] {3508} INFO -  at 424.9s,\testimator transformer's best error=0.1514,\tbest estimator transformer's best error=0.1514\n",
      "[flaml.automl.automl: 02-13 00:32:19] {3323} INFO - iteration 3, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5324, 'learning_rate': 8.879996750213199e-06, 'epoch': 0.8}\n",
      "{'eval_loss': 0.3838350474834442, 'eval_automl_metric': 0.15366972477064222, 'eval_runtime': 12.7994, 'eval_samples_per_second': 68.128, 'eval_steps_per_second': 68.128, 'epoch': 1.0}\n",
      "{'loss': 0.3629, 'learning_rate': 2.959998916737733e-06, 'epoch': 1.6}\n",
      "{'eval_loss': 0.3726535737514496, 'eval_automl_metric': 0.14678899082568808, 'eval_runtime': 12.0318, 'eval_samples_per_second': 72.475, 'eval_steps_per_second': 72.475, 'epoch': 2.0}\n",
      "{'train_runtime': 112.1491, 'train_samples_per_second': 178.334, 'train_steps_per_second': 11.146, 'train_loss': 0.42414498901367187, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_00-29-18/train_74241bc2_3_s=9223372036854775807,e=1.0297e-05,s=-1,s=3,e=16,d=26_2023-02-13_00-29-18/checkpoint-1250 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:34:41] {3508} INFO -  at 566.7s,\testimator transformer's best error=0.1468,\tbest estimator transformer's best error=0.1468\n",
      "[flaml.automl.automl: 02-13 00:34:41] {3323} INFO - iteration 4, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.505, 'learning_rate': 1.543094173639824e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.38370218873023987, 'eval_automl_metric': 0.16284403669724767, 'eval_runtime': 12.1256, 'eval_samples_per_second': 71.914, 'eval_steps_per_second': 71.914, 'epoch': 1.0}\n",
      "{'loss': 0.334, 'learning_rate': 5.14364724546608e-06, 'epoch': 1.6}\n",
      "{'eval_loss': 0.359171986579895, 'eval_automl_metric': 0.14220183486238536, 'eval_runtime': 12.8676, 'eval_samples_per_second': 67.767, 'eval_steps_per_second': 67.767, 'epoch': 2.0}\n",
      "{'train_runtime': 111.5411, 'train_samples_per_second': 179.306, 'train_steps_per_second': 11.207, 'train_loss': 0.3914000427246094, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_00-32-19/train_e00c1c18_4_s=9223372036854775807,e=1.48e-05,s=-1,s=2,e=16,d=25_2023-02-13_00-32-19/checkpoint-1250 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:37:02] {3508} INFO -  at 707.2s,\testimator transformer's best error=0.1422,\tbest estimator transformer's best error=0.1422\n",
      "[flaml.automl.automl: 02-13 00:37:02] {3323} INFO - iteration 5, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5324, 'learning_rate': 8.879996750213194e-06, 'epoch': 0.8}\n",
      "{'eval_loss': 0.3838350474834442, 'eval_automl_metric': 0.15366972477064222, 'eval_runtime': 10.9249, 'eval_samples_per_second': 79.818, 'eval_steps_per_second': 79.818, 'epoch': 1.0}\n",
      "{'loss': 0.3629, 'learning_rate': 2.9599989167377317e-06, 'epoch': 1.6}\n",
      "{'eval_loss': 0.3726535737514496, 'eval_automl_metric': 0.14678899082568808, 'eval_runtime': 12.9539, 'eval_samples_per_second': 67.316, 'eval_steps_per_second': 67.316, 'epoch': 2.0}\n",
      "{'train_runtime': 112.0161, 'train_samples_per_second': 178.546, 'train_steps_per_second': 11.159, 'train_loss': 0.42414498901367187, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:39:22] {3508} INFO -  at 847.2s,\testimator transformer's best error=0.1422,\tbest estimator transformer's best error=0.1422\n",
      "[flaml.automl.automl: 02-13 00:39:22] {3323} INFO - iteration 6, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4443, 'learning_rate': 3.3201834726658944e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.36220869421958923, 'eval_automl_metric': 0.14678899082568808, 'eval_runtime': 11.431, 'eval_samples_per_second': 76.284, 'eval_steps_per_second': 76.284, 'epoch': 1.0}\n",
      "{'loss': 0.2946, 'learning_rate': 1.106727824221965e-05, 'epoch': 1.6}\n",
      "{'eval_loss': 0.3497336804866791, 'eval_automl_metric': 0.125, 'eval_runtime': 12.9801, 'eval_samples_per_second': 67.18, 'eval_steps_per_second': 67.18, 'epoch': 2.0}\n",
      "{'train_runtime': 112.2927, 'train_samples_per_second': 178.106, 'train_steps_per_second': 11.132, 'train_loss': 0.3430988311767578, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_00-34-41/train_348c34e4_5_s=9223372036854775807,e=2.5718e-05,s=-1,s=2,e=16,d=31_2023-02-13_00-34-41/checkpoint-1250 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:41:42] {3508} INFO -  at 987.7s,\testimator transformer's best error=0.1250,\tbest estimator transformer's best error=0.1250\n",
      "[flaml.automl.automl: 02-13 00:41:42] {3323} INFO - iteration 7, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.505, 'learning_rate': 1.543094173639823e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.38370218873023987, 'eval_automl_metric': 0.16284403669724767, 'eval_runtime': 12.736, 'eval_samples_per_second': 68.467, 'eval_steps_per_second': 68.467, 'epoch': 1.0}\n",
      "{'loss': 0.334, 'learning_rate': 5.143647245466077e-06, 'epoch': 1.6}\n",
      "{'eval_loss': 0.359171986579895, 'eval_automl_metric': 0.14220183486238536, 'eval_runtime': 12.4032, 'eval_samples_per_second': 70.304, 'eval_steps_per_second': 70.304, 'epoch': 2.0}\n",
      "{'train_runtime': 111.8954, 'train_samples_per_second': 178.738, 'train_steps_per_second': 11.171, 'train_loss': 0.3914000427246094, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:44:01] {3508} INFO -  at 1126.2s,\testimator transformer's best error=0.1250,\tbest estimator transformer's best error=0.1250\n",
      "[flaml.automl.automl: 02-13 00:44:01] {3323} INFO - iteration 8, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4925, 'learning_rate': 4.960961475644055e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3719, 'learning_rate': 3.720721106733041e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.32833412289619446, 'eval_automl_metric': 0.12729357798165142, 'eval_runtime': 12.8717, 'eval_samples_per_second': 67.746, 'eval_steps_per_second': 67.746, 'epoch': 1.0}\n",
      "{'loss': 0.2951, 'learning_rate': 2.4804807378220275e-05, 'epoch': 1.2}\n",
      "{'loss': 0.287, 'learning_rate': 1.2402403689110137e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2508, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "{'eval_loss': 0.4392344057559967, 'eval_automl_metric': 0.11926605504587151, 'eval_runtime': 12.9208, 'eval_samples_per_second': 67.488, 'eval_steps_per_second': 67.488, 'epoch': 2.0}\n",
      "{'train_runtime': 198.5167, 'train_samples_per_second': 100.747, 'train_steps_per_second': 12.593, 'train_loss': 0.3394467529296875, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_00-39-22/train_dbc10ec4_7_s=9223372036854775807,e=5.5336e-05,s=-1,s=2,e=16,d=29_2023-02-13_00-39-22/checkpoint-1250 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:47:48] {3508} INFO -  at 1353.5s,\testimator transformer's best error=0.1193,\tbest estimator transformer's best error=0.1193\n",
      "[flaml.automl.automl: 02-13 00:47:48] {3323} INFO - iteration 9, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4951, 'learning_rate': 3.355118619720658e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3835, 'learning_rate': 2.516338964790493e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.40686866641044617, 'eval_automl_metric': 0.14564220183486243, 'eval_runtime': 12.9366, 'eval_samples_per_second': 67.406, 'eval_steps_per_second': 67.406, 'epoch': 1.0}\n",
      "{'loss': 0.3335, 'learning_rate': 1.677559309860329e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2949, 'learning_rate': 8.387796549301644e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2842, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "{'eval_loss': 0.368007630109787, 'eval_automl_metric': 0.11353211009174313, 'eval_runtime': 12.7474, 'eval_samples_per_second': 68.406, 'eval_steps_per_second': 68.406, 'epoch': 2.0}\n",
      "{'train_runtime': 197.7826, 'train_samples_per_second': 101.121, 'train_steps_per_second': 12.64, 'train_loss': 0.35824859619140625, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_00-44-01/train_8210a316_9_s=9223372036854775807,e=6.2012e-05,s=-1,s=2,e=8,d=29_2023-02-13_00-44-01/checkpoint-2500 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:51:34] {3508} INFO -  at 1579.6s,\testimator transformer's best error=0.1135,\tbest estimator transformer's best error=0.1135\n",
      "[flaml.automl.automl: 02-13 00:51:34] {3323} INFO - iteration 10, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4925, 'learning_rate': 4.960961475644053e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3719, 'learning_rate': 3.7207211067330393e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.32833412289619446, 'eval_automl_metric': 0.12729357798165142, 'eval_runtime': 12.4911, 'eval_samples_per_second': 69.809, 'eval_steps_per_second': 69.809, 'epoch': 1.0}\n",
      "{'loss': 0.2951, 'learning_rate': 2.4804807378220265e-05, 'epoch': 1.2}\n",
      "{'loss': 0.287, 'learning_rate': 1.2402403689110132e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2508, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "{'eval_loss': 0.4392344057559967, 'eval_automl_metric': 0.11926605504587151, 'eval_runtime': 10.9419, 'eval_samples_per_second': 79.694, 'eval_steps_per_second': 79.694, 'epoch': 2.0}\n",
      "{'train_runtime': 197.751, 'train_samples_per_second': 101.137, 'train_steps_per_second': 12.642, 'train_loss': 0.3394467529296875, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:55:19] {3508} INFO -  at 1804.5s,\testimator transformer's best error=0.1135,\tbest estimator transformer's best error=0.1135\n",
      "[flaml.automl.automl: 02-13 00:55:19] {3323} INFO - iteration 11, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5649, 'learning_rate': 1.6003786830637153e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3906, 'learning_rate': 1.2002840122977863e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.4519190788269043, 'eval_automl_metric': 0.17087155963302747, 'eval_runtime': 11.4068, 'eval_samples_per_second': 76.445, 'eval_steps_per_second': 76.445, 'epoch': 1.0}\n",
      "{'loss': 0.3366, 'learning_rate': 8.001893415318577e-06, 'epoch': 1.2}\n",
      "{'loss': 0.3267, 'learning_rate': 4.000946707659288e-06, 'epoch': 1.6}\n",
      "{'loss': 0.3139, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3998706638813019, 'eval_automl_metric': 0.13417431192660545, 'eval_runtime': 13.0361, 'eval_samples_per_second': 66.891, 'eval_steps_per_second': 66.891, 'epoch': 2.0}\n",
      "{'train_runtime': 199.0852, 'train_samples_per_second': 100.459, 'train_steps_per_second': 12.557, 'train_loss': 0.3865417297363281, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 00:59:07] {3508} INFO -  at 2032.8s,\testimator transformer's best error=0.1135,\tbest estimator transformer's best error=0.1135\n",
      "[flaml.automl.automl: 02-13 00:59:07] {3323} INFO - iteration 12, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4763, 'learning_rate': 7.033848345721858e-05, 'epoch': 0.4}\n",
      "{'loss': 0.4177, 'learning_rate': 5.2753862592913935e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.4707236886024475, 'eval_automl_metric': 0.1513761467889908, 'eval_runtime': 12.9802, 'eval_samples_per_second': 67.179, 'eval_steps_per_second': 67.179, 'epoch': 1.0}\n",
      "{'loss': 0.3258, 'learning_rate': 3.516924172860929e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2725, 'learning_rate': 1.7584620864304646e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2605, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "{'eval_loss': 0.49139130115509033, 'eval_automl_metric': 0.13990825688073394, 'eval_runtime': 13.1322, 'eval_samples_per_second': 66.402, 'eval_steps_per_second': 66.402, 'epoch': 2.0}\n",
      "{'train_runtime': 199.6589, 'train_samples_per_second': 100.171, 'train_steps_per_second': 12.521, 'train_loss': 0.3505607055664062, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 01:02:55] {3508} INFO -  at 2260.8s,\testimator transformer's best error=0.1135,\tbest estimator transformer's best error=0.1135\n",
      "[flaml.automl.automl: 02-13 01:02:55] {3323} INFO - iteration 13, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4729, 'learning_rate': 2.059405133813667e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.3603905141353607, 'eval_automl_metric': 0.14564220183486243, 'eval_runtime': 11.161, 'eval_samples_per_second': 78.129, 'eval_steps_per_second': 78.129, 'epoch': 1.0}\n",
      "{'loss': 0.3122, 'learning_rate': 6.86468377937889e-06, 'epoch': 1.6}\n",
      "{'eval_loss': 0.3239300847053528, 'eval_automl_metric': 0.1307339449541285, 'eval_runtime': 12.9568, 'eval_samples_per_second': 67.3, 'eval_steps_per_second': 67.3, 'epoch': 2.0}\n",
      "{'train_runtime': 112.89, 'train_samples_per_second': 177.164, 'train_steps_per_second': 11.073, 'train_loss': 0.3677707702636719, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 01:05:17] {3508} INFO -  at 2402.1s,\testimator transformer's best error=0.1135,\tbest estimator transformer's best error=0.1135\n",
      "[flaml.automl.automl: 02-13 01:05:17] {3323} INFO - iteration 14, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4923, 'learning_rate': 4.069361563798855e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3861, 'learning_rate': 3.944824852946889e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3634, 'learning_rate': 3.8202881420949226e-05, 'epoch': 0.18}\n",
      "{'loss': 0.368, 'learning_rate': 3.6957514312429564e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3541, 'learning_rate': 3.5712147203909896e-05, 'epoch': 0.3}\n",
      "{'loss': 0.349, 'learning_rate': 3.4466780095390234e-05, 'epoch': 0.36}\n",
      "{'loss': 0.322, 'learning_rate': 3.3221412986870565e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3086, 'learning_rate': 3.19760458783509e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3231, 'learning_rate': 3.073067876983124e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3047, 'learning_rate': 2.9485311661311575e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3052, 'learning_rate': 2.823994455279191e-05, 'epoch': 0.65}\n",
      "{'loss': 0.274, 'learning_rate': 2.6994577444272248e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2929, 'learning_rate': 2.574921033575258e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2996, 'learning_rate': 2.4503843227232917e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2735, 'learning_rate': 2.3258476118713252e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2682, 'learning_rate': 2.201310901019359e-05, 'epoch': 0.95}\n",
      "{'eval_loss': 0.39278969168663025, 'eval_automl_metric': 0.10894495412844041, 'eval_runtime': 12.8977, 'eval_samples_per_second': 67.609, 'eval_steps_per_second': 67.609, 'epoch': 1.0}\n",
      "{'loss': 0.2653, 'learning_rate': 2.0767741901673924e-05, 'epoch': 1.01}\n",
      "{'loss': 0.225, 'learning_rate': 1.952237479315426e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2228, 'learning_rate': 1.8277007684634594e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2253, 'learning_rate': 1.7031640576114932e-05, 'epoch': 1.19}\n",
      "{'loss': 0.2137, 'learning_rate': 1.5786273467595266e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2255, 'learning_rate': 1.4540906359075603e-05, 'epoch': 1.31}\n",
      "{'loss': 0.2269, 'learning_rate': 1.3295539250555937e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1927, 'learning_rate': 1.2050172142036274e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2241, 'learning_rate': 1.0804805033516608e-05, 'epoch': 1.48}\n",
      "{'loss': 0.1966, 'learning_rate': 9.559437924996944e-06, 'epoch': 1.54}\n",
      "{'loss': 0.1995, 'learning_rate': 8.31407081647728e-06, 'epoch': 1.6}\n",
      "{'loss': 0.1947, 'learning_rate': 7.0687037079576146e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2044, 'learning_rate': 5.823336599437951e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2211, 'learning_rate': 4.577969490918286e-06, 'epoch': 1.78}\n",
      "{'loss': 0.213, 'learning_rate': 3.3326023823986218e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2114, 'learning_rate': 2.0872352738789577e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2087, 'learning_rate': 8.418681653592932e-07, 'epoch': 1.96}\n",
      "{'eval_loss': 0.4246375262737274, 'eval_automl_metric': 0.09747706422018354, 'eval_runtime': 11.5236, 'eval_samples_per_second': 75.671, 'eval_steps_per_second': 75.671, 'epoch': 2.0}\n",
      "{'train_runtime': 1192.7324, 'train_samples_per_second': 112.932, 'train_steps_per_second': 14.117, 'train_loss': 0.2704715459161847, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_00-47-48/train_09860728_10_s=9223372036854775807,e=4.1939e-05,s=-1,s=2,e=8,d=23_2023-02-13_00-47-48/checkpoint-2500 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 01:26:04] {3508} INFO -  at 3650.0s,\testimator transformer's best error=0.0975,\tbest estimator transformer's best error=0.0975\n",
      "[flaml.automl.automl: 02-13 01:26:04] {3323} INFO - iteration 15, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5739, 'learning_rate': 2.408968303739432e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4011, 'learning_rate': 2.2568677668194023e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3829, 'learning_rate': 2.1047672298993725e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3575, 'learning_rate': 1.9526666929793427e-05, 'epoch': 0.24}\n",
      "{'loss': 0.363, 'learning_rate': 1.800566156059313e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3042, 'learning_rate': 1.6484656191392832e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3367, 'learning_rate': 1.4963650822192532e-05, 'epoch': 0.42}\n",
      "{'loss': 0.316, 'learning_rate': 1.3442645452992235e-05, 'epoch': 0.48}\n",
      "{'loss': 0.31, 'learning_rate': 1.1921640083791935e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2886, 'learning_rate': 1.0400634714591638e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2969, 'learning_rate': 8.87962934539134e-06, 'epoch': 0.65}\n",
      "{'loss': 0.2877, 'learning_rate': 7.358623976191042e-06, 'epoch': 0.71}\n",
      "{'loss': 0.2849, 'learning_rate': 5.837618606990744e-06, 'epoch': 0.77}\n",
      "{'loss': 0.305, 'learning_rate': 4.316613237790445e-06, 'epoch': 0.83}\n",
      "{'loss': 0.2853, 'learning_rate': 2.7956078685901475e-06, 'epoch': 0.89}\n",
      "{'loss': 0.2602, 'learning_rate': 1.2746024993898498e-06, 'epoch': 0.95}\n",
      "{'eval_loss': 0.3659135401248932, 'eval_automl_metric': 0.10779816513761464, 'eval_runtime': 13.039, 'eval_samples_per_second': 66.876, 'eval_steps_per_second': 66.876, 'epoch': 1.0}\n",
      "{'train_runtime': 606.6203, 'train_samples_per_second': 111.023, 'train_steps_per_second': 13.879, 'train_loss': 0.331692495458213, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 01:37:07] {3508} INFO -  at 4312.1s,\testimator transformer's best error=0.0975,\tbest estimator transformer's best error=0.0975\n",
      "[flaml.automl.automl: 02-13 01:37:07] {3323} INFO - iteration 16, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4741, 'learning_rate': 6.731793486594161e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4037, 'learning_rate': 6.595836111173186e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3736, 'learning_rate': 6.459878735752211e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3737, 'learning_rate': 6.323921360331236e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3497, 'learning_rate': 6.187963984910261e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3372, 'learning_rate': 6.0520066094892854e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3302, 'learning_rate': 5.9160492340683104e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3342, 'learning_rate': 5.7800918586473354e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3208, 'learning_rate': 5.6441344832263605e-05, 'epoch': 0.53}\n",
      "{'loss': 0.326, 'learning_rate': 5.508177107805385e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2936, 'learning_rate': 5.37221973238441e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3126, 'learning_rate': 5.236262356963435e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2791, 'learning_rate': 5.10030498154246e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2795, 'learning_rate': 4.964347606121484e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2932, 'learning_rate': 4.82839023070051e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2838, 'learning_rate': 4.692432855279534e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2678, 'learning_rate': 4.556475479858559e-05, 'epoch': 1.01}\n",
      "{'loss': 0.2232, 'learning_rate': 4.420518104437584e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2278, 'learning_rate': 4.284560729016609e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2215, 'learning_rate': 4.1486033535956336e-05, 'epoch': 1.19}\n",
      "{'loss': 0.2276, 'learning_rate': 4.012645978174659e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2221, 'learning_rate': 3.8766886027536836e-05, 'epoch': 1.31}\n",
      "{'loss': 0.2285, 'learning_rate': 3.7407312273327086e-05, 'epoch': 1.37}\n",
      "{'loss': 0.2212, 'learning_rate': 3.604773851911734e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2168, 'learning_rate': 3.4688164764907586e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2089, 'learning_rate': 3.3328591010697836e-05, 'epoch': 1.54}\n",
      "{'loss': 0.2235, 'learning_rate': 3.1969017256488087e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2245, 'learning_rate': 3.060944350227834e-05, 'epoch': 1.66}\n",
      "{'loss': 0.2016, 'learning_rate': 2.924986974806858e-05, 'epoch': 1.72}\n",
      "{'loss': 0.2177, 'learning_rate': 2.789029599385883e-05, 'epoch': 1.78}\n",
      "{'loss': 0.2062, 'learning_rate': 2.653072223964908e-05, 'epoch': 1.84}\n",
      "{'loss': 0.2109, 'learning_rate': 2.5171148485439327e-05, 'epoch': 1.9}\n",
      "{'loss': 0.2188, 'learning_rate': 2.3811574731229577e-05, 'epoch': 1.96}\n",
      "{'eval_loss': 0.4582779109477997, 'eval_automl_metric': 0.10894495412844041, 'eval_runtime': 13.2757, 'eval_samples_per_second': 65.684, 'eval_steps_per_second': 65.684, 'epoch': 2.0}\n",
      "{'loss': 0.1888, 'learning_rate': 2.2452000977019827e-05, 'epoch': 2.02}\n",
      "{'loss': 0.1694, 'learning_rate': 2.1092427222810074e-05, 'epoch': 2.08}\n",
      "{'loss': 0.1365, 'learning_rate': 1.9732853468600328e-05, 'epoch': 2.14}\n",
      "{'loss': 0.1475, 'learning_rate': 1.8373279714390578e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1739, 'learning_rate': 1.7013705960180824e-05, 'epoch': 2.26}\n",
      "{'loss': 0.1591, 'learning_rate': 1.5654132205971075e-05, 'epoch': 2.32}\n",
      "{'loss': 0.1716, 'learning_rate': 1.4294558451761321e-05, 'epoch': 2.38}\n",
      "{'loss': 0.1392, 'learning_rate': 1.293498469755157e-05, 'epoch': 2.43}\n",
      "{'loss': 0.1728, 'learning_rate': 1.1575410943341818e-05, 'epoch': 2.49}\n",
      "{'loss': 0.1383, 'learning_rate': 1.021583718913207e-05, 'epoch': 2.55}\n",
      "{'loss': 0.1444, 'learning_rate': 8.856263434922318e-06, 'epoch': 2.61}\n",
      "{'loss': 0.1564, 'learning_rate': 7.496689680712566e-06, 'epoch': 2.67}\n",
      "{'loss': 0.1404, 'learning_rate': 6.137115926502816e-06, 'epoch': 2.73}\n",
      "{'loss': 0.1456, 'learning_rate': 4.7775421722930654e-06, 'epoch': 2.79}\n",
      "{'loss': 0.1422, 'learning_rate': 3.417968418083314e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1517, 'learning_rate': 2.058394663873563e-06, 'epoch': 2.91}\n",
      "{'loss': 0.1477, 'learning_rate': 6.988209096638119e-07, 'epoch': 2.97}\n",
      "{'eval_loss': 0.5042977929115295, 'eval_automl_metric': 0.10665137614678899, 'eval_runtime': 13.1252, 'eval_samples_per_second': 66.437, 'eval_steps_per_second': 66.437, 'epoch': 3.0}\n",
      "{'train_runtime': 1807.2996, 'train_samples_per_second': 111.795, 'train_steps_per_second': 13.975, 'train_loss': 0.2342723334879161, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 02:08:10] {3508} INFO -  at 6175.7s,\testimator transformer's best error=0.0975,\tbest estimator transformer's best error=0.0975\n",
      "[flaml.automl.automl: 02-13 02:08:10] {3323} INFO - iteration 17, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4992, 'learning_rate': 4.02239864714216e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3841, 'learning_rate': 3.941161044703615e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3619, 'learning_rate': 3.859923442265069e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3485, 'learning_rate': 3.778685839826523e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3446, 'learning_rate': 3.6974482373879774e-05, 'epoch': 0.3}\n",
      "{'loss': 0.339, 'learning_rate': 3.616210634949431e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2968, 'learning_rate': 3.534973032510885e-05, 'epoch': 0.42}\n",
      "{'loss': 0.312, 'learning_rate': 3.4537354300723395e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3151, 'learning_rate': 3.372497827633794e-05, 'epoch': 0.53}\n",
      "{'loss': 0.314, 'learning_rate': 3.291260225195248e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3154, 'learning_rate': 3.2100226227567015e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2954, 'learning_rate': 3.128785020318156e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2894, 'learning_rate': 3.04754741787961e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2822, 'learning_rate': 2.966309815441064e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2924, 'learning_rate': 2.8850722130025183e-05, 'epoch': 0.89}\n",
      "{'loss': 0.277, 'learning_rate': 2.8038346105639725e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2471, 'learning_rate': 2.7225970081254262e-05, 'epoch': 1.01}\n",
      "{'loss': 0.2433, 'learning_rate': 2.6413594056868807e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2029, 'learning_rate': 2.560121803248335e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2298, 'learning_rate': 2.4788842008097886e-05, 'epoch': 1.19}\n",
      "{'loss': 0.2261, 'learning_rate': 2.397646598371243e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2079, 'learning_rate': 2.3164089959326972e-05, 'epoch': 1.31}\n",
      "{'loss': 0.2248, 'learning_rate': 2.235171393494151e-05, 'epoch': 1.37}\n",
      "{'loss': 0.2124, 'learning_rate': 2.1539337910556055e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2218, 'learning_rate': 2.0726961886170596e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2206, 'learning_rate': 1.9914585861785137e-05, 'epoch': 1.54}\n",
      "{'loss': 0.2285, 'learning_rate': 1.9102209837399678e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2194, 'learning_rate': 1.828983381301422e-05, 'epoch': 1.66}\n",
      "{'loss': 0.2036, 'learning_rate': 1.747745778862876e-05, 'epoch': 1.72}\n",
      "{'loss': 0.1845, 'learning_rate': 1.6665081764243302e-05, 'epoch': 1.78}\n",
      "{'loss': 0.2248, 'learning_rate': 1.5852705739857843e-05, 'epoch': 1.84}\n",
      "{'loss': 0.2226, 'learning_rate': 1.5040329715472383e-05, 'epoch': 1.9}\n",
      "{'loss': 0.188, 'learning_rate': 1.4227953691086926e-05, 'epoch': 1.96}\n",
      "{'eval_loss': 0.4531180262565613, 'eval_automl_metric': 0.10435779816513757, 'eval_runtime': 11.0855, 'eval_samples_per_second': 78.661, 'eval_steps_per_second': 78.661, 'epoch': 2.0}\n",
      "{'loss': 0.1916, 'learning_rate': 1.3415577666701467e-05, 'epoch': 2.02}\n",
      "{'loss': 0.1445, 'learning_rate': 1.2603201642316007e-05, 'epoch': 2.08}\n",
      "{'loss': 0.1602, 'learning_rate': 1.179082561793055e-05, 'epoch': 2.14}\n",
      "{'loss': 0.1474, 'learning_rate': 1.097844959354509e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1602, 'learning_rate': 1.016607356915963e-05, 'epoch': 2.26}\n",
      "{'loss': 0.175, 'learning_rate': 9.353697544774173e-06, 'epoch': 2.32}\n",
      "{'loss': 0.1466, 'learning_rate': 8.541321520388714e-06, 'epoch': 2.38}\n",
      "{'loss': 0.1767, 'learning_rate': 7.728945496003254e-06, 'epoch': 2.43}\n",
      "{'loss': 0.1566, 'learning_rate': 6.916569471617795e-06, 'epoch': 2.49}\n",
      "{'loss': 0.1339, 'learning_rate': 6.104193447232337e-06, 'epoch': 2.55}\n",
      "{'loss': 0.1568, 'learning_rate': 5.2918174228468786e-06, 'epoch': 2.61}\n",
      "{'loss': 0.1673, 'learning_rate': 4.47944139846142e-06, 'epoch': 2.67}\n",
      "{'loss': 0.1646, 'learning_rate': 3.667065374075961e-06, 'epoch': 2.73}\n",
      "{'loss': 0.1399, 'learning_rate': 2.8546893496905023e-06, 'epoch': 2.79}\n",
      "{'loss': 0.1653, 'learning_rate': 2.042313325305043e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1468, 'learning_rate': 1.2299373009195846e-06, 'epoch': 2.91}\n",
      "{'loss': 0.1713, 'learning_rate': 4.175612765341258e-07, 'epoch': 2.97}\n",
      "{'eval_loss': 0.5318318605422974, 'eval_automl_metric': 0.10206422018348627, 'eval_runtime': 11.6108, 'eval_samples_per_second': 75.102, 'eval_steps_per_second': 75.102, 'epoch': 3.0}\n",
      "{'train_runtime': 1796.2935, 'train_samples_per_second': 112.48, 'train_steps_per_second': 14.061, 'train_loss': 0.2324963676989416, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 02:39:00] {3508} INFO -  at 8025.9s,\testimator transformer's best error=0.0975,\tbest estimator transformer's best error=0.0975\n",
      "[flaml.automl.automl: 02-13 02:39:00] {3323} INFO - iteration 18, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5025, 'learning_rate': 4.031593722826686e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4022, 'learning_rate': 3.77704177669544e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3645, 'learning_rate': 3.522489830564193e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3443, 'learning_rate': 3.267937884432946e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3342, 'learning_rate': 3.0133859383016993e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3166, 'learning_rate': 2.7588339921704524e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3179, 'learning_rate': 2.5042820460392058e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3186, 'learning_rate': 2.2497300999079592e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2959, 'learning_rate': 1.9951781537767123e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2828, 'learning_rate': 1.7406262076454653e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3003, 'learning_rate': 1.4860742615142185e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2866, 'learning_rate': 1.231522315382972e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2788, 'learning_rate': 9.769703692517252e-06, 'epoch': 0.77}\n",
      "{'loss': 0.296, 'learning_rate': 7.224184231204782e-06, 'epoch': 0.83}\n",
      "{'loss': 0.2678, 'learning_rate': 4.6786647698923155e-06, 'epoch': 0.89}\n",
      "{'loss': 0.2514, 'learning_rate': 2.133145308579848e-06, 'epoch': 0.95}\n",
      "{'eval_loss': 0.33468347787857056, 'eval_automl_metric': 0.09403669724770647, 'eval_runtime': 11.7415, 'eval_samples_per_second': 74.267, 'eval_steps_per_second': 74.267, 'epoch': 1.0}\n",
      "{'train_runtime': 592.3182, 'train_samples_per_second': 113.704, 'train_steps_per_second': 14.214, 'train_loss': 0.320609719244371, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_01-05-17/train_7a85e7b6_15_s=9223372036854775807,e=4.1939e-05,s=-1,s=2,e=8,d=23_2023-02-13_01-05-17/checkpoint-16838 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 02:49:50] {3508} INFO -  at 8675.1s,\testimator transformer's best error=0.0940,\tbest estimator transformer's best error=0.0940\n",
      "[flaml.automl.automl: 02-13 02:49:50] {3323} INFO - iteration 19, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.515, 'learning_rate': 4.645015164493278e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4126, 'learning_rate': 4.351732226970025e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3849, 'learning_rate': 4.058449289446772e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3515, 'learning_rate': 3.76516635192352e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3525, 'learning_rate': 3.4718834144002665e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3334, 'learning_rate': 3.178600476877014e-05, 'epoch': 0.36}\n",
      "{'loss': 0.333, 'learning_rate': 2.885317539353761e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3061, 'learning_rate': 2.5920346018305086e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2958, 'learning_rate': 2.2987516643072554e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2959, 'learning_rate': 2.0054687267840025e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2981, 'learning_rate': 1.71218578926075e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2927, 'learning_rate': 1.418902851737497e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3006, 'learning_rate': 1.1256199142142443e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2522, 'learning_rate': 8.323369766909915e-06, 'epoch': 0.83}\n",
      "{'loss': 0.2777, 'learning_rate': 5.390540391677386e-06, 'epoch': 0.89}\n",
      "{'loss': 0.2751, 'learning_rate': 2.4577110164448582e-06, 'epoch': 0.95}\n",
      "{'eval_loss': 0.3303256928920746, 'eval_automl_metric': 0.0905963302752294, 'eval_runtime': 12.8617, 'eval_samples_per_second': 67.798, 'eval_steps_per_second': 67.798, 'epoch': 1.0}\n",
      "{'train_runtime': 597.0564, 'train_samples_per_second': 112.802, 'train_steps_per_second': 14.101, 'train_loss': 0.326611539816004, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_02-39-00/train_9296d920_19_s=9223372036854775807,e=4.2861e-05,s=-1,s=1,e=8,d=21_2023-02-13_02-39-00/checkpoint-8419 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:00:42] {3508} INFO -  at 9327.5s,\testimator transformer's best error=0.0906,\tbest estimator transformer's best error=0.0906\n",
      "[flaml.automl.automl: 02-13 03:00:42] {3624} INFO - selected model: None\n",
      "[flaml.automl.automl: 02-13 03:00:42] {3024} INFO - fit succeeded\n",
      "[flaml.automl.automl: 02-13 03:00:42] {3025} INFO - Time taken to find the best model: 9327.470085382462\n"
     ]
    }
   ],
   "source": [
    "'''The main flaml automl API'''\n",
    "automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eE5pLdH4v9M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehn1SDb5xAH9"
   },
   "source": [
    "The run takes 2.5 hours. We can print the best trial's loss, which is 1-the accuracy. The accuracy we got is 90.9% which is close to 91.2% reported by [the Electra model github](https://github.com/google-research/electra). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbTAqBsnTjhG",
    "outputId": "6c524f93-60c7-460f-d7e0-ff9116b03319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best loss by FLAML: 0.9094036697247706\n"
     ]
    }
   ],
   "source": [
    "print(\"The best loss by FLAML: {}\".format(1-automl.best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcO2th5M6AIu"
   },
   "source": [
    "If you have more GPUs on your server, you can use flaml.tune with the ray tune option, which will often give you a better score. For example, with 4x NVIDIA V100 GPU, the accuracy was 92.2% after searching for half an hour. For that experiment, you can open this notebook on your GPU server and set \"use_ray\" to {\"local_dir\": \"data/output/\"} and n_concurrent_trials to more than 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFP5JNdPTjhG"
   },
   "source": [
    "### Best model and metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY07pTY_xlIV"
   },
   "source": [
    "Next, we can print the best hyperparameter and the best score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbnhP3WrTjhG",
    "outputId": "642ff229-263f-4b9b-b408-e5b0cbaaa0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparmeter config: {'learning_rate': 4.9382981020165306e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 8, 'seed': 28, 'global_max_steps': 8419}\n",
      "Best accuracy on validation data: 0.9094\n",
      "Training duration of best run: 652.3 s\n"
     ]
    }
   ],
   "source": [
    "'''retrieve best config and best learner'''\n",
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best accuracy on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqIpmxl0dKWu"
   },
   "source": [
    "Save and load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfUNXfcNTBA2"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "automl.pickle(\"automl.pkl\")\n",
    "\n",
    "with open(\"automl.pkl\", \"rb\") as f:\n",
    "    automl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mdBURdexxJS"
   },
   "source": [
    "Run the prediction:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRl7pnEKTjhH",
    "outputId": "1ac788e9-7217-4fed-c7af-e3f40a91423f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 1\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels [1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
      " 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 1 0\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0\n",
      " 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
      " 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0\n",
      " 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0\n",
      " 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1\n",
      " 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1\n",
      " 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1\n",
      " 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0\n",
      " 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1\n",
      " 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1\n",
      " 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0\n",
      " 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1\n",
      " 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1\n",
      " 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "'''compute predictions of testing dataset''' \n",
    "y_pred = automl.predict(X_val, **{\"per_device_eval_batch_size\": 1})\n",
    "print('Predicted labels', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QThcVssKTjhH"
   },
   "source": [
    "### Log history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEFqWAuLyYIQ"
   },
   "source": [
    "You can also save and plot the history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58wpj4vPTjhH",
    "outputId": "5ab89591-abfe-4bc2-9f10-ffde9abe3f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 9.999999999999999e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'seed': 20, 'global_max_steps': 939, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.999999999999999e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'seed': 20, 'global_max_steps': 939, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 471, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.999999999999999e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'seed': 20, 'global_max_steps': 939, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 1.0296683485633468e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'seed': 26, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 1.0296683485633468e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'seed': 26, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 1.4799994583688665e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 25, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 1.4799994583688665e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 25, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 2.57182362273304e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 31, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 2.57182362273304e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 31, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 1.4799994583688658e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 25, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 2.57182362273304e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 31, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 5.5336391211098245e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 29, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 5.5336391211098245e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 29, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 2.5718236227330384e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 31, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 5.5336391211098245e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 29, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 6.201201844555069e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 29, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 6.201201844555069e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 29, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 4.193898274650822e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 23, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 4.193898274650822e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 23, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 6.201201844555066e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 29, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 4.193898274650822e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 23, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 2.000473353829644e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 27, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 4.193898274650822e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 23, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 8.792310432152323e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 19, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 4.193898274650822e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 23, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 3.432341889689445e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'seed': 27, 'global_max_steps': 1250, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 4.193898274650822e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'seed': 23, 'global_max_steps': 2500, 'FLAML_sample_size': 10000}}\n"
     ]
    }
   ],
   "source": [
    "from flaml.data import get_output_from_log\n",
    "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "    get_output_from_log(filename=automl_settings['log_file_name'], time_budget=3000)\n",
    "for config in config_history:\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtWSrLsdTjhH",
    "outputId": "24f68159-0943-423b-aa1c-de24580a973a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Wall Clock Time (s)')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "print(len(valid_loss_history))\n",
    "plt.scatter(time_history, 1 - np.array(valid_loss_history))\n",
    "plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xudzM73mTjhI"
   },
   "source": [
    "## 3. Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3gC3u_E4cO1"
   },
   "source": [
    "Given a dataset, which language model should you use for the fine tuning? It appears this is a simple question: just choose the best model according to the benchmarks such as [GLUE](https://gluebenchmark.com/leaderboard). However, we will see that under the resource constraints, the model selection is non trivial. \n",
    "\n",
    "In this example, we will tune the [spooky-author-identification](https://www.kaggle.com/competitions/spooky-author-identification/data?select=train.zip) dataset from kaggle. You can download the dataset from the [here](https://drive.google.com/file/d/1Jk-_Vg_SxOUDfFVzF7S85oBasY8fFvOY/view?usp=sharing) and upload it to Colab. The following command also downloads the file. We run FLAML for 30 mins using bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bty5Qz3x_OzJ",
    "outputId": "fa395df3-f2ad-4ca0-fe68-686284256049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Jk-_Vg_SxOUDfFVzF7S85oBasY8fFvOY\n",
      "To: /content/spooky-author-identification.csv\n",
      "\r",
      "  0% 0.00/3.30M [00:00<?, ?B/s]\r",
      "100% 3.30M/3.30M [00:00<00:00, 130MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1Jk-_Vg_SxOUDfFVzF7S85oBasY8fFvOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjvdojhfTjhI",
    "outputId": "3cb37ec5-fb6f-4186-dbe2-fed27ea5067a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:13:54] {2716} INFO - task = seq-classification\n",
      "[flaml.automl.automl: 02-13 03:13:54] {2718} INFO - Data split method: stratified\n",
      "[flaml.automl.automl: 02-13 03:13:54] {2721} INFO - Evaluation method: holdout\n",
      "[flaml.automl.automl: 02-13 03:13:54] {2848} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.automl: 02-13 03:13:54] {2994} INFO - List of ML learners in AutoML Run: ['transformer']\n",
      "[flaml.automl.automl: 02-13 03:13:54] {3323} INFO - iteration 0, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfe3d2166f54f3b95d7d895d16cd515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8833162188529968, 'eval_automl_metric': 0.38222676200204286, 'eval_runtime': 61.6983, 'eval_samples_per_second': 79.338, 'eval_steps_per_second': 79.338, 'epoch': 0.3}\n",
      "{'train_runtime': 137.7335, 'train_samples_per_second': 31.984, 'train_steps_per_second': 1.002, 'train_loss': 0.9795026917388474, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:17:50] {3461} INFO - Estimated sufficient time budget=2360025s. Estimated necessary time budget=2360s.\n",
      "[flaml.automl.automl: 02-13 03:17:50] {3508} INFO -  at 236.1s,\testimator transformer's best error=0.3822,\tbest estimator transformer's best error=0.3822\n",
      "[flaml.automl.automl: 02-13 03:17:50] {3323} INFO - iteration 1, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9505071043968201, 'eval_automl_metric': 0.454341164453524, 'eval_runtime': 61.2927, 'eval_samples_per_second': 79.863, 'eval_steps_per_second': 79.863, 'epoch': 0.3}\n",
      "{'train_runtime': 143.6116, 'train_samples_per_second': 30.674, 'train_steps_per_second': 0.48, 'train_loss': 1.0153622834578804, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:21:33] {3508} INFO -  at 458.3s,\testimator transformer's best error=0.3822,\tbest estimator transformer's best error=0.3822\n",
      "[flaml.automl.automl: 02-13 03:21:33] {3323} INFO - iteration 2, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7665733098983765, 'eval_automl_metric': 0.30500510725229824, 'eval_runtime': 60.752, 'eval_samples_per_second': 80.574, 'eval_steps_per_second': 80.574, 'epoch': 0.3}\n",
      "{'train_runtime': 138.3585, 'train_samples_per_second': 31.839, 'train_steps_per_second': 1.995, 'train_loss': 0.9176469332929971, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_03-14-04/train_72c6e40a_1_s=9223372036854775807,e=1e-05,s=-1,s=0.3,e=32,d=20_2023-02-13_03-14-04/checkpoint-138 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:25:09] {3508} INFO -  at 674.6s,\testimator transformer's best error=0.3050,\tbest estimator transformer's best error=0.3050\n",
      "[flaml.automl.automl: 02-13 03:25:09] {3323} INFO - iteration 3, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.722491443157196, 'eval_automl_metric': 0.28498467824310525, 'eval_runtime': 61.6444, 'eval_samples_per_second': 79.407, 'eval_steps_per_second': 79.407, 'epoch': 0.3}\n",
      "{'train_runtime': 140.4925, 'train_samples_per_second': 31.355, 'train_steps_per_second': 1.965, 'train_loss': 0.8815114670905514, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_03-21-33/train_841579e6_3_s=9223372036854775807,e=1.0297e-05,s=-1,s=0.3,e=16,d=26_2023-02-13_03-21-33/checkpoint-276 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:28:48] {3508} INFO -  at 893.5s,\testimator transformer's best error=0.2850,\tbest estimator transformer's best error=0.2850\n",
      "[flaml.automl.automl: 02-13 03:28:48] {3323} INFO - iteration 4, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7627, 'learning_rate': 4.688468079515019e-06, 'epoch': 0.54}\n",
      "{'eval_loss': 0.4817824065685272, 'eval_automl_metric': 0.18263534218590394, 'eval_runtime': 62.5183, 'eval_samples_per_second': 78.297, 'eval_steps_per_second': 78.297, 'epoch': 1.0}\n",
      "{'train_runtime': 320.5224, 'train_samples_per_second': 45.813, 'train_steps_per_second': 2.864, 'train_loss': 0.6535059112349367, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_03-25-09/train_04eb2642_4_s=9223372036854775807,e=1.48e-05,s=-1,s=0.3,e=16,d=25_2023-02-13_03-25-09/checkpoint-276 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:35:27] {3508} INFO -  at 1292.8s,\testimator transformer's best error=0.1826,\tbest estimator transformer's best error=0.1826\n",
      "[flaml.automl.automl: 02-13 03:35:27] {3323} INFO - iteration 5, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6646, 'learning_rate': 8.147241469004167e-06, 'epoch': 0.54}\n",
      "{'eval_loss': 0.40298137068748474, 'eval_automl_metric': 0.155464759959142, 'eval_runtime': 59.9582, 'eval_samples_per_second': 81.64, 'eval_steps_per_second': 81.64, 'epoch': 1.0}\n",
      "{'train_runtime': 318.4215, 'train_samples_per_second': 46.115, 'train_steps_per_second': 2.883, 'train_loss': 0.5554015215705422, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_03-28-48/train_876a6c90_5_s=9223372036854775807,e=1.0297e-05,s=-1,s=1,e=16,d=26_2023-02-13_03-28-48/checkpoint-918 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:42:03] {3508} INFO -  at 1688.6s,\testimator transformer's best error=0.1555,\tbest estimator transformer's best error=0.1555\n",
      "[flaml.automl.automl: 02-13 03:42:03] {3624} INFO - selected model: None\n",
      "[flaml.automl.automl: 02-13 03:42:03] {3024} INFO - fit succeeded\n",
      "[flaml.automl.automl: 02-13 03:42:03] {3025} INFO - Time taken to find the best model: 1688.5666162967682\n",
      "[flaml.automl.automl: 02-13 03:42:03] {3037} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "import flaml\n",
    "from flaml import AutoML\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('/content/spooky-author-identification.csv')\n",
    "X, y = df.drop('author', axis=1), df['author']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=123)\n",
    "automl_model = AutoML()\n",
    "\n",
    "automl_settings = {\n",
    "    \"time_budget\": 1800,                 \n",
    "    \"task\": \"seq-classification\",       \n",
    "    \"fit_kwargs_by_estimator\": {\n",
    "        \"transformer\": {\n",
    "            \"output_dir\": \"data/output/\",   \n",
    "            \"model_path\": \"bert-base-uncased\",  \n",
    "        }\n",
    "    },\n",
    "    \"metric\": \"accuracy\",\n",
    "    \"gpu_per_trial\": 1,  \n",
    "    \"log_file_name\": \"spooky_bert.log\", \n",
    "    \"log_type\": \"all\",                 \n",
    "    \"use_ray\": False,                    # set whether to use Ray\n",
    "    \"n_concurrent_trials\": 1,\n",
    "    \"keep_search_state\": True,          # keeping the search state\n",
    "}\n",
    "\n",
    "from flaml import tune\n",
    "custom_hp = {\n",
    "    \"transformer\": {\n",
    "            \"num_train_epochs\": {\n",
    "                \"domain\": tune.choice([0.3, 1, 2, 3, 4, 5]),\n",
    "                \"init_value\": 0.3,  \n",
    "                \"low_cost_init_value\": 0.3,\n",
    "            },\n",
    "        }\n",
    "}\n",
    "\n",
    "automl_model.fit(X_train=X_train, y_train=y_train,X_val=X_val, y_val=y_val, custom_hp=custom_hp, **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jZiKSU75jjl"
   },
   "source": [
    "The job ran for 23m and searched for 4 trials. This time is shorter than our budget 30m because FLAML early stops the last trial which will run for too long. If you want to run for longer time, set a larger time budget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpA-rzYzTjhI",
    "outputId": "b8aaefef-0c28-4d71-fc40-7dd8e72074ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best loss for spooky author identification: 0.155464759959142\n"
     ]
    }
   ],
   "source": [
    "print(\"the best loss for spooky author identification: {}\".format(automl_model.best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzDjaBTA6ZaD"
   },
   "source": [
    "Next, we set the model to roberta and run again. RoBERTa outperforms BERT by 15% on the [SuperGLUE](https://super.gluebenchmark.com/) benchmark, as well as [GLUE](https://gluebenchmark.com/), [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), [RACE](https://www.cs.cmu.edu/~glai1/data/race/), etc. Does this mean we should always use RoBERTa and never use BERT? To answer this question, we run the same experiment again with RoBERTa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MTZCJz1TjhJ",
    "outputId": "be604f85-d4a0-4c4b-98cd-249088130a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:42:03] {2716} INFO - task = seq-classification\n",
      "[flaml.automl.automl: 02-13 03:42:03] {2718} INFO - Data split method: stratified\n",
      "[flaml.automl.automl: 02-13 03:42:03] {2721} INFO - Evaluation method: holdout\n",
      "[flaml.automl.automl: 02-13 03:42:03] {2848} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.automl: 02-13 03:42:03] {2994} INFO - List of ML learners in AutoML Run: ['transformer']\n",
      "[flaml.automl.automl: 02-13 03:42:03] {3323} INFO - iteration 0, current learner transformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7729002010eb4248806c481213a8e015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbc97103470494bbae98c67ca0b6f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9842b1cbf52c422a8ee1d80ec9b4d7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfa3a71df9d4f82a4ae7c6ea0a5bfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e44c5b26a148d5926433cb5f572584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8742608428001404, 'eval_automl_metric': 0.3556690500510725, 'eval_runtime': 61.4906, 'eval_samples_per_second': 79.606, 'eval_steps_per_second': 79.606, 'epoch': 0.3}\n",
      "{'train_runtime': 146.3103, 'train_samples_per_second': 30.109, 'train_steps_per_second': 0.943, 'train_loss': 1.034589131673177, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:45:55] {3461} INFO - Estimated sufficient time budget=2312449s. Estimated necessary time budget=2312s.\n",
      "[flaml.automl.automl: 02-13 03:45:55] {3508} INFO -  at 231.4s,\testimator transformer's best error=0.3557,\tbest estimator transformer's best error=0.3557\n",
      "[flaml.automl.automl: 02-13 03:45:55] {3323} INFO - iteration 1, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.060534954071045, 'eval_automl_metric': 0.6020429009193053, 'eval_runtime': 58.805, 'eval_samples_per_second': 83.241, 'eval_steps_per_second': 83.241, 'epoch': 0.3}\n",
      "{'train_runtime': 141.2488, 'train_samples_per_second': 31.188, 'train_steps_per_second': 0.488, 'train_loss': 1.082918581755265, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:49:36] {3508} INFO -  at 452.4s,\testimator transformer's best error=0.3557,\tbest estimator transformer's best error=0.3557\n",
      "[flaml.automl.automl: 02-13 03:49:36] {3323} INFO - iteration 2, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6515962481498718, 'eval_automl_metric': 0.26312563840653724, 'eval_runtime': 60.5053, 'eval_samples_per_second': 80.902, 'eval_steps_per_second': 80.902, 'epoch': 0.3}\n",
      "{'train_runtime': 142.2129, 'train_samples_per_second': 30.976, 'train_steps_per_second': 1.941, 'train_loss': 0.8834930641063745, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_03-42-03/train_6166bba0_7_s=9223372036854775807,e=1e-05,s=-1,s=0.3,e=32,d=20_2023-02-13_03-42-03/checkpoint-138 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:53:18] {3508} INFO -  at 674.4s,\testimator transformer's best error=0.2631,\tbest estimator transformer's best error=0.2631\n",
      "[flaml.automl.automl: 02-13 03:53:18] {3323} INFO - iteration 3, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5998778939247131, 'eval_automl_metric': 0.24330949948927472, 'eval_runtime': 60.8573, 'eval_samples_per_second': 80.434, 'eval_steps_per_second': 80.434, 'epoch': 0.3}\n",
      "{'train_runtime': 142.9032, 'train_samples_per_second': 30.826, 'train_steps_per_second': 1.931, 'train_loss': 0.8051085541213768, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_03-49-36/train_6f1e3ccc_9_s=9223372036854775807,e=1.0297e-05,s=-1,s=0.3,e=16,d=26_2023-02-13_03-49-36/checkpoint-276 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 03:57:03] {3508} INFO -  at 899.6s,\testimator transformer's best error=0.2433,\tbest estimator transformer's best error=0.2433\n",
      "[flaml.automl.automl: 02-13 03:57:03] {3323} INFO - iteration 4, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7301, 'learning_rate': 4.688468079515019e-06, 'epoch': 0.54}\n",
      "{'eval_loss': 0.4913536310195923, 'eval_automl_metric': 0.19305413687436157, 'eval_runtime': 61.253, 'eval_samples_per_second': 79.914, 'eval_steps_per_second': 79.914, 'epoch': 1.0}\n",
      "{'train_runtime': 324.8695, 'train_samples_per_second': 45.2, 'train_steps_per_second': 2.826, 'train_loss': 0.6321186240202461, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_03-53-18/train_f35ce0a6_10_s=9223372036854775807,e=1.48e-05,s=-1,s=0.3,e=16,d=25_2023-02-13_03-53-18/checkpoint-276 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:03:59] {3508} INFO -  at 1315.5s,\testimator transformer's best error=0.1931,\tbest estimator transformer's best error=0.1931\n",
      "[flaml.automl.automl: 02-13 04:03:59] {3323} INFO - iteration 5, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6743, 'learning_rate': 8.147241469004167e-06, 'epoch': 0.54}\n",
      "{'eval_loss': 0.4497910141944885, 'eval_automl_metric': 0.17936670071501537, 'eval_runtime': 61.0611, 'eval_samples_per_second': 80.166, 'eval_steps_per_second': 80.166, 'epoch': 1.0}\n",
      "{'train_runtime': 324.3258, 'train_samples_per_second': 45.275, 'train_steps_per_second': 2.83, 'train_loss': 0.5846692918432564, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_03-57-03/train_79c73d08_11_s=9223372036854775807,e=1.0297e-05,s=-1,s=1,e=16,d=26_2023-02-13_03-57-03/checkpoint-918 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:10:45] {3508} INFO -  at 1721.9s,\testimator transformer's best error=0.1794,\tbest estimator transformer's best error=0.1794\n",
      "[flaml.automl.automl: 02-13 04:10:45] {3624} INFO - selected model: None\n",
      "[flaml.automl.automl: 02-13 04:10:45] {3024} INFO - fit succeeded\n",
      "[flaml.automl.automl: 02-13 04:10:45] {3025} INFO - Time taken to find the best model: 1721.9135808944702\n",
      "[flaml.automl.automl: 02-13 04:10:45] {3037} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "automl_settings[\"fit_kwargs_by_estimator\"][\"transformer\"][\"model_path\"] = \"roberta-base\"\n",
    "automl_settings[\"log_file_name\"] = \"spooky_roberta.log\"\n",
    "automl_model.fit(X_train=X_train, y_train=y_train,X_val=X_val, y_val=y_val, custom_hp=custom_hp, **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJygKvYzkHwQ",
    "outputId": "6436b6f7-e6ab-4d56-aac3-cf38865c44df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:10:45] {2716} INFO - task = seq-classification\n",
      "[flaml.automl.automl: 02-13 04:10:45] {2718} INFO - Data split method: stratified\n",
      "[flaml.automl.automl: 02-13 04:10:45] {2721} INFO - Evaluation method: holdout\n",
      "[flaml.automl.automl: 02-13 04:10:45] {2848} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.automl: 02-13 04:10:45] {2994} INFO - List of ML learners in AutoML Run: ['transformer_ms']\n",
      "[flaml.automl.automl: 02-13 04:10:45] {3323} INFO - iteration 0, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8833162188529968, 'eval_automl_metric': 0.38222676200204286, 'eval_runtime': 58.7085, 'eval_samples_per_second': 83.378, 'eval_steps_per_second': 83.378, 'epoch': 0.3}\n",
      "{'train_runtime': 138.8407, 'train_samples_per_second': 31.728, 'train_steps_per_second': 0.994, 'train_loss': 0.9795026917388474, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:14:22] {3461} INFO - Estimated sufficient time budget=2165067s. Estimated necessary time budget=2165s.\n",
      "[flaml.automl.automl: 02-13 04:14:22] {3508} INFO -  at 216.6s,\testimator transformer_ms's best error=0.3822,\tbest estimator transformer_ms's best error=0.3822\n",
      "[flaml.automl.automl: 02-13 04:14:22] {3323} INFO - iteration 1, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9486640095710754, 'eval_automl_metric': 0.4533197139938713, 'eval_runtime': 60.2154, 'eval_samples_per_second': 81.292, 'eval_steps_per_second': 81.292, 'epoch': 0.3}\n",
      "{'train_runtime': 144.9232, 'train_samples_per_second': 30.397, 'train_steps_per_second': 0.476, 'train_loss': 1.0138076561084692, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:18:04] {3508} INFO -  at 438.4s,\testimator transformer_ms's best error=0.3822,\tbest estimator transformer_ms's best error=0.3822\n",
      "[flaml.automl.automl: 02-13 04:18:04] {3323} INFO - iteration 2, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6545296311378479, 'eval_automl_metric': 0.2633299284984678, 'eval_runtime': 60.7743, 'eval_samples_per_second': 80.544, 'eval_steps_per_second': 80.544, 'epoch': 0.3}\n",
      "{'train_runtime': 142.543, 'train_samples_per_second': 30.904, 'train_steps_per_second': 1.936, 'train_loss': 0.8829950526140738, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_04-10-45/train_63ca1f78_13_s=9223372036854775807,e=1e-05,h=bert-base-uncased,s=-1,s=0.3,e=32,d=20_2023-02-13_04-10-45/checkpoint-138 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:21:47] {3508} INFO -  at 661.7s,\testimator transformer_ms's best error=0.2633,\tbest estimator transformer_ms's best error=0.2633\n",
      "[flaml.automl.automl: 02-13 04:21:47] {3323} INFO - iteration 3, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9636730551719666, 'eval_automl_metric': 0.47293156281920323, 'eval_runtime': 59.573, 'eval_samples_per_second': 82.168, 'eval_steps_per_second': 82.168, 'epoch': 0.3}\n",
      "{'train_runtime': 138.8512, 'train_samples_per_second': 31.726, 'train_steps_per_second': 1.988, 'train_loss': 1.0159533956776494, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:25:23] {3508} INFO -  at 877.7s,\testimator transformer_ms's best error=0.2633,\tbest estimator transformer_ms's best error=0.2633\n",
      "[flaml.automl.automl: 02-13 04:25:23] {3323} INFO - iteration 4, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5459058880805969, 'eval_automl_metric': 0.214708886618999, 'eval_runtime': 60.9184, 'eval_samples_per_second': 80.353, 'eval_steps_per_second': 80.353, 'epoch': 0.3}\n",
      "{'train_runtime': 141.4886, 'train_samples_per_second': 31.135, 'train_steps_per_second': 1.951, 'train_loss': 0.7395283242930537, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_04-18-04/train_69262eac_15_s=9223372036854775807,e=1.0302e-05,h=roberta-base,s=-1,s=0.3,e=16,d=26_2023-02-13_04-18-04/checkpoint-276 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:29:06] {3508} INFO -  at 1100.5s,\testimator transformer_ms's best error=0.2147,\tbest estimator transformer_ms's best error=0.2147\n",
      "[flaml.automl.automl: 02-13 04:29:06] {3323} INFO - iteration 5, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46095335483551025, 'eval_automl_metric': 0.18140960163432074, 'eval_runtime': 60.1135, 'eval_samples_per_second': 81.429, 'eval_steps_per_second': 81.429, 'epoch': 1.0}\n",
      "{'train_runtime': 316.3333, 'train_samples_per_second': 46.419, 'train_steps_per_second': 1.451, 'train_loss': 0.6190375035105188, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_04-25-23/train_6efe4a20_17_s=9223372036854775807,e=2.41e-05,h=roberta-base,s=-1,s=0.3,e=16,d=21_2023-02-13_04-25-23/checkpoint-276 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:35:43] {3508} INFO -  at 1497.6s,\testimator transformer_ms's best error=0.1814,\tbest estimator transformer_ms's best error=0.1814\n",
      "[flaml.automl.automl: 02-13 04:35:43] {3323} INFO - iteration 6, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5466792583465576, 'eval_automl_metric': 0.21511746680286004, 'eval_runtime': 59.3528, 'eval_samples_per_second': 82.473, 'eval_steps_per_second': 82.473, 'epoch': 0.3}\n",
      "{'train_runtime': 139.3941, 'train_samples_per_second': 31.602, 'train_steps_per_second': 1.98, 'train_loss': 0.7631930475649626, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:39:19] {3508} INFO -  at 1713.6s,\testimator transformer_ms's best error=0.1814,\tbest estimator transformer_ms's best error=0.1814\n",
      "[flaml.automl.automl: 02-13 04:39:19] {3323} INFO - iteration 7, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49973979592323303, 'eval_automl_metric': 0.2016343207354443, 'eval_runtime': 60.9962, 'eval_samples_per_second': 80.251, 'eval_steps_per_second': 80.251, 'epoch': 1.0}\n",
      "{'train_runtime': 318.038, 'train_samples_per_second': 46.171, 'train_steps_per_second': 1.443, 'train_loss': 0.6465242672589869, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:45:58] {3508} INFO -  at 2112.5s,\testimator transformer_ms's best error=0.1814,\tbest estimator transformer_ms's best error=0.1814\n",
      "[flaml.automl.automl: 02-13 04:45:58] {3323} INFO - iteration 8, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.414522647857666, 'eval_automl_metric': 0.15995914198161387, 'eval_runtime': 58.9534, 'eval_samples_per_second': 83.032, 'eval_steps_per_second': 83.032, 'epoch': 1.0}\n",
      "{'train_runtime': 316.5553, 'train_samples_per_second': 46.387, 'train_steps_per_second': 1.45, 'train_loss': 0.5728639423977057, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_04-29-06/train_f3c77e2a_18_s=9223372036854775807,e=1.8547e-05,h=roberta-base,s=-1,s=1,e=32,d=19_2023-02-13_04-29-06/checkpoint-459 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:52:31] {3508} INFO -  at 2505.6s,\testimator transformer_ms's best error=0.1600,\tbest estimator transformer_ms's best error=0.1600\n",
      "[flaml.automl.automl: 02-13 04:52:31] {3323} INFO - iteration 9, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4655350148677826, 'eval_automl_metric': 0.18243105209397348, 'eval_runtime': 59.9605, 'eval_samples_per_second': 81.637, 'eval_steps_per_second': 81.637, 'epoch': 1.0}\n",
      "{'train_runtime': 315.7155, 'train_samples_per_second': 46.51, 'train_steps_per_second': 1.454, 'train_loss': 0.6243883186955338, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 04:59:07] {3508} INFO -  at 2902.0s,\testimator transformer_ms's best error=0.1600,\tbest estimator transformer_ms's best error=0.1600\n",
      "[flaml.automl.automl: 02-13 04:59:07] {3323} INFO - iteration 10, current learner transformer_ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38274356722831726, 'eval_automl_metric': 0.14811031664964247, 'eval_runtime': 57.8994, 'eval_samples_per_second': 84.543, 'eval_steps_per_second': 84.543, 'epoch': 1.0}\n",
      "{'train_runtime': 313.3559, 'train_samples_per_second': 46.86, 'train_steps_per_second': 1.465, 'train_loss': 0.5366245178355631, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4895\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_04-45-58/train_4f0eadc4_21_s=9223372036854775807,e=2.5867e-05,h=bert-base-uncased,s=-1,s=1,e=32,d=27_2023-02-13_04-45-58/checkpoint-459 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:05:38] {3508} INFO -  at 3292.4s,\testimator transformer_ms's best error=0.1481,\tbest estimator transformer_ms's best error=0.1481\n",
      "[flaml.automl.automl: 02-13 05:05:38] {3624} INFO - selected model: None\n",
      "[flaml.automl.automl: 02-13 05:05:38] {3024} INFO - fit succeeded\n",
      "[flaml.automl.automl: 02-13 05:05:38] {3025} INFO - Time taken to find the best model: 3292.4112989902496\n",
      "[flaml.automl.automl: 02-13 05:05:38] {3037} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "automl_settings[\"time_budget\"] = 3600\n",
    "automl_settings[\"estimator_list\"] = [\"transformer_ms\"]\n",
    "automl_settings[\"log_file_name\"] = \"spooky_ms.log\"     \n",
    "automl_settings[\"fit_kwargs_by_estimator\"] =  { \n",
    "    \"transformer_ms\": {\n",
    "            \"output_dir\": \"data/output/\"     \n",
    "    }\n",
    "}    \n",
    "\n",
    "from flaml import tune\n",
    "\n",
    "custom_hp = {\n",
    "    \"transformer_ms\": {\n",
    "            \"model_path\": {\n",
    "                \"domain\": tune.choice([\"bert-base-uncased\", \"roberta-base\"]),\n",
    "                \"init_value\": \"bert-base-uncased\"\n",
    "            },\n",
    "            \"num_train_epochs\": {\n",
    "                \"domain\": tune.choice([0.3, 1, 2, 3, 4, 5]),\n",
    "                \"init_value\": 0.3,  \n",
    "                \"low_cost_init_value\": 0.3,\n",
    "            },\n",
    "        }\n",
    "}\n",
    "\n",
    "automl_model.fit(X_train=X_train, y_train=y_train,X_val=X_val, y_val=y_val, custom_hp=custom_hp, **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58KIeU-xyj13",
    "outputId": "4f8804be-bda4-40cc-df40-a24f23bce6fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"record_id\": 0, \"iter_per_learner\": 1, \"logged_metric\": {\"pred_time\": 0.012863580734907546, \"intermediate_results\": [{\"eval_loss\": 0.8833162188529968, \"eval_automl_metric\": 0.38222676200204286, \"eval_runtime\": 58.7085, \"eval_samples_per_second\": 83.378, \"eval_steps_per_second\": 83.378, \"epoch\": 0.3, \"train_runtime\": 138.8407, \"train_samples_per_second\": 31.728, \"train_steps_per_second\": 0.994, \"train_loss\": 0.9795026917388474}]}, \"trial_time\": 216.50436329841614, \"wall_clock_time\": 216.5950379371643, \"validation_loss\": 0.38222676200204286, \"config\": {\"learning_rate\": 9.999999999999999e-06, \"num_train_epochs\": 0.3, \"per_device_train_batch_size\": 32, \"seed\": 20, \"global_max_steps\": 138, \"model_path\": \"bert-base-uncased\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 1, \"iter_per_learner\": 2, \"logged_metric\": {\"pred_time\": 0.012905323712404463, \"intermediate_results\": [{\"eval_loss\": 0.9486640095710754, \"eval_automl_metric\": 0.4533197139938713, \"eval_runtime\": 60.2154, \"eval_samples_per_second\": 81.292, \"eval_steps_per_second\": 81.292, \"epoch\": 0.3, \"train_runtime\": 144.9232, \"train_samples_per_second\": 30.397, \"train_steps_per_second\": 0.476, \"train_loss\": 1.0138076561084692}]}, \"trial_time\": 221.7699818611145, \"wall_clock_time\": 438.3745439052582, \"validation_loss\": 0.4533197139938713, \"config\": {\"learning_rate\": 9.706892218498696e-06, \"num_train_epochs\": 0.3, \"per_device_train_batch_size\": 64, \"seed\": 14, \"global_max_steps\": 69, \"model_path\": \"bert-base-uncased\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 2, \"iter_per_learner\": 3, \"logged_metric\": {\"pred_time\": 0.013086232515594202, \"intermediate_results\": [{\"eval_loss\": 0.6545296311378479, \"eval_automl_metric\": 0.2633299284984678, \"eval_runtime\": 60.7743, \"eval_samples_per_second\": 80.544, \"eval_steps_per_second\": 80.544, \"epoch\": 0.3, \"train_runtime\": 142.543, \"train_samples_per_second\": 30.904, \"train_steps_per_second\": 1.936, \"train_loss\": 0.8829950526140738}]}, \"trial_time\": 223.17051100730896, \"wall_clock_time\": 661.7489302158356, \"validation_loss\": 0.2633299284984678, \"config\": {\"learning_rate\": 1.0301958417692867e-05, \"num_train_epochs\": 0.3, \"per_device_train_batch_size\": 16, \"seed\": 26, \"global_max_steps\": 276, \"model_path\": \"roberta-base\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 3, \"iter_per_learner\": 4, \"logged_metric\": {\"pred_time\": 0.012731756545428236, \"intermediate_results\": [{\"eval_loss\": 0.9636730551719666, \"eval_automl_metric\": 0.47293156281920323, \"eval_runtime\": 59.573, \"eval_samples_per_second\": 82.168, \"eval_steps_per_second\": 82.168, \"epoch\": 0.3, \"train_runtime\": 138.8512, \"train_samples_per_second\": 31.726, \"train_steps_per_second\": 1.988, \"train_loss\": 1.0159533956776494}]}, \"trial_time\": 215.775652885437, \"wall_clock_time\": 877.6699576377869, \"validation_loss\": 0.47293156281920323, \"config\": {\"learning_rate\": 4.403698954265022e-06, \"num_train_epochs\": 0.3, \"per_device_train_batch_size\": 16, \"seed\": 31, \"global_max_steps\": 276, \"model_path\": \"bert-base-uncased\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 4, \"iter_per_learner\": 5, \"logged_metric\": {\"pred_time\": 0.013174241117607951, \"intermediate_results\": [{\"eval_loss\": 0.5459058880805969, \"eval_automl_metric\": 0.214708886618999, \"eval_runtime\": 60.9184, \"eval_samples_per_second\": 80.353, \"eval_steps_per_second\": 80.353, \"epoch\": 0.3, \"train_runtime\": 141.4886, \"train_samples_per_second\": 31.135, \"train_steps_per_second\": 1.951, \"train_loss\": 0.7395283242930537}]}, \"trial_time\": 222.6075041294098, \"wall_clock_time\": 1100.4867150783539, \"validation_loss\": 0.214708886618999, \"config\": {\"learning_rate\": 2.4100273052744602e-05, \"num_train_epochs\": 0.3, \"per_device_train_batch_size\": 16, \"seed\": 21, \"global_max_steps\": 276, \"model_path\": \"roberta-base\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 5, \"iter_per_learner\": 6, \"logged_metric\": {\"pred_time\": 0.013119906229675729, \"intermediate_results\": [{\"eval_loss\": 0.46095335483551025, \"eval_automl_metric\": 0.18140960163432074, \"eval_runtime\": 60.1135, \"eval_samples_per_second\": 81.429, \"eval_steps_per_second\": 81.429, \"epoch\": 1.0, \"train_runtime\": 316.3333, \"train_samples_per_second\": 46.419, \"train_steps_per_second\": 1.451, \"train_loss\": 0.6190375035105188}]}, \"trial_time\": 396.91534209251404, \"wall_clock_time\": 1497.5713059902191, \"validation_loss\": 0.18140960163432074, \"config\": {\"learning_rate\": 1.85469436732702e-05, \"num_train_epochs\": 1, \"per_device_train_batch_size\": 32, \"seed\": 19, \"global_max_steps\": 459, \"model_path\": \"roberta-base\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 6, \"iter_per_learner\": 7, \"logged_metric\": {\"pred_time\": 0.01268796350909692, \"intermediate_results\": [{\"eval_loss\": 0.5466792583465576, \"eval_automl_metric\": 0.21511746680286004, \"eval_runtime\": 59.3528, \"eval_samples_per_second\": 82.473, \"eval_steps_per_second\": 82.473, \"epoch\": 0.3, \"train_runtime\": 139.3941, \"train_samples_per_second\": 31.602, \"train_steps_per_second\": 1.98, \"train_loss\": 0.7631930475649626}]}, \"trial_time\": 215.909264087677, \"wall_clock_time\": 1713.6097302436829, \"validation_loss\": 0.21511746680286004, \"config\": {\"learning_rate\": 2.4100273052744602e-05, \"num_train_epochs\": 0.3, \"per_device_train_batch_size\": 16, \"seed\": 21, \"global_max_steps\": 276, \"model_path\": \"bert-base-uncased\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 7, \"iter_per_learner\": 8, \"logged_metric\": {\"pred_time\": 0.013066816135129842, \"intermediate_results\": [{\"eval_loss\": 0.49973979592323303, \"eval_automl_metric\": 0.2016343207354443, \"eval_runtime\": 60.9962, \"eval_samples_per_second\": 80.251, \"eval_steps_per_second\": 80.251, \"epoch\": 1.0, \"train_runtime\": 318.038, \"train_samples_per_second\": 46.171, \"train_steps_per_second\": 1.443, \"train_loss\": 0.6465242672589869}]}, \"trial_time\": 398.5816743373871, \"wall_clock_time\": 2112.4975728988647, \"validation_loss\": 0.2016343207354443, \"config\": {\"learning_rate\": 1.3298483157591481e-05, \"num_train_epochs\": 1, \"per_device_train_batch_size\": 32, \"seed\": 11, \"global_max_steps\": 459, \"model_path\": \"roberta-base\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 8, \"iter_per_learner\": 9, \"logged_metric\": {\"pred_time\": 0.012655172844829306, \"intermediate_results\": [{\"eval_loss\": 0.414522647857666, \"eval_automl_metric\": 0.15995914198161387, \"eval_runtime\": 58.9534, \"eval_samples_per_second\": 83.032, \"eval_steps_per_second\": 83.032, \"epoch\": 1.0, \"train_runtime\": 316.5553, \"train_samples_per_second\": 46.387, \"train_steps_per_second\": 1.45, \"train_loss\": 0.5728639423977057}]}, \"trial_time\": 392.8052430152893, \"wall_clock_time\": 2505.59459233284, \"validation_loss\": 0.15995914198161387, \"config\": {\"learning_rate\": 2.586679364428794e-05, \"num_train_epochs\": 1, \"per_device_train_batch_size\": 32, \"seed\": 27, \"global_max_steps\": 459, \"model_path\": \"bert-base-uncased\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 9, \"iter_per_learner\": 10, \"logged_metric\": {\"pred_time\": 0.013127029368290497, \"intermediate_results\": [{\"eval_loss\": 0.4655350148677826, \"eval_automl_metric\": 0.18243105209397348, \"eval_runtime\": 59.9605, \"eval_samples_per_second\": 81.637, \"eval_steps_per_second\": 81.637, \"epoch\": 1.0, \"train_runtime\": 315.7155, \"train_samples_per_second\": 46.51, \"train_steps_per_second\": 1.454, \"train_loss\": 0.6243883186955338}]}, \"trial_time\": 396.29456973075867, \"wall_clock_time\": 2902.004398584366, \"validation_loss\": 0.18243105209397348, \"config\": {\"learning_rate\": 1.7750603229357797e-05, \"num_train_epochs\": 1, \"per_device_train_batch_size\": 32, \"seed\": 21, \"global_max_steps\": 459, \"model_path\": \"roberta-base\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"record_id\": 10, \"iter_per_learner\": 11, \"logged_metric\": {\"pred_time\": 0.012711567240667295, \"intermediate_results\": [{\"eval_loss\": 0.38274356722831726, \"eval_automl_metric\": 0.14811031664964247, \"eval_runtime\": 57.8994, \"eval_samples_per_second\": 84.543, \"eval_steps_per_second\": 84.543, \"epoch\": 1.0, \"train_runtime\": 313.3559, \"train_samples_per_second\": 46.86, \"train_steps_per_second\": 1.465, \"train_loss\": 0.5366245178355631}]}, \"trial_time\": 390.18142199516296, \"wall_clock_time\": 3292.4112989902496, \"validation_loss\": 0.14811031664964247, \"config\": {\"learning_rate\": 3.7693987341768903e-05, \"num_train_epochs\": 1, \"per_device_train_batch_size\": 32, \"seed\": 33, \"global_max_steps\": 459, \"model_path\": \"bert-base-uncased\"}, \"learner\": \"transformer_ms\", \"sample_size\": 14684}\n",
      "\n",
      "{\"curr_best_record_id\": 10}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"spooky_ms.log\", \"r\") as fin:\n",
    "  for line in fin:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujti_Dih5_-3"
   },
   "source": [
    "We plot the performance of BERT, RoBERTa, and model selection w.r.t. the wall clock time. We find two things: \n",
    "\n",
    "(1) although RoBERTa frequently outperforms BERT on benchmark datasets, its performance on the spooky-author-identification dataset is worse than BERT using the same time budget. Therefore, model selection is a non trivial problem;\n",
    "\n",
    "(2) by using FLAML's automated model selection, we are able to achieve a better performance than using just one model. Therefore, automated model selection is helpful;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxV-dTUhxaQO",
    "outputId": "2f605050-98cc-4f2a-99f3-1da5b366914e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from flaml.data import get_output_from_log\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for each_file_name in ['bert', 'roberta', 'ms']:\n",
    "    time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "        get_output_from_log(filename='spooky_' + each_file_name + '.log', time_budget=4000)\n",
    "    print(len(valid_loss_history))\n",
    "    plt.scatter(time_history, 1 - np.array(valid_loss_history))\n",
    "    plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')\n",
    "\n",
    "plt.legend(['bert', 'roberta', 'ms'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lT7IwNCoTjhJ"
   },
   "source": [
    "## 4. Other Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzkr77iATjhJ"
   },
   "source": [
    "Besides sequence classification, FLAML currently also supports four other tasks (more tasks are to be supported, which can be found on [FLAML's documentation website](https://microsoft.github.io/FLAML/docs/Examples/AutoML-NLP)):\n",
    "\n",
    "- sequence regression: predicting a float number from the input sequence, e.g., predicting the rating of a hotel review based on the text content;\n",
    "- token classification: predicting the label of each token in a sequence, e.g., named entity recognition;\n",
    "- multiple choice: predicting the best second half of a sentence that comes next to the first part of a sentence based on common sensen reasoning. An example is seen below;\n",
    "- (abstractive) summarization: generating the textual summarization of an input paragraph;\n",
    "\n",
    "Here we look into two tasks: multiple choice classification and text summarization. These tasks require significant computational resources, therefore instead of Colab, we run them using 4 NVIDIA V100 GPUs and Ray Tune on our server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4VgUR5TTjhJ"
   },
   "source": [
    "### 4.1 Multiple Choice Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO8GqaH3TjhJ"
   },
   "source": [
    "Multiple choice is a task of predicting the best second half of a sentence that follows the first half based on common sense reasoning. An example of multiple-choice classification problem is:\n",
    "\n",
    "On stage, a woman takes a seat at the piano. She\n",
    "a) sits on a bench as her sister plays with the doll.\n",
    "b) smiles with someone as the music plays.\n",
    "c) is in the crowd, watching the dancers.\n",
    "d) *nervously sets her fingers on the keys*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ5fX0N3TjhJ",
    "outputId": "1ceeed8c-4c7d-4a08-f04b-efb16bf2e881"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5b312426a649738434daaccd896676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c9fe8b7e274e3498ead735688d74bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/7.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a7ad531eb24ebaab47a8213080517b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:No config specified, defaulting to: swag/regular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset swag/regular to /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58b33697c654fcfb04cb353a7704036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24160b84827f458ebf0cc39c4d295178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb177263d2be441da00427d1396eefe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab55fdbd566648d9ba82e074d5af2d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f727ce4773814c55941ee374002e6f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81336be848544a94b134481772deafab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/73546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed4897c52a044398ccb0dd093227218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/20006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f5681a681f48b0a0a076dec968d843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/20005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset swag downloaded and prepared to /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:No config specified, defaulting to: swag/regular\n",
      "WARNING:datasets.builder:Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n",
      "WARNING:datasets.builder:No config specified, defaulting to: swag/regular\n",
      "WARNING:datasets.builder:Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"swag\", split=\"train\").to_pandas()[:2000]\n",
    "dev_dataset = load_dataset(\"swag\", split=\"validation\").to_pandas()[:1000]\n",
    "test_dataset = load_dataset(\"swag\", split=\"test\").to_pandas()\n",
    "\n",
    "custom_sent_keys = [\n",
    "        \"sent1\",\n",
    "        \"sent2\",\n",
    "        \"ending0\",\n",
    "        \"ending1\",\n",
    "        \"ending2\",\n",
    "        \"ending3\",\n",
    "        \"gold-source\",\n",
    "        \"video-id\",\n",
    "        \"startphrase\",\n",
    "        \"fold-ind\",\n",
    "    ]                                                  # specify the column names of the input sentences\n",
    "label_key = \"label\"                                    # specify the column name of the label\n",
    "\n",
    "X_train, y_train = train_dataset[custom_sent_keys], train_dataset[label_key]\n",
    "X_val, y_val = dev_dataset[custom_sent_keys], dev_dataset[label_key]\n",
    "X_test = test_dataset[custom_sent_keys]\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19m2ZpRGTjhJ",
    "outputId": "85edadc5-9d57-4c56-c216-4c275892a805"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Members of the procession walk down the street holding small horn brass instruments.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.iloc[0][\"sent1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvNeyzFsTjhJ",
    "outputId": "babed3c8-f106-48a4-9ea5-28d65ad0d7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 03-11 22:00:13] {2726} INFO - task = multichoice-classification\n",
      "[flaml.automl.automl: 03-11 22:00:13] {2728} INFO - Data split method: stratified\n",
      "[flaml.automl.automl: 03-11 22:00:13] {2731} INFO - Evaluation method: holdout\n",
      "[flaml.automl.automl: 03-11 22:00:13] {2858} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.automl: 03-11 22:00:13] {3004} INFO - List of ML learners in AutoML Run: ['transformer']\n",
      "[flaml.automl.automl: 03-11 22:00:14] {3334} INFO - iteration 0, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py:3641: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07fb73f6946443287240d8c734aca5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4342675063e24500bb8dcd50d42b2197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a17f46abbd248b580184c095be288f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083f2124157a4dfba91ef6bc5259d542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e40b9607124206b19f6e1d2e5ae08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0382, 'learning_rate': 6.666666666666666e-06, 'epoch': 1.0}\n",
      "{'loss': 0.5771, 'learning_rate': 3.333333333333333e-06, 'epoch': 2.0}\n",
      "{'eval_loss': 0.8690862059593201, 'eval_automl_metric': 0.30300000000000005, 'eval_runtime': 15.3509, 'eval_samples_per_second': 65.143, 'eval_steps_per_second': 65.143, 'epoch': 2.0}\n",
      "{'loss': 0.3655, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'eval_loss': 0.9104129076004028, 'eval_automl_metric': 0.28900000000000003, 'eval_runtime': 15.8677, 'eval_samples_per_second': 63.021, 'eval_steps_per_second': 63.021, 'epoch': 3.0}\n",
      "{'train_runtime': 286.4576, 'train_samples_per_second': 20.946, 'train_steps_per_second': 5.236, 'train_loss': 0.6602674560546875, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForMultipleChoice.forward` and have been ignored: fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2. If fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2 are not expected by `BertForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 03-11 22:05:48] {3472} INFO - Estimated sufficient time budget=3348100s. Estimated necessary time budget=3348s.\n",
      "[flaml.automl.automl: 03-11 22:05:48] {3519} INFO -  at 334.9s,\testimator transformer's best error=0.2890,\tbest estimator transformer's best error=0.2890\n",
      "[flaml.automl.automl: 03-11 22:05:48] {3334} INFO - iteration 1, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0537, 'learning_rate': 6.474576669243438e-06, 'epoch': 1.0}\n",
      "{'loss': 0.6124, 'learning_rate': 3.237288334621719e-06, 'epoch': 2.0}\n",
      "{'eval_loss': 0.8518794178962708, 'eval_automl_metric': 0.30600000000000005, 'eval_runtime': 18.7528, 'eval_samples_per_second': 53.325, 'eval_steps_per_second': 53.325, 'epoch': 2.0}\n",
      "{'loss': 0.3928, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'eval_loss': 0.8848311305046082, 'eval_automl_metric': 0.30200000000000005, 'eval_runtime': 18.5953, 'eval_samples_per_second': 53.777, 'eval_steps_per_second': 53.777, 'epoch': 3.0}\n",
      "{'train_runtime': 290.6383, 'train_samples_per_second': 20.644, 'train_steps_per_second': 5.161, 'train_loss': 0.6863048095703125, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForMultipleChoice.forward` and have been ignored: fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2. If fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2 are not expected by `BertForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 03-11 22:11:13] {3519} INFO -  at 659.3s,\testimator transformer's best error=0.2890,\tbest estimator transformer's best error=0.2890\n",
      "[flaml.automl.automl: 03-11 22:11:13] {3334} INFO - iteration 2, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0317, 'learning_rate': 6.864455657088979e-06, 'epoch': 1.0}\n",
      "{'loss': 0.572, 'learning_rate': 3.4322278285444894e-06, 'epoch': 2.0}\n",
      "{'eval_loss': 0.7877548933029175, 'eval_automl_metric': 0.28300000000000003, 'eval_runtime': 17.9408, 'eval_samples_per_second': 55.739, 'eval_steps_per_second': 55.739, 'epoch': 2.0}\n",
      "{'loss': 0.3356, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'eval_loss': 0.8435595631599426, 'eval_automl_metric': 0.28200000000000003, 'eval_runtime': 18.0908, 'eval_samples_per_second': 55.277, 'eval_steps_per_second': 55.277, 'epoch': 3.0}\n",
      "{'train_runtime': 288.492, 'train_samples_per_second': 20.798, 'train_steps_per_second': 5.199, 'train_loss': 0.6464457295735677, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForMultipleChoice.forward` and have been ignored: fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2. If fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2 are not expected by `BertForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-03-11_22-00-16/train_19c0e8ee_1_s=9223372036854775807,e=1e-05,s=-1,s=3,e=4,d=20_2023-03-11_22-00-16/checkpoint-1500 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 03-11 22:16:32] {3519} INFO -  at 978.5s,\testimator transformer's best error=0.2820,\tbest estimator transformer's best error=0.2820\n",
      "[flaml.automl.automl: 03-11 22:16:32] {3334} INFO - iteration 3, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9886, 'learning_rate': 7.3999972918443325e-06, 'epoch': 1.0}\n",
      "{'eval_loss': 0.7779488563537598, 'eval_automl_metric': 0.28900000000000003, 'eval_runtime': 18.8271, 'eval_samples_per_second': 53.115, 'eval_steps_per_second': 53.115, 'epoch': 1.0}\n",
      "{'loss': 0.4753, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "{'eval_loss': 0.8431324362754822, 'eval_automl_metric': 0.29200000000000004, 'eval_runtime': 16.4451, 'eval_samples_per_second': 60.809, 'eval_steps_per_second': 60.809, 'epoch': 2.0}\n",
      "{'train_runtime': 206.9591, 'train_samples_per_second': 19.327, 'train_steps_per_second': 4.832, 'train_loss': 0.7319369354248046, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForMultipleChoice.forward` and have been ignored: fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2. If fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2 are not expected by `BertForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 03-11 22:20:31] {3519} INFO -  at 1217.8s,\testimator transformer's best error=0.2820,\tbest estimator transformer's best error=0.2820\n",
      "[flaml.automl.automl: 03-11 22:20:31] {3334} INFO - iteration 4, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0922, 'learning_rate': 5.372722783974802e-06, 'epoch': 1.0}\n",
      "{'loss': 0.66, 'learning_rate': 3.5818151893165346e-06, 'epoch': 2.0}\n",
      "{'loss': 0.438, 'learning_rate': 1.7909075946582673e-06, 'epoch': 3.0}\n",
      "{'eval_loss': 0.8734284043312073, 'eval_automl_metric': 0.29200000000000004, 'eval_runtime': 15.0023, 'eval_samples_per_second': 66.656, 'eval_steps_per_second': 66.656, 'epoch': 3.0}\n",
      "{'loss': 0.3224, 'learning_rate': 0.0, 'epoch': 4.0}\n",
      "{'eval_loss': 0.9009197950363159, 'eval_automl_metric': 0.29000000000000004, 'eval_runtime': 16.3053, 'eval_samples_per_second': 61.33, 'eval_steps_per_second': 61.33, 'epoch': 4.0}\n",
      "{'train_runtime': 365.2073, 'train_samples_per_second': 21.905, 'train_steps_per_second': 5.476, 'train_loss': 0.6281257781982422, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForMultipleChoice.forward` and have been ignored: fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2. If fold-ind, gold-source, ending0, ending3, sent2, sent1, video-id, ending1, startphrase, ending2 are not expected by `BertForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 03-11 22:27:08] {3519} INFO -  at 1614.3s,\testimator transformer's best error=0.2820,\tbest estimator transformer's best error=0.2820\n",
      "[flaml.automl.automl: 03-11 22:27:08] {3635} INFO - selected model: None\n",
      "[flaml.automl.automl: 03-11 22:27:08] {3034} INFO - fit succeeded\n",
      "[flaml.automl.automl: 03-11 22:27:08] {3035} INFO - Time taken to find the best model: 978.4503326416016\n"
     ]
    }
   ],
   "source": [
    "''' import AutoML class from flaml package '''\n",
    "from flaml import AutoML\n",
    "automl = AutoML()\n",
    "\n",
    "automl_settings = {\n",
    "    \"time_budget\": 1800,                 # setting the time budget\n",
    "    \"task\": \"multichoice-classification\",       # setting the task as multiplechoice-classification\n",
    "    \"fit_kwargs_by_estimator\": {          # if model_path is not set, the default model is facebook/muppet-roberta-base: https://huggingface.co/facebook/muppet-roberta-base\n",
    "        \"transformer\": {\n",
    "            \"output_dir\": \"data/output/\",  # setting the output directory\n",
    "            \"model_path\": \"bert-base-uncased\", # the batch size for validation (inference)\n",
    "        }\n",
    "    },\n",
    "    \"gpu_per_trial\": 1,                 # set to 0 if no GPU is available\n",
    "    \"log_file_name\": \"seqclass.log\",    # set the file to save the log for HPO\n",
    "    \"log_type\": \"all\",                  # the log type for trials: \"all\" if logging all the trials, \"better\" if only keeping the better trials\n",
    "    \"use_ray\": False,                    # set whether to use Ray\n",
    "    \"n_concurrent_trials\": 1,\n",
    "    \"fp16\": False\n",
    "}\n",
    "\n",
    "from flaml import tune\n",
    "custom_hp = {\n",
    "    \"transformer\": {\n",
    "            \"per_device_train_batch_size\": {\n",
    "                \"domain\": tune.choice([1, 2, 4]),\n",
    "                \"init_value\": 4,\n",
    "                \"low_cost_init_value\": 4,\n",
    "            },\n",
    "        }\n",
    "}\n",
    "\n",
    "'''The main flaml automl API'''\n",
    "automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, custom_hp=custom_hp, **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh7ZJsIKTjhJ",
    "outputId": "b74305d2-ad28-44f0-e7f5-64bba52e0360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Current Learner': 'transformer', 'Current Sample': 2000, 'Current Hyper-parameters': {'learning_rate': 9.999999999999999e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'seed': 20, 'global_max_steps': 1500}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.999999999999999e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'seed': 20, 'global_max_steps': 1500}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 2000, 'Current Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'seed': 14, 'global_max_steps': 1500}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.999999999999999e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'seed': 20, 'global_max_steps': 1500}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 2000, 'Current Hyper-parameters': {'learning_rate': 1.0296683485633468e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'seed': 26, 'global_max_steps': 1500}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 1.0296683485633468e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'seed': 26, 'global_max_steps': 1500}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 2000, 'Current Hyper-parameters': {'learning_rate': 1.4799994583688665e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 4, 'seed': 25, 'global_max_steps': 500}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 1.0296683485633468e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'seed': 26, 'global_max_steps': 1500}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 2000, 'Current Hyper-parameters': {'learning_rate': 7.163630378633069e-06, 'num_train_epochs': 4, 'per_device_train_batch_size': 4, 'seed': 27, 'global_max_steps': 2000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 1.0296683485633468e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'seed': 26, 'global_max_steps': 1500}}\n",
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from flaml.data import get_output_from_log\n",
    "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "    get_output_from_log(filename=automl_settings['log_file_name'], time_budget=3000)\n",
    "for config in config_history:\n",
    "    print(config)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Wall Clock Time (s)')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "print(len(valid_loss_history))\n",
    "plt.scatter(time_history, 1 - np.array(valid_loss_history))\n",
    "plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "664qCdihTjhJ"
   },
   "source": [
    "### 4.2 Text Summarization Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmB4kaF_TjhJ"
   },
   "source": [
    "The text summarization task summarizes a long text into a short sentence. For example:\n",
    "\n",
    "- Document: Army explosives experts were called out to deal with a suspect package at the offices on the Newtownards Road on Friday night. Roads were sealed off and traffic diverted as a controlled explosion was carried out. The premises, used by East Belfast MP Naomi Long, have been targeted a number of times. Most recently, petrol bomb attacks were carried out on the offices on consecutive nights in April and May. The attacks began following a Belfast City Council vote in December 2012 restricting the flying of the union flag at the City Hall. Condemning the latest hoax, Alliance MLA Chris Lyttle said: \"It is a serious incident for the local area, it causes serious disruption, it puts people's lives at risk, it can prevent emergency services reaching the area. \"Ultimately we need people with information to share that with the police in order for them to do their job and bring these people to justice.\n",
    "\n",
    "- Summary: A suspicious package left outside an Alliance Party office in east Belfast has been declared a hoax.\n",
    "\n",
    "In this example, we use FLAML to perform *abstractive summarization* using the t5-small language model, i.e., the summary is generated word-by-word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amlQnvcxTjhK",
    "outputId": "63e42d3a-f61d-4a04-aeef-e37481661c7e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c4af22790947199cfe525914d3b29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9ad4626326498b855b35fb803abdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset xsum/default to /root/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df09a8f42a445caba4a48c180ad7336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b115a06f4f4c4eb35590a4299a2510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485d88adcc6143258960439cabbe48fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d3e7c0c5f44a01a12436df302354e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc6a402e60f4020a599a406aa357ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b8fb6dfb894790b62b3c69cee886f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xsum downloaded and prepared to /root/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset xsum (/root/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "WARNING:datasets.builder:Found cached dataset xsum (/root/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"xsum\", split=\"train\").to_pandas()[:1000]\n",
    "valid_dataset = load_dataset(\"xsum\", split=\"validation\").to_pandas()[:400]\n",
    "test_dataset = load_dataset(\"xsum\", split=\"test\").to_pandas()\n",
    "\n",
    "custom_sent_keys = [\"document\"]       # specify the column names of the input sentences\n",
    "label_key = \"summary\"                 # specify the column name of the label                              \n",
    "\n",
    "X_train, y_train = train_dataset[custom_sent_keys], train_dataset[label_key]\n",
    "X_val, y_val = valid_dataset[custom_sent_keys], valid_dataset[label_key]\n",
    "X_test = test_dataset[custom_sent_keys]\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYq8XAtxTjhK",
    "outputId": "d5f7bc98-2af0-4453-af11-95ecc5b7e320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:31:03] {2716} INFO - task = summarization\n",
      "[flaml.automl.automl: 02-13 05:31:03] {2718} INFO - Data split method: uniform\n",
      "[flaml.automl.automl: 02-13 05:31:03] {2721} INFO - Evaluation method: holdout\n",
      "[flaml.automl.automl: 02-13 05:31:03] {2848} INFO - Minimizing error metric: rouge1\n",
      "[flaml.automl.automl: 02-13 05:31:03] {2994} INFO - List of ML learners in AutoML Run: ['transformer']\n",
      "[flaml.automl.automl: 02-13 05:31:03] {3323} INFO - iteration 0, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3641: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315a1c554bcb40eba96dfa451d04995b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9694211923dd4e1dae072d85acf4f571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c073981b7da045399c8979902b268c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10a5a1da28a4919b0e6f977de69faa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27527d9c39f3422c86c62a183350eafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "/usr/local/lib/python3.8/dist-packages/flaml/automl/ml.py:171: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(datasets_metric_name)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a932514412d64f7a88942d0aac83521a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.069125175476074, 'eval_automl_metric': 0.8538466562851643, 'eval_runtime': 81.2302, 'eval_samples_per_second': 4.924, 'eval_steps_per_second': 4.924, 'epoch': 0.12}\n",
      "{'train_runtime': 86.1489, 'train_samples_per_second': 1.161, 'train_steps_per_second': 0.046, 'train_loss': 4.296197891235352, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:34:04] {3461} INFO - Estimated sufficient time budget=1808487s. Estimated necessary time budget=1808s.\n",
      "[flaml.automl.automl: 02-13 05:34:04] {3508} INFO -  at 180.9s,\testimator transformer's best error=0.8538,\tbest estimator transformer's best error=0.8538\n",
      "[flaml.automl.automl: 02-13 05:34:04] {3323} INFO - iteration 1, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.079618453979492, 'eval_automl_metric': 0.8530483735569663, 'eval_runtime': 81.748, 'eval_samples_per_second': 4.893, 'eval_steps_per_second': 4.893, 'epoch': 0.12}\n",
      "{'train_runtime': 86.3434, 'train_samples_per_second': 1.158, 'train_steps_per_second': 0.023, 'train_loss': 4.259670257568359, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_05-31-03/train_9b860caa_25_s=9223372036854775807,e=1e-05,s=-1,s=0.1,e=32,d=20_2023-02-13_05-31-03/checkpoint-4 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:37:01] {3508} INFO -  at 357.5s,\testimator transformer's best error=0.8530,\tbest estimator transformer's best error=0.8530\n",
      "[flaml.automl.automl: 02-13 05:37:01] {3323} INFO - iteration 2, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.069125175476074, 'eval_automl_metric': 0.8538466562851643, 'eval_runtime': 80.2842, 'eval_samples_per_second': 4.982, 'eval_steps_per_second': 4.982, 'epoch': 0.12}\n",
      "{'train_runtime': 84.8978, 'train_samples_per_second': 1.178, 'train_steps_per_second': 0.047, 'train_loss': 4.296197891235352, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:39:58] {3508} INFO -  at 534.6s,\testimator transformer's best error=0.8530,\tbest estimator transformer's best error=0.8530\n",
      "[flaml.automl.automl: 02-13 05:39:58] {3323} INFO - iteration 3, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.070651054382324, 'eval_automl_metric': 0.8528544017608968, 'eval_runtime': 81.2356, 'eval_samples_per_second': 4.924, 'eval_steps_per_second': 4.924, 'epoch': 0.12}\n",
      "{'train_runtime': 85.8005, 'train_samples_per_second': 1.165, 'train_steps_per_second': 0.023, 'train_loss': 4.219771385192871, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_05-34-04/train_0752a7ea_26_s=9223372036854775807,e=9.7119e-06,s=-1,s=0.1,e=64,d=14_2023-02-13_05-34-04/checkpoint-2 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:42:55] {3508} INFO -  at 711.4s,\testimator transformer's best error=0.8529,\tbest estimator transformer's best error=0.8529\n",
      "[flaml.automl.automl: 02-13 05:42:55] {3323} INFO - iteration 4, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.992288589477539, 'eval_automl_metric': 0.8522236109094221, 'eval_runtime': 80.4119, 'eval_samples_per_second': 4.974, 'eval_steps_per_second': 4.974, 'epoch': 1.0}\n",
      "{'train_runtime': 98.8691, 'train_samples_per_second': 10.114, 'train_steps_per_second': 0.162, 'train_loss': 4.1770782470703125, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "WARNING:flaml.automl:checkpoint data/output/train_2023-02-13_05-39-58/train_da3c904e_28_s=9223372036854775807,e=1.3959e-05,s=-1,s=0.1,e=64,d=13_2023-02-13_05-39-58/checkpoint-2 not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:46:05] {3508} INFO -  at 902.0s,\testimator transformer's best error=0.8522,\tbest estimator transformer's best error=0.8522\n",
      "[flaml.automl.automl: 02-13 05:46:05] {3323} INFO - iteration 5, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.9141499996185303, 'eval_automl_metric': 0.8535998619043884, 'eval_runtime': 80.1459, 'eval_samples_per_second': 4.991, 'eval_steps_per_second': 4.991, 'epoch': 1.0}\n",
      "{'train_runtime': 98.3049, 'train_samples_per_second': 10.172, 'train_steps_per_second': 0.163, 'train_loss': 4.1529316902160645, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:49:16] {3508} INFO -  at 1092.3s,\testimator transformer's best error=0.8522,\tbest estimator transformer's best error=0.8522\n",
      "[flaml.automl.automl: 02-13 05:49:16] {3323} INFO - iteration 6, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.0360307693481445, 'eval_automl_metric': 0.8539379033064027, 'eval_runtime': 80.4358, 'eval_samples_per_second': 4.973, 'eval_steps_per_second': 4.973, 'epoch': 1.0}\n",
      "{'train_runtime': 98.3949, 'train_samples_per_second': 10.163, 'train_steps_per_second': 0.163, 'train_loss': 4.209002494812012, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:52:25] {3508} INFO -  at 1281.5s,\testimator transformer's best error=0.8522,\tbest estimator transformer's best error=0.8522\n",
      "[flaml.automl.automl: 02-13 05:52:25] {3323} INFO - iteration 7, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.876713275909424, 'eval_automl_metric': 0.8523548026125396, 'eval_runtime': 80.2794, 'eval_samples_per_second': 4.983, 'eval_steps_per_second': 4.983, 'epoch': 1.0}\n",
      "{'train_runtime': 98.0878, 'train_samples_per_second': 10.195, 'train_steps_per_second': 0.163, 'train_loss': 4.11635684967041, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:55:38] {3508} INFO -  at 1474.6s,\testimator transformer's best error=0.8522,\tbest estimator transformer's best error=0.8522\n",
      "[flaml.automl.automl: 02-13 05:55:38] {3323} INFO - iteration 8, current learner transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.04831075668335, 'eval_automl_metric': 0.8531664365713729, 'eval_runtime': 86.5339, 'eval_samples_per_second': 4.622, 'eval_steps_per_second': 4.622, 'epoch': 1.0}\n",
      "{'train_runtime': 104.6534, 'train_samples_per_second': 9.555, 'train_steps_per_second': 0.153, 'train_loss': 4.229860305786133, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.automl: 02-13 05:58:57] {3508} INFO -  at 1673.9s,\testimator transformer's best error=0.8522,\tbest estimator transformer's best error=0.8522\n",
      "[flaml.automl.automl: 02-13 05:58:57] {3624} INFO - selected model: None\n",
      "[flaml.automl.automl: 02-13 05:58:57] {3024} INFO - fit succeeded\n",
      "[flaml.automl.automl: 02-13 05:58:57] {3025} INFO - Time taken to find the best model: 902.0195167064667\n"
     ]
    }
   ],
   "source": [
    "''' import AutoML class from flaml package '''\n",
    "from flaml import AutoML\n",
    "automl = AutoML()\n",
    "\n",
    "import ray\n",
    "\n",
    "\n",
    "automl_settings = {\n",
    "    \"time_budget\": 1800,         # setting the time budget\n",
    "    \"task\": \"summarization\",    # setting the task as summarization\n",
    "    \"fit_kwargs_by_estimator\": {  # if model_path is not set, the default model is t5-small: https://huggingface.co/t5-small\n",
    "        \"transformer\": {\n",
    "            \"output_dir\": \"data/output/\",  # setting the output directory\n",
    "            \"model_path\": \"t5-small\",\n",
    "            \"pad_to_max_length\": True,\n",
    "        }\n",
    "    },\n",
    "    \"gpu_per_trial\": 1,  # set to 0 if no GPU is available\n",
    "    \"log_file_name\": \"seqclass.log\",  # set the file to save the log for HPO\n",
    "    \"log_type\": \"all\",   # the log type for trials: \"all\" if logging all the trials, \"better\" if only keeping the better trials\n",
    "    \"use_ray\": False,  # set whether to use Ray\n",
    "    \"metric\": \"rouge1\",\n",
    "    \"n_concurrent_trials\": 1,  \n",
    "    \"fp16\": False\n",
    "}\n",
    "\n",
    "from flaml import tune\n",
    "custom_hp = {\n",
    "    \"transformer\": {\n",
    "            \"num_train_epochs\": {\n",
    "                \"domain\": tune.choice([0.1, 1, 2, 3, 4, 5]),\n",
    "                \"init_value\": 0.1,  \n",
    "                \"low_cost_init_value\": 0.1,\n",
    "            },\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "'''The main flaml automl API'''\n",
    "automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, custom_hp=custom_hp, **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRWfyDdSJZRT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPy67MBFTjhK",
    "outputId": "4d24c1c2-a605-42c2-a7e7-5634ddb19474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Current Learner': 'transformer', 'Current Sample': 1000, 'Current Hyper-parameters': {'learning_rate': 9.999999999999999e-06, 'num_train_epochs': 0.1, 'per_device_train_batch_size': 32, 'seed': 20, 'global_max_steps': 4}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.999999999999999e-06, 'num_train_epochs': 0.1, 'per_device_train_batch_size': 32, 'seed': 20, 'global_max_steps': 4}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 1000, 'Current Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 0.1, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 2}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 0.1, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 2}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 1000, 'Current Hyper-parameters': {'learning_rate': 9.999999999999997e-06, 'num_train_epochs': 0.1, 'per_device_train_batch_size': 32, 'seed': 20, 'global_max_steps': 4}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 0.1, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 2}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 1000, 'Current Hyper-parameters': {'learning_rate': 1.3959402525606234e-05, 'num_train_epochs': 0.1, 'per_device_train_batch_size': 64, 'seed': 13, 'global_max_steps': 2}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 1.3959402525606234e-05, 'num_train_epochs': 0.1, 'per_device_train_batch_size': 64, 'seed': 13, 'global_max_steps': 2}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 1000, 'Current Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 16}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 16}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 1000, 'Current Hyper-parameters': {'learning_rate': 1.6876495255790516e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 20, 'global_max_steps': 16}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 16}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 1000, 'Current Hyper-parameters': {'learning_rate': 5.588857190057775e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 8, 'global_max_steps': 16}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 16}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 1000, 'Current Hyper-parameters': {'learning_rate': 2.0896439261730886e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 12, 'global_max_steps': 16}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 16}}\n",
      "{'Current Learner': 'transformer', 'Current Sample': 1000, 'Current Hyper-parameters': {'learning_rate': 4.513703060694946e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 16, 'global_max_steps': 16}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 9.711865003865157e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 64, 'seed': 14, 'global_max_steps': 16}}\n",
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from flaml.data import get_output_from_log\n",
    "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "    get_output_from_log(filename=automl_settings['log_file_name'], time_budget=3000)\n",
    "for config in config_history:\n",
    "    print(config)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Wall Clock Time (s)')\n",
    "plt.ylabel('Rouge 1')\n",
    "print(len(valid_loss_history))\n",
    "plt.scatter(time_history, 1 - np.array(valid_loss_history))\n",
    "plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "e9d36fc5b7c3dd4177ff1b60184dd696c0acc18150a44682abca4d769811bd46"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
