{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# FineTuning NLP Models with FLAML Library\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "FLAML is a Python library (https://github.com/microsoft/FLAML) designed to automatically produce accurate machine learning models \n",
    "with low computational cost. It is fast and economical. The simple and lightweight design makes it easy to use and extend, such as adding new learners. FLAML can \n",
    "- serve as an economical AutoML engine,\n",
    "- be used as a fast hyperparameter tuning tool, or \n",
    "- be embedded in self-tuning software that requires low latency & resource in repetitive\n",
    "   tuning tasks.\n",
    "\n",
    "In this notebook, we demonstrate how to use the FLAML library to fine tune an NLP language model with hyperparameter search. \n",
    "\n",
    "FLAML requires `Python>=3.6`. To run this notebook example, please install flaml with the `notebook` option:\n",
    "```bash\n",
    "pip install flaml[notebook,nlp]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flaml[nlp,notebook,ray] in /data/xliu127/projects/hyperopt/FLAML (0.9.7)\n",
      "Requirement already satisfied: NumPy>=1.16.2 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.22.2)\n",
      "Requirement already satisfied: lightgbm>=2.3.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (3.3.2)\n",
      "Requirement already satisfied: xgboost<=1.3.3,>=0.90 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.3.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.8.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.0.2)\n",
      "Requirement already satisfied: openml==0.10.2 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (0.10.2)\n",
      "Requirement already satisfied: jupyter in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (3.5.1)\n",
      "Requirement already satisfied: rgf-python in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (3.12.0)\n",
      "Requirement already satisfied: catboost>=0.26 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.0.4)\n",
      "Requirement already satisfied: ray[tune]~=1.10 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.10.0)\n",
      "Requirement already satisfied: transformers>=4.14 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (4.16.2)\n",
      "Requirement already satisfied: datasets in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.18.3)\n",
      "Requirement already satisfied: torch in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.10.2)\n",
      "Requirement already satisfied: seqeval in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (1.2.2)\n",
      "Requirement already satisfied: nltk in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (3.7)\n",
      "Requirement already satisfied: rouge_score in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from flaml[nlp,notebook,ray]) (0.0.4)\n",
      "Requirement already satisfied: xmltodict in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from openml==0.10.2->flaml[nlp,notebook,ray]) (0.12.0)\n",
      "Requirement already satisfied: liac-arff>=2.4.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from openml==0.10.2->flaml[nlp,notebook,ray]) (2.5.0)\n",
      "Requirement already satisfied: requests in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from openml==0.10.2->flaml[nlp,notebook,ray]) (2.27.1)\n",
      "Requirement already satisfied: python-dateutil in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from openml==0.10.2->flaml[nlp,notebook,ray]) (2.8.2)\n",
      "Requirement already satisfied: graphviz in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from catboost>=0.26->flaml[nlp,notebook,ray]) (0.19.1)\n",
      "Requirement already satisfied: plotly in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from catboost>=0.26->flaml[nlp,notebook,ray]) (5.6.0)\n",
      "Requirement already satisfied: six in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from catboost>=0.26->flaml[nlp,notebook,ray]) (1.16.0)\n",
      "Requirement already satisfied: wheel in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from lightgbm>=2.3.1->flaml[nlp,notebook,ray]) (0.37.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from pandas>=1.1.4->flaml[nlp,notebook,ray]) (2021.3)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (1.44.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (3.19.4)\n",
      "Requirement already satisfied: click>=7.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (8.0.3)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (1.0.3)\n",
      "Requirement already satisfied: jsonschema in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (3.2.0)\n",
      "Requirement already satisfied: pyyaml in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (6.0)\n",
      "Requirement already satisfied: attrs in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (21.4.0)\n",
      "Requirement already satisfied: redis>=3.5.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (4.1.4)\n",
      "Requirement already satisfied: filelock in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (3.6.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (2.5)\n",
      "Requirement already satisfied: tabulate in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ray[tune]~=1.10->flaml[nlp,notebook,ray]) (0.8.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from scikit-learn>=0.24->flaml[nlp,notebook,ray]) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from scikit-learn>=0.24->flaml[nlp,notebook,ray]) (1.1.0)\n",
      "Requirement already satisfied: sacremoses in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from transformers>=4.14->flaml[nlp,notebook,ray]) (0.0.47)\n",
      "Requirement already satisfied: packaging>=20.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from transformers>=4.14->flaml[nlp,notebook,ray]) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from transformers>=4.14->flaml[nlp,notebook,ray]) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from transformers>=4.14->flaml[nlp,notebook,ray]) (2022.1.18)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from transformers>=4.14->flaml[nlp,notebook,ray]) (0.11.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from transformers>=4.14->flaml[nlp,notebook,ray]) (0.4.0)\n",
      "Requirement already satisfied: xxhash in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from datasets->flaml[nlp,notebook,ray]) (3.0.0)\n",
      "Requirement already satisfied: aiohttp in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from datasets->flaml[nlp,notebook,ray]) (3.8.1)\n",
      "Requirement already satisfied: dill in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from datasets->flaml[nlp,notebook,ray]) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from datasets->flaml[nlp,notebook,ray]) (2022.2.0)\n",
      "Requirement already satisfied: multiprocess in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from datasets->flaml[nlp,notebook,ray]) (0.70.12.2)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from datasets->flaml[nlp,notebook,ray]) (7.0.0)\n",
      "Requirement already satisfied: jupyter-console in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jupyter->flaml[nlp,notebook,ray]) (6.4.0)\n",
      "Requirement already satisfied: notebook in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jupyter->flaml[nlp,notebook,ray]) (6.4.8)\n",
      "Requirement already satisfied: qtconsole in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jupyter->flaml[nlp,notebook,ray]) (5.2.2)\n",
      "Requirement already satisfied: nbconvert in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jupyter->flaml[nlp,notebook,ray]) (6.4.2)\n",
      "Requirement already satisfied: ipykernel in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jupyter->flaml[nlp,notebook,ray]) (6.9.1)\n",
      "Requirement already satisfied: ipywidgets in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jupyter->flaml[nlp,notebook,ray]) (7.6.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from matplotlib->flaml[nlp,notebook,ray]) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from matplotlib->flaml[nlp,notebook,ray]) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from matplotlib->flaml[nlp,notebook,ray]) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from matplotlib->flaml[nlp,notebook,ray]) (4.29.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from matplotlib->flaml[nlp,notebook,ray]) (1.3.2)\n",
      "Requirement already satisfied: absl-py in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from rouge_score->flaml[nlp,notebook,ray]) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from torch->flaml[nlp,notebook,ray]) (4.1.1)\n",
      "Requirement already satisfied: deprecated>=1.2.3 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from redis>=3.5.0->ray[tune]~=1.10->flaml[nlp,notebook,ray]) (1.2.13)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from requests->openml==0.10.2->flaml[nlp,notebook,ray]) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from requests->openml==0.10.2->flaml[nlp,notebook,ray]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from requests->openml==0.10.2->flaml[nlp,notebook,ray]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from requests->openml==0.10.2->flaml[nlp,notebook,ray]) (2.0.12)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from aiohttp->datasets->flaml[nlp,notebook,ray]) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from aiohttp->datasets->flaml[nlp,notebook,ray]) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from aiohttp->datasets->flaml[nlp,notebook,ray]) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from aiohttp->datasets->flaml[nlp,notebook,ray]) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from aiohttp->datasets->flaml[nlp,notebook,ray]) (1.7.2)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipykernel->jupyter->flaml[nlp,notebook,ray]) (8.0.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipykernel->jupyter->flaml[nlp,notebook,ray]) (6.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.1.3)\n",
      "Requirement already satisfied: nest-asyncio in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipykernel->jupyter->flaml[nlp,notebook,ray]) (1.5.4)\n",
      "Requirement already satisfied: traitlets<6.0,>=5.1.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipykernel->jupyter->flaml[nlp,notebook,ray]) (5.1.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipykernel->jupyter->flaml[nlp,notebook,ray]) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipykernel->jupyter->flaml[nlp,notebook,ray]) (7.1.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipywidgets->jupyter->flaml[nlp,notebook,ray]) (3.5.2)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipywidgets->jupyter->flaml[nlp,notebook,ray]) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipywidgets->jupyter->flaml[nlp,notebook,ray]) (1.0.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipywidgets->jupyter->flaml[nlp,notebook,ray]) (5.1.3)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jsonschema->ray[tune]~=1.10->flaml[nlp,notebook,ray]) (0.18.0)\n",
      "Requirement already satisfied: setuptools in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jsonschema->ray[tune]~=1.10->flaml[nlp,notebook,ray]) (60.9.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jupyter-console->jupyter->flaml[nlp,notebook,ray]) (3.0.28)\n",
      "Requirement already satisfied: pygments in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jupyter-console->jupyter->flaml[nlp,notebook,ray]) (2.11.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (0.5.11)\n",
      "Requirement already satisfied: jinja2>=2.4 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (3.0.3)\n",
      "Requirement already satisfied: bleach in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (0.1.2)\n",
      "Requirement already satisfied: jupyter-core in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (4.9.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (1.5.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (0.4)\n",
      "Requirement already satisfied: testpath in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from nbconvert->jupyter->flaml[nlp,notebook,ray]) (0.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from notebook->jupyter->flaml[nlp,notebook,ray]) (22.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from notebook->jupyter->flaml[nlp,notebook,ray]) (0.13.1)\n",
      "Requirement already satisfied: prometheus-client in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from notebook->jupyter->flaml[nlp,notebook,ray]) (0.13.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from notebook->jupyter->flaml[nlp,notebook,ray]) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from notebook->jupyter->flaml[nlp,notebook,ray]) (21.3.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from plotly->catboost>=0.26->flaml[nlp,notebook,ray]) (8.0.1)\n",
      "Requirement already satisfied: qtpy in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from qtconsole->jupyter->flaml[nlp,notebook,ray]) (2.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[tune]~=1.10->flaml[nlp,notebook,ray]) (1.13.3)\n",
      "Requirement already satisfied: black in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (22.1.0)\n",
      "Requirement already satisfied: pickleshare in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (4.8.0)\n",
      "Requirement already satisfied: decorator in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (5.1.1)\n",
      "Requirement already satisfied: backcall in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter->flaml[nlp,notebook,ray]) (2.0.1)\n",
      "Requirement already satisfied: wcwidth in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->flaml[nlp,notebook,ray]) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from terminado>=0.8.3->notebook->jupyter->flaml[nlp,notebook,ray]) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter->flaml[nlp,notebook,ray]) (21.2.0)\n",
      "Requirement already satisfied: webencodings in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from bleach->nbconvert->jupyter->flaml[nlp,notebook,ray]) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.8.3)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->flaml[nlp,notebook,ray]) (1.15.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from black->ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.4.3)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from black->ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.9.0)\n",
      "Requirement already satisfied: platformdirs>=2 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from black->ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (2.5.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from black->ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (2.0.1)\n",
      "Requirement already satisfied: executing in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.8.2)\n",
      "Requirement already satisfied: asttokens in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->flaml[nlp,notebook,ray]) (0.2.2)\n",
      "Requirement already satisfied: pycparser in /data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->flaml[nlp,notebook,ray]) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install flaml[nlp,ray,notebook];\n",
    "# from v0.6.6, catboost is made an optional dependency to build conda package.\n",
    "# to install catboost without installing the notebook option, you can run:\n",
    "# !pip install flaml[catboost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Classification Example\n",
    "## Load data and preprocess\n",
    "\n",
    "The Stanford Sentiment treebank (SST-2) dataset is a dataset for sentiment classification. First, let's load this dataset into pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/xliu127/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/xliu127/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/xliu127/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"glue\", \"sst2\", split=\"train\").to_pandas()\n",
    "dev_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\").to_pandas()\n",
    "test_dataset = load_dataset(\"glue\", \"sst2\", split=\"test\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 5 examples of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  idx\n",
       "0       hide new secretions from the parental units       0    0\n",
       "1               contains no wit , only labored gags       0    1\n",
       "2  that loves its characters and communicates som...      1    2\n",
       "3  remains utterly satisfied to remain the same t...      0    3\n",
       "4  on the worst revenge-of-the-nerds clichés the ...      0    4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data into X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_keys = [\"sentence\"]          # specify the column names of the input sentences\n",
    "label_key = \"label\"                                    # specify the column name of the label\n",
    "\n",
    "X_train, y_train = train_dataset[custom_sent_keys], train_dataset[label_key]\n",
    "X_val, y_val = dev_dataset[custom_sent_keys], dev_dataset[label_key]\n",
    "X_test = test_dataset[custom_sent_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run FLAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' import AutoML class from flaml package '''\n",
    "from flaml import AutoML\n",
    "automl = AutoML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"time_budget\": 100,                                 # setting the time budget\n",
    "    \"task\": \"seq-classification\",                       # specifying your task is seq-classification \n",
    "    \"custom_hpo_args\": {\"output_dir\": \"data/output/\"},  # specifying your output directory\n",
    "    \"gpu_per_trial\": 1,  \n",
    "    \"log_file_name\": \"seqclass.log\"                               # set to 0 if no GPU is available\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xliu127/projects/hyperopt/FLAML/flaml/data.py:270: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[str_columns] = X[str_columns].astype(\"string\")\n",
      "[flaml.automl: 02-26 15:22:48] {2059} INFO - task = seq-classification\n",
      "[flaml.automl: 02-26 15:22:48] {2061} INFO - Data split method: stratified\n",
      "[flaml.automl: 02-26 15:22:48] {2065} INFO - Evaluation method: holdout\n",
      "[flaml.automl: 02-26 15:22:48] {2146} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl: 02-26 15:22:48] {2204} INFO - List of ML learners in AutoML Run: ['transformer']\n",
      "[flaml.automl: 02-26 15:22:48] {2457} INFO - iteration 0, current learner transformer\n",
      "/data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14093726873397827, 'eval_automl_metric': 0.04587155963302747, 'eval_runtime': 8.2421, 'eval_samples_per_second': 105.799, 'eval_steps_per_second': 105.799, 'epoch': 0.23}\n",
      "{'eval_loss': 0.14093726873397827, 'eval_automl_metric': 0.04587155963302747, 'eval_runtime': 8.27, 'eval_samples_per_second': 105.441, 'eval_steps_per_second': 105.441, 'epoch': 0.23}\n",
      "{'train_runtime': 38.3098, 'train_samples_per_second': 783.088, 'train_steps_per_second': 24.511, 'train_loss': 0.1946606031605895, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 4\n",
      "[flaml.automl: 02-26 15:24:03] {2572} INFO - Estimated sufficient time budget=5080918s. Estimated necessary time budget=5081s.\n",
      "[flaml.automl: 02-26 15:24:03] {2619} INFO -  at 75.7s,\testimator transformer's best error=0.0459,\tbest estimator transformer's best error=0.0459\n",
      "[flaml.automl: 02-26 15:24:03] {2739} INFO - selected model: RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "[flaml.automl: 02-26 15:24:03] {2233} INFO - fit succeeded\n",
      "[flaml.automl: 02-26 15:24:03] {2234} INFO - Time taken to find the best model: 75.67725992202759\n",
      "[flaml.automl: 02-26 15:24:03] {2245} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "'''The main flaml automl API'''\n",
    "automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ML leaner: transformer\n",
      "Best hyperparmeter config: {'learning_rate': 1.0000000000000003e-05, 'num_train_epochs': 3.0, 'per_device_train_batch_size': 32, 'warmup_ratio': 0.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-06, 'seed': 42, 'global_max_steps': 71, 'FLAML_sample_size': 10000}\n",
      "Best accuracy on validation data: 0.9541\n",
      "Training duration of best run: 75.44 s\n"
     ]
    }
   ],
   "source": [
    "'''retrieve best config and best learner'''\n",
    "print('Best ML leaner:', automl.best_estimator)\n",
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best accuracy on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pickle and save the automl object'''\n",
    "import pickle\n",
    "with open('automl.pkl', 'wb') as f:\n",
    "    pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n",
    "'''load pickled automl object'''\n",
    "with open('automl.pkl', 'rb') as f:\n",
    "    automl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1821\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='456' max='456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [456/456 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels [0 0 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "'''compute predictions of testing dataset''' \n",
    "y_pred = automl.predict(X_test)\n",
    "print('Predicted labels', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 1.0000000000000003e-05, 'num_train_epochs': 3.0, 'per_device_train_batch_size': 32, 'warmup_ratio': 0.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-06, 'seed': 42, 'global_max_steps': 71, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 1.0000000000000003e-05, 'num_train_epochs': 3.0, 'per_device_train_batch_size': 32, 'warmup_ratio': 0.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-06, 'seed': 42, 'global_max_steps': 71, 'FLAML_sample_size': 10000}}\n"
     ]
    }
   ],
   "source": [
    "from flaml.data import get_output_from_log\n",
    "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "    get_output_from_log(filename=automl_settings['log_file_name'], time_budget=240)\n",
    "for config in config_history:\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Wall Clock Time (s)')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.scatter(time_history, 1 - np.array(valid_loss_history))\n",
    "plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Paraphrase Detection Example\n",
    "### Load data and preprocess\n",
    "\n",
    "The Microsoft Research Parallel Corpus (MRPC) is a dataset for paraphrase detection. An example pair of sentences is:\n",
    "\n",
    "* sentence 1: Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
    "* sentence 2: Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
    "\n",
    "First, let's load this dataset in to pandas dataframes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/xliu127/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/xliu127/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/xliu127/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\").to_pandas()\n",
    "dev_dataset = load_dataset(\"glue\", \"mrpc\", split=\"validation\").to_pandas()\n",
    "test_dataset = load_dataset(\"glue\", \"mrpc\", split=\"test\").to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 5 examples of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother , whom he called \" ...</td>\n",
       "      <td>Referring to him as only \" the witness \" , Amr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick 's before selling the c...</td>\n",
       "      <td>Yucaipa bought Dominick 's in 1995 for $ 693 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10 , the ship 's owners had published ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT , Tab shares were up 19 cents ...</td>\n",
       "      <td>Tab shares jumped 20 cents , or 4.6 % , to set...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $ 2.11 , or about 11 percent , ...</td>\n",
       "      <td>PG &amp; E Corp. shares jumped $ 1.63 or 8 percent...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  Amrozi accused his brother , whom he called \" ...   \n",
       "1  Yucaipa owned Dominick 's before selling the c...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT , Tab shares were up 19 cents ...   \n",
       "4  The stock rose $ 2.11 , or about 11 percent , ...   \n",
       "\n",
       "                                           sentence2  label  idx  \n",
       "0  Referring to him as only \" the witness \" , Amr...      1    0  \n",
       "1  Yucaipa bought Dominick 's in 1995 for $ 693 m...      0    1  \n",
       "2  On June 10 , the ship 's owners had published ...      1    2  \n",
       "3  Tab shares jumped 20 cents , or 4.6 % , to set...      0    3  \n",
       "4  PG & E Corp. shares jumped $ 1.63 or 8 percent...      1    4  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Run FLAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 02-26 15:24:47] {620} WARNING - checkpoint data/output/train_2022-02-26_15-22-52/train_02792629_1_n=1e-06,s=9223372036854775807,e=1e-05,s=-1,s=3.0,e=32,d=42,o=0.0,y=0.0_2022-02-26_15-22-52/checkpoint-71 not found\n"
     ]
    }
   ],
   "source": [
    "''' import AutoML class from flaml package '''\n",
    "from flaml import AutoML\n",
    "automl = AutoML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"time_budget\": 100,                                 # setting the time budget\n",
    "    \"task\": \"seq-classification\",                       # specifying your task is seq-classification \n",
    "    \"custom_hpo_args\": {\"output_dir\": \"data/output/\"},  # specifying your output directory\n",
    "    \"gpu_per_trial\": 1,    \n",
    "    \"log_file_name\": \"seqclass.log\"                             # set to 0 if no GPU is available\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 02-26 15:27:21] {2059} INFO - task = seq-classification\n",
      "[flaml.automl: 02-26 15:27:21] {2061} INFO - Data split method: stratified\n",
      "[flaml.automl: 02-26 15:27:21] {2065} INFO - Evaluation method: holdout\n",
      "[flaml.automl: 02-26 15:27:21] {2146} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl: 02-26 15:27:21] {2204} INFO - List of ML learners in AutoML Run: ['transformer']\n",
      "[flaml.automl: 02-26 15:27:21] {2457} INFO - iteration 0, current learner transformer\n",
      "/data/installation/anaconda3/envs/tmp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14149832725524902, 'eval_automl_metric': 0.044724770642201817, 'eval_runtime': 9.8455, 'eval_samples_per_second': 88.569, 'eval_steps_per_second': 88.569, 'epoch': 0.22}\n",
      "{'eval_loss': 0.14149832725524902, 'eval_automl_metric': 0.044724770642201817, 'eval_runtime': 10.3417, 'eval_samples_per_second': 84.319, 'eval_steps_per_second': 84.319, 'epoch': 0.22}\n",
      "{'train_runtime': 39.5017, 'train_samples_per_second': 759.46, 'train_steps_per_second': 23.771, 'train_loss': 0.19679561559704767, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 4\n",
      "[flaml.automl: 02-26 15:28:26] {2572} INFO - Estimated sufficient time budget=4394880s. Estimated necessary time budget=4395s.\n",
      "[flaml.automl: 02-26 15:28:26] {2619} INFO -  at 65.4s,\testimator transformer's best error=0.0447,\tbest estimator transformer's best error=0.0447\n",
      "[flaml.automl: 02-26 15:28:26] {2739} INFO - selected model: RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "[flaml.automl: 02-26 15:28:26] {2233} INFO - fit succeeded\n",
      "[flaml.automl: 02-26 15:28:26] {2234} INFO - Time taken to find the best model: 65.39444327354431\n"
     ]
    }
   ],
   "source": [
    "'''The main flaml automl API'''\n",
    "automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ML leaner: transformer\n",
      "Best hyperparmeter config: {'learning_rate': 1.0000000000000003e-05, 'num_train_epochs': 3.0, 'per_device_train_batch_size': 32, 'warmup_ratio': 0.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-06, 'seed': 42, 'global_max_steps': 69, 'FLAML_sample_size': 10000}\n",
      "Best accuracy on validation data: 0.9553\n",
      "Training duration of best run: 65.26 s\n"
     ]
    }
   ],
   "source": [
    "'''retrieve best config and best learner'''\n",
    "print('Best ML leaner:', automl.best_estimator)\n",
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best accuracy on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pickle and save the automl object'''\n",
    "import pickle\n",
    "with open('automl.pkl', 'wb') as f:\n",
    "    pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n",
    "'''load pickled automl object'''\n",
    "with open('automl.pkl', 'rb') as f:\n",
    "    automl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1821\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='456' max='456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [456/456 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels [0 0 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "'''compute predictions of testing dataset''' \n",
    "y_pred = automl.predict(X_test)\n",
    "print('Predicted labels', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Log history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Current Learner': 'transformer', 'Current Sample': 10000, 'Current Hyper-parameters': {'learning_rate': 1.0000000000000003e-05, 'num_train_epochs': 3.0, 'per_device_train_batch_size': 32, 'warmup_ratio': 0.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-06, 'seed': 42, 'global_max_steps': 71, 'FLAML_sample_size': 10000}, 'Best Learner': 'transformer', 'Best Hyper-parameters': {'learning_rate': 1.0000000000000003e-05, 'num_train_epochs': 3.0, 'per_device_train_batch_size': 32, 'warmup_ratio': 0.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-06, 'seed': 42, 'global_max_steps': 71, 'FLAML_sample_size': 10000}}\n"
     ]
    }
   ],
   "source": [
    "from flaml.data import get_output_from_log\n",
    "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "    get_output_from_log(filename=automl_settings['log_file_name'], time_budget=240)\n",
    "for config in config_history:\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Wall Clock Time (s)')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.scatter(time_history, 1 - np.array(valid_loss_history))\n",
    "plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9d36fc5b7c3dd4177ff1b60184dd696c0acc18150a44682abca4d769811bd46"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}