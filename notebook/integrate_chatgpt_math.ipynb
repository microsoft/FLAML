{
      "cells": [
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {
                        "slideshow": {
                              "slide_type": "slide"
                        }
                  },
                  "source": [
                        "Copyright (c) Microsoft Corporation. All rights reserved. \n",
                        "\n",
                        "Licensed under the MIT License.\n",
                        "\n",
                        "# Use FLAML to Tune ChatGPT\n",
                        "\n",
                        "In this notebook, we tune OpenAI ChatGPT model for math problem solving. We use [the MATH benchmark](https://crfm.stanford.edu/helm/latest/?group=math_chain_of_thought) for measuring mathematical problem solving on competition math problems with chain-of-thoughts style reasoning. \n",
                        "\n",
                        "## Requirements\n",
                        "\n",
                        "FLAML requires `Python>=3.7`. To run this notebook example, please install flaml with the [openai] option:\n",
                        "```bash\n",
                        "pip install flaml[openai]==1.2.0\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
                              "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
                              "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
                              "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
                        }
                  },
                  "outputs": [],
                  "source": [
                        "# %pip install flaml[openai]==1.2.0 datasets"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Set your OpenAI key:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 45,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:52.324240Z",
                              "iopub.status.busy": "2023-02-13T23:40:52.323783Z",
                              "iopub.status.idle": "2023-02-13T23:40:52.330570Z",
                              "shell.execute_reply": "2023-02-13T23:40:52.329750Z"
                        }
                  },
                  "outputs": [],
                  "source": [
                        "import os\n",
                        "\n",
                        "if \"OPENAI_API_KEY\" not in os.environ:\n",
                        "    os.environ[\"OPENAI_API_KEY\"] = \"<your OpenAI API key here>\""
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "When ChatGPT is available in Azure OpenAI, uncomment the following to use Azure OpenAI:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:52.333547Z",
                              "iopub.status.busy": "2023-02-13T23:40:52.333249Z",
                              "iopub.status.idle": "2023-02-13T23:40:52.336508Z",
                              "shell.execute_reply": "2023-02-13T23:40:52.335858Z"
                        }
                  },
                  "outputs": [],
                  "source": [
                        "# openai.api_type = \"azure\"\n",
                        "# openai.api_base = \"https://<your_endpoint>.openai.azure.com/\"\n",
                        "# openai.api_version = \"2023-3-01\""
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Load dataset\n",
                        "\n",
                        "First, we load the competition_math dataset. The dataset contains 457 \"Level 1\" examples. We use a random sample of 20 examples for tuning the generation hyperparameters and the remaining for evaluation. We use one demonstration example in the prompt."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:52.339977Z",
                              "iopub.status.busy": "2023-02-13T23:40:52.339556Z",
                              "iopub.status.idle": "2023-02-13T23:40:54.603349Z",
                              "shell.execute_reply": "2023-02-13T23:40:54.602630Z"
                        }
                  },
                  "outputs": [],
                  "source": [
                        "import datasets\n",
                        "\n",
                        "seed = 41\n",
                        "data = datasets.load_dataset(\"competition_math\")\n",
                        "train_data = data[\"train\"].shuffle(seed=seed)\n",
                        "test_data = data[\"test\"].shuffle(seed=seed)\n",
                        "n_tune_data = 20\n",
                        "tune_data = [\n",
                        "    {\n",
                        "        \"problem\": train_data[x][\"problem\"],\n",
                        "        \"solution\": train_data[x][\"solution\"],\n",
                        "    }\n",
                        "    for x in range(len(train_data)) if train_data[x][\"level\"] == \"Level 1\"\n",
                        "][:n_tune_data]\n",
                        "test_data = [\n",
                        "    {\n",
                        "        \"problem\": test_data[x][\"problem\"],\n",
                        "        \"solution\": test_data[x][\"solution\"],\n",
                        "    }\n",
                        "    for x in range(len(test_data)) if test_data[x][\"level\"] == \"Level 1\"\n",
                        "]\n",
                        "input_field = \"problem\"\n",
                        "output_fields = [\"solution\"]\n",
                        "print(\"max tokens in tuning data's canonical solutions\", max([len(x[\"solution\"].split()) for x in tune_data]))\n",
                        "print(len(tune_data), len(test_data))\n",
                        "# prompt template\n",
                        "prompts = [lambda data: \"Given a mathematics problem, determine the answer. Simplify your answer as much as possible.\\n###\\nProblem: What is the value of $\\\\sqrt{3! \\\\cdot 3!}$ expressed as a positive integer?\\nAnswer: $\\\\sqrt{3!\\\\cdot3!}$ is equal to $\\\\sqrt{(3!)^2}=3!=3\\\\cdot2\\\\cdot1=\\\\boxed{6}$.\\n###\\nProblem: %s\\nAnswer:\" + data[\"problem\"]]\n"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {
                        "slideshow": {
                              "slide_type": "slide"
                        }
                  },
                  "source": [
                        "Check a tuning example:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 47,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:54.607152Z",
                              "iopub.status.busy": "2023-02-13T23:40:54.606441Z",
                              "iopub.status.idle": "2023-02-13T23:40:54.610504Z",
                              "shell.execute_reply": "2023-02-13T23:40:54.609759Z"
                        },
                        "slideshow": {
                              "slide_type": "subslide"
                        },
                        "tags": []
                  },
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "What is $88 \\div 4 \\div 2$?\n"
                              ]
                        }
                  ],
                  "source": [
                        "print(tune_data[1][\"problem\"])"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Here is one example of the canonical solution:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 48,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:54.613590Z",
                              "iopub.status.busy": "2023-02-13T23:40:54.613168Z",
                              "iopub.status.idle": "2023-02-13T23:40:54.616873Z",
                              "shell.execute_reply": "2023-02-13T23:40:54.616193Z"
                        }
                  },
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "We perform division, going from left to right, to get our answer: \\[88 \\div 4 \\div 2 = 22 \\div 2 = \\boxed{11}.\\]\n"
                              ]
                        }
                  ],
                  "source": [
                        "print(tune_data[1][\"solution\"])"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Define Success Metric\n",
                        "\n",
                        "Before we start tuning, we need to define the success metric we want to opotimize. For each math task, if one of the returned responses has an equivalent answer to the canonical solution, we consider the task as successfully solved. Then we can define the mean success rate of a collection of tasks."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 49,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:54.626998Z",
                              "iopub.status.busy": "2023-02-13T23:40:54.626593Z",
                              "iopub.status.idle": "2023-02-13T23:40:54.631383Z",
                              "shell.execute_reply": "2023-02-13T23:40:54.630770Z"
                        }
                  },
                  "outputs": [],
                  "source": [
                        "from typing import Optional\n",
                        "\n",
                        "def remove_boxed(string: str) -> Optional[str]:\n",
                        "    \"\"\"Source: https://github.com/hendrycks/math\n",
                        "    Extract the text within a \\\\boxed{...} environment.\n",
                        "    Example:\n",
                        "    >>> remove_boxed(\\\\boxed{\\\\frac{2}{3}})\n",
                        "    \\\\frac{2}{3}\n",
                        "    \"\"\"\n",
                        "    left = \"\\\\boxed{\"\n",
                        "    try:\n",
                        "        assert string[: len(left)] == left\n",
                        "        assert string[-1] == \"}\"\n",
                        "        return string[len(left) : -1]\n",
                        "    except Exception:\n",
                        "        return None\n",
                        "\n",
                        "\n",
                        "def last_boxed_only_string(string: str) -> Optional[str]:\n",
                        "    \"\"\"Source: https://github.com/hendrycks/math\n",
                        "    Extract the last \\\\boxed{...} or \\\\fbox{...} element from a string.\n",
                        "    \"\"\"\n",
                        "    idx = string.rfind(\"\\\\boxed\")\n",
                        "    if idx < 0:\n",
                        "        idx = string.rfind(\"\\\\fbox\")\n",
                        "        if idx < 0:\n",
                        "            return None\n",
                        "\n",
                        "    i = idx\n",
                        "    right_brace_idx = None\n",
                        "    num_left_braces_open = 0\n",
                        "    while i < len(string):\n",
                        "        if string[i] == \"{\":\n",
                        "            num_left_braces_open += 1\n",
                        "        if string[i] == \"}\":\n",
                        "            num_left_braces_open -= 1\n",
                        "            if num_left_braces_open == 0:\n",
                        "                right_brace_idx = i\n",
                        "                break\n",
                        "        i += 1\n",
                        "\n",
                        "    if right_brace_idx is None:\n",
                        "        retval = None\n",
                        "    else:\n",
                        "        retval = string[idx : right_brace_idx + 1]\n",
                        "\n",
                        "    return retval\n",
                        "\n",
                        "\n",
                        "def _fix_fracs(string: str) -> str:\n",
                        "    \"\"\"Source: https://github.com/hendrycks/math\n",
                        "    Reformat fractions.\n",
                        "    Examples:\n",
                        "    >>> _fix_fracs(\"\\\\frac1b\")\n",
                        "    \\frac{1}{b}\n",
                        "    >>> _fix_fracs(\"\\\\frac12\")\n",
                        "    \\frac{1}{2}\n",
                        "    >>> _fix_fracs(\"\\\\frac1{72}\")\n",
                        "    \\frac{1}{72}\n",
                        "    \"\"\"\n",
                        "    substrs = string.split(\"\\\\frac\")\n",
                        "    new_str = substrs[0]\n",
                        "    if len(substrs) > 1:\n",
                        "        substrs = substrs[1:]\n",
                        "        for substr in substrs:\n",
                        "            new_str += \"\\\\frac\"\n",
                        "            if substr[0] == \"{\":\n",
                        "                new_str += substr\n",
                        "            else:\n",
                        "                try:\n",
                        "                    assert len(substr) >= 2\n",
                        "                except Exception:\n",
                        "                    return string\n",
                        "                a = substr[0]\n",
                        "                b = substr[1]\n",
                        "                if b != \"{\":\n",
                        "                    if len(substr) > 2:\n",
                        "                        post_substr = substr[2:]\n",
                        "                        new_str += \"{\" + a + \"}{\" + b + \"}\" + post_substr\n",
                        "                    else:\n",
                        "                        new_str += \"{\" + a + \"}{\" + b + \"}\"\n",
                        "                else:\n",
                        "                    if len(substr) > 2:\n",
                        "                        post_substr = substr[2:]\n",
                        "                        new_str += \"{\" + a + \"}\" + b + post_substr\n",
                        "                    else:\n",
                        "                        new_str += \"{\" + a + \"}\" + b\n",
                        "    string = new_str\n",
                        "    return string\n",
                        "\n",
                        "\n",
                        "def _fix_a_slash_b(string: str) -> str:\n",
                        "    \"\"\"Source: https://github.com/hendrycks/math\n",
                        "    Reformat fractions formatted as a/b to \\\\frac{a}{b}.\n",
                        "    Example:\n",
                        "    >>> _fix_a_slash_b(\"2/3\")\n",
                        "    \\frac{2}{3}\n",
                        "    \"\"\"\n",
                        "    if len(string.split(\"/\")) != 2:\n",
                        "        return string\n",
                        "    a_str = string.split(\"/\")[0]\n",
                        "    b_str = string.split(\"/\")[1]\n",
                        "    try:\n",
                        "        a = int(a_str)\n",
                        "        b = int(b_str)\n",
                        "        assert string == \"{}/{}\".format(a, b)\n",
                        "        new_string = \"\\\\frac{\" + str(a) + \"}{\" + str(b) + \"}\"\n",
                        "        return new_string\n",
                        "    except Exception:\n",
                        "        return string\n",
                        "\n",
                        "\n",
                        "def _remove_right_units(string: str) -> str:\n",
                        "    \"\"\"Source: https://github.com/hendrycks/math\n",
                        "    Remove units (on the right).\n",
                        "    \"\\\\text{ \" only ever occurs (at least in the val set) when describing units.\n",
                        "    \"\"\"\n",
                        "    if \"\\\\text{ \" in string:\n",
                        "        splits = string.split(\"\\\\text{ \")\n",
                        "        assert len(splits) == 2\n",
                        "        return splits[0]\n",
                        "    else:\n",
                        "        return string\n",
                        "\n",
                        "\n",
                        "def _fix_sqrt(string: str) -> str:\n",
                        "    \"\"\"Source: https://github.com/hendrycks/math\n",
                        "    Reformat square roots.\n",
                        "    Example:\n",
                        "    >>> _fix_sqrt(\"\\\\sqrt3\")\n",
                        "    \\sqrt{3}\n",
                        "    \"\"\"\n",
                        "    if \"\\\\sqrt\" not in string:\n",
                        "        return string\n",
                        "    splits = string.split(\"\\\\sqrt\")\n",
                        "    new_string = splits[0]\n",
                        "    for split in splits[1:]:\n",
                        "        if split[0] != \"{\":\n",
                        "            a = split[0]\n",
                        "            new_substr = \"\\\\sqrt{\" + a + \"}\" + split[1:]\n",
                        "        else:\n",
                        "            new_substr = \"\\\\sqrt\" + split\n",
                        "        new_string += new_substr\n",
                        "    return new_string\n",
                        "\n",
                        "\n",
                        "def _strip_string(string: str) -> str:\n",
                        "    \"\"\"Source: https://github.com/hendrycks/math\n",
                        "    Apply the reformatting helper functions above.\n",
                        "    \"\"\"\n",
                        "    # linebreaks\n",
                        "    string = string.replace(\"\\n\", \"\")\n",
                        "    # print(string)\n",
                        "\n",
                        "    # remove inverse spaces\n",
                        "    string = string.replace(\"\\\\!\", \"\")\n",
                        "    # print(string)\n",
                        "\n",
                        "    # replace \\\\ with \\\n",
                        "    string = string.replace(\"\\\\\\\\\", \"\\\\\")\n",
                        "    # print(string)\n",
                        "\n",
                        "    # replace tfrac and dfrac with frac\n",
                        "    string = string.replace(\"tfrac\", \"frac\")\n",
                        "    string = string.replace(\"dfrac\", \"frac\")\n",
                        "    # print(string)\n",
                        "\n",
                        "    # remove \\left and \\right\n",
                        "    string = string.replace(\"\\\\left\", \"\")\n",
                        "    string = string.replace(\"\\\\right\", \"\")\n",
                        "    # print(string)\n",
                        "\n",
                        "    # Remove circ (degrees)\n",
                        "    string = string.replace(\"^{\\\\circ}\", \"\")\n",
                        "    string = string.replace(\"^\\\\circ\", \"\")\n",
                        "\n",
                        "    # remove dollar signs\n",
                        "    string = string.replace(\"\\\\$\", \"\")\n",
                        "\n",
                        "    # remove units (on the right)\n",
                        "    string = _remove_right_units(string)\n",
                        "\n",
                        "    # remove percentage\n",
                        "    string = string.replace(\"\\\\%\", \"\")\n",
                        "    string = string.replace(\"\\%\", \"\")\n",
                        "\n",
                        "    # \" 0.\" equivalent to \" .\" and \"{0.\" equivalent to \"{.\" Alternatively, add \"0\" if \".\" is the start of the string\n",
                        "    string = string.replace(\" .\", \" 0.\")\n",
                        "    string = string.replace(\"{.\", \"{0.\")\n",
                        "    # if empty, return empty string\n",
                        "    if len(string) == 0:\n",
                        "        return string\n",
                        "    if string[0] == \".\":\n",
                        "        string = \"0\" + string\n",
                        "\n",
                        "    # to consider: get rid of e.g. \"k = \" or \"q = \" at beginning\n",
                        "    if len(string.split(\"=\")) == 2:\n",
                        "        if len(string.split(\"=\")[0]) <= 2:\n",
                        "            string = string.split(\"=\")[1]\n",
                        "\n",
                        "    # fix sqrt3 --> sqrt{3}\n",
                        "    string = _fix_sqrt(string)\n",
                        "\n",
                        "    # remove spaces\n",
                        "    string = string.replace(\" \", \"\")\n",
                        "\n",
                        "    # \\frac1b or \\frac12 --> \\frac{1}{b} and \\frac{1}{2}, etc.\n",
                        "    # Even works with \\frac1{72} (but not \\frac{72}1).\n",
                        "    # Also does a/b --> \\\\frac{a}{b}\n",
                        "    string = _fix_fracs(string)\n",
                        "\n",
                        "    # manually change 0.5 --> \\frac{1}{2}\n",
                        "    if string == \"0.5\":\n",
                        "        string = \"\\\\frac{1}{2}\"\n",
                        "\n",
                        "    # NOTE: X/Y changed to \\frac{X}{Y} in dataset, but in simple cases fix in case the model output is X/Y\n",
                        "    string = _fix_a_slash_b(string)\n",
                        "\n",
                        "    return string\n",
                        "\n",
                        "\n",
                        "def get_answer(solution: Optional[str]) -> Optional[str]:\n",
                        "    if solution is None:\n",
                        "        return None\n",
                        "    last_boxed = last_boxed_only_string(solution)\n",
                        "    if last_boxed is None:\n",
                        "        return None\n",
                        "    answer = remove_boxed(last_boxed)\n",
                        "    if answer is None:\n",
                        "        return None\n",
                        "    return answer\n",
                        "\n",
                        "\n",
                        "def is_equiv(str1: Optional[str], str2: Optional[str]) -> float:\n",
                        "    \"\"\"Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in\n",
                        "    - units\n",
                        "    - fractions\n",
                        "    - square roots\n",
                        "    - superfluous LaTeX.\n",
                        "    Source: https://github.com/hendrycks/math\n",
                        "    \"\"\"\n",
                        "    if str1 is None and str2 is None:\n",
                        "        print(\"WARNING: Both None\")\n",
                        "        return 1.0\n",
                        "    if str1 is None or str2 is None:\n",
                        "        return 0.0\n",
                        "\n",
                        "    try:\n",
                        "        ss1 = _strip_string(str1)\n",
                        "        ss2 = _strip_string(str2)\n",
                        "        return float(ss1 == ss2)\n",
                        "    except Exception:\n",
                        "        return float(str1 == str2)\n",
                        "\n",
                        "\n",
                        "def is_equiv_chain_of_thought(str1: str, str2: str) -> float:\n",
                        "    \"\"\"Strips the solution first before calling `is_equiv`.\"\"\"\n",
                        "    ans1 = get_answer(str1)\n",
                        "    ans2 = get_answer(str2)\n",
                        "\n",
                        "    return is_equiv(ans1, ans2)\n",
                        "\n",
                        "\n",
                        "def success_metrics(responses, solution, **args):\n",
                        "    \"\"\"Check if each response is correct.\n",
                        "    \n",
                        "    Args:\n",
                        "        responses (list): The list of responses.\n",
                        "        solution (str): The canonical solution.\n",
                        "    \n",
                        "    Returns:\n",
                        "        dict: The success metrics.\n",
                        "    \"\"\"\n",
                        "    success_list = []\n",
                        "    n = len(responses)\n",
                        "    for i in range(n):\n",
                        "        response = responses[i]\n",
                        "        succeed = is_equiv_chain_of_thought(response, solution)\n",
                        "        success_list.append(succeed)\n",
                        "    return {\n",
                        "        \"expected_success\": 1 - pow(1 - sum(success_list) / n, n),\n",
                        "        \"success\": any(s for s in success_list),\n",
                        "    }\n"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {
                        "slideshow": {
                              "slide_type": "slide"
                        }
                  },
                  "source": [
                        "## Use the tuning data to find a good configuration\n",
                        "\n",
                        "### Import the oai and tune subpackages from flaml.\n",
                        "\n",
                        "FLAML has provided an API for hyperparameter optimization of OpenAI ChatGPT models: `oai.ChatCompletion.tune` and to make a request with the tuned config: `oai.ChatCompletion.create`. First, we import oai from flaml:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 50,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:54.634335Z",
                              "iopub.status.busy": "2023-02-13T23:40:54.633929Z",
                              "iopub.status.idle": "2023-02-13T23:40:56.105700Z",
                              "shell.execute_reply": "2023-02-13T23:40:56.105085Z"
                        },
                        "slideshow": {
                              "slide_type": "slide"
                        }
                  },
                  "outputs": [],
                  "source": [
                        "from flaml import oai"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "For (local) reproducibility and cost efficiency, we cache responses from OpenAI. This will create a disk cache in \".cache/{seed}\". You can change `cache_path` in `set_cache()`. The cache for different seeds are stored separately."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 43,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:56.109177Z",
                              "iopub.status.busy": "2023-02-13T23:40:56.108624Z",
                              "iopub.status.idle": "2023-02-13T23:40:56.112651Z",
                              "shell.execute_reply": "2023-02-13T23:40:56.112076Z"
                        },
                        "slideshow": {
                              "slide_type": "slide"
                        }
                  },
                  "outputs": [],
                  "source": [
                        "oai.ChatCompletion.set_cache(seed)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Next, we evaluate the performance of chatGPT with a vanilla config on a subsample of the test data."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 71,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "{'expected_success': 0.5, 'success': 0.5}\n"
                              ]
                        }
                  ],
                  "source": [
                        "config_vanilla =  {'model': 'gpt-3.5-turbo', 'top_p': 1.0, 'temperature': 1.0, 'max_tokens': 200, 'n': 1, 'prompt': prompts[0], 'stop': \"###\"}\n",
                        "\n",
                        "subsample_test_data = test_data[0:10]\n",
                        "result_vanilla = oai.ChatCompletion.test(subsample_test_data, config_vanilla, eval_func=success_metrics)\n",
                        "print(result_vanilla)"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Perform tuning\n",
                        "\n",
                        "The tuning will take a while to finish, depending on the optimization budget. The tuning will be performed under the specified optimization budgets.\n",
                        "\n",
                        "* `inference_budget` is the target average inference budget per instance in the benchmark. For example, 0.002 means the target inference budget is 0.002 dollars, which translates to 1000 tokens (input + output combined) if the gpt-3.5-turbo model is used.\n",
                        "* `optimization_budget` is the total budget allowed to perform the tuning. For example, 0.5 means 0.5 dollars are allowed in total, which translates to 250K tokens for the gpt-3.5-turbo model.\n",
                        "* `num_sumples` is the number of different hyperparameter configurations which is allowed to try. The tuning will stop after either num_samples trials or after optimization_budget dollars spent, whichever happens first. -1 means no hard restriction in the number of trials and the actual number is decided by `optimization_budget`.\n",
                        "\n",
                        "Users can specify tuning data, optimization metric, optimization mode, evaluation function, search spaces etc.. The default search space is:\n",
                        "\n",
                        "```python\n",
                        "price1K = {\n",
                        "    \"gpt-3.5-turbo\": 0.002,\n",
                        "}\n",
                        "\n",
                        "default_search_space = {\n",
                        "    \"model\": tune.choice(list(price1K.keys())),\n",
                        "    \"temperature_or_top_p\": tune.choice(\n",
                        "        [\n",
                        "            {\"temperature\": tune.uniform(0, 1)},\n",
                        "            {\"top_p\": tune.uniform(0, 1)},\n",
                        "        ]\n",
                        "    ),\n",
                        "    \"max_tokens\": tune.lograndint(50, 1000),\n",
                        "    \"n\": tune.randint(1, 100),\n",
                        "    \"prompt\": \"{prompt}\",\n",
                        "}\n",
                        "```\n",
                        "\n",
                        "The default search space can be overriden by users' input.\n",
                        "For example, the following code specifies a fixed prompt template and a list of stop sequences. For hyperparameters which don't appear in users' input, the default search space will be used."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 8,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:40:56.115383Z",
                              "iopub.status.busy": "2023-02-13T23:40:56.114975Z",
                              "iopub.status.idle": "2023-02-13T23:41:55.045654Z",
                              "shell.execute_reply": "2023-02-13T23:41:55.044973Z"
                        }
                  },
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "\u001b[32m[I 2023-03-14 15:51:06,735]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "[flaml.tune.tune: 03-14 15:51:06] {811} INFO - trial 1 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.36280922847807595}, 'max_tokens': 347, 'n': 10, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:51:48] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.012004, 'cost': 0.012004, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.36280922847807595}, 'max_tokens': 347, 'n': 10, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.36280922847807595}, 'config/max_tokens': 347, 'config/n': 10, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 41.492459535598755}\n",
                                    "[flaml.tune.tune: 03-14 15:51:48] {811} INFO - trial 2 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6336482349262754}, 'max_tokens': 470, 'n': 50, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:51:48] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6336482349262754}, 'max_tokens': 470, 'n': 50, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6336482349262754}, 'config/max_tokens': 470, 'config/n': 50, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0011649131774902344}\n",
                                    "[flaml.tune.tune: 03-14 15:51:48] {811} INFO - trial 3 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7605307121989587}, 'max_tokens': 82, 'n': 9, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:53:05] {215} INFO - result: {'expected_success': 0.5920890244914228, 'success': 0.6, 'total_cost': 0.040448000000000005, 'cost': 0.028444000000000004, 'inference_cost': 0.0014222000000000002, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7605307121989587}, 'max_tokens': 82, 'n': 9, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.7605307121989587}, 'config/max_tokens': 82, 'config/n': 9, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 77.44748282432556}\n",
                                    "[flaml.tune.tune: 03-14 15:53:05] {811} INFO - trial 4 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.003948266327914451}, 'max_tokens': 231, 'n': 81, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:53:18] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.04722200000000001, 'cost': 0.0067740000000000005, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.003948266327914451}, 'max_tokens': 231, 'n': 81, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.003948266327914451}, 'config/max_tokens': 231, 'config/n': 81, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 13.027142763137817}\n",
                                    "[flaml.tune.tune: 03-14 15:53:18] {811} INFO - trial 5 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.29187606817063316}, 'max_tokens': 781, 'n': 71, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:53:18] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.29187606817063316}, 'max_tokens': 781, 'n': 71, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.29187606817063316}, 'config/max_tokens': 781, 'config/n': 71, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.001203298568725586}\n",
                                    "[flaml.tune.tune: 03-14 15:53:18] {811} INFO - trial 6 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3733407600514692}, 'max_tokens': 375, 'n': 44, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:53:18] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3733407600514692}, 'max_tokens': 375, 'n': 44, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.3733407600514692}, 'config/max_tokens': 375, 'config/n': 44, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0011205673217773438}\n",
                                    "[flaml.tune.tune: 03-14 15:53:18] {811} INFO - trial 7 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.5131382425543909}, 'max_tokens': 350, 'n': 60, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:53:18] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.5131382425543909}, 'max_tokens': 350, 'n': 60, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.5131382425543909}, 'config/max_tokens': 350, 'config/n': 60, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0021543502807617188}\n",
                                    "[flaml.tune.tune: 03-14 15:53:18] {811} INFO - trial 8 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9086488808086682}, 'max_tokens': 129, 'n': 9, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:55:01] {215} INFO - result: {'expected_success': 0.845289843330924, 'success': 0.9, 'total_cost': 0.08091200000000001, 'cost': 0.033690000000000005, 'inference_cost': 0.0016845, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9086488808086682}, 'max_tokens': 129, 'n': 9, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9086488808086682}, 'config/max_tokens': 129, 'config/n': 9, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 102.69731760025024}\n",
                                    "[flaml.tune.tune: 03-14 15:55:01] {811} INFO - trial 9 config: {'model': 'gpt-3.5-turbo', 'max_tokens': 130, 'n': 1, 'prompt': 0, 'stop': 0, 'temperature_or_top_p': {'temperature': 0.7719496887712277}}\n",
                                    "[flaml.tune.tune: 03-14 15:56:19] {215} INFO - result: {'expected_success': 0.6, 'success': 0.6, 'total_cost': 0.08969200000000001, 'cost': 0.00878, 'inference_cost': 0.000439, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'max_tokens': 130, 'n': 1, 'prompt': 0, 'stop': 0, 'temperature_or_top_p': {'temperature': 0.7719496887712277}}, 'config/model': 'gpt-3.5-turbo', 'config/max_tokens': 130, 'config/n': 1, 'config/prompt': 0, 'config/stop': 0, 'config/temperature_or_top_p': {'temperature': 0.7719496887712277}, 'experiment_tag': 'exp', 'time_total_s': 78.42686462402344}\n",
                                    "[flaml.tune.tune: 03-14 15:56:19] {811} INFO - trial 10 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8286813263076767}, 'max_tokens': 57, 'n': 63, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:56:25] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.09484200000000001, 'cost': 0.00515, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8286813263076767}, 'max_tokens': 57, 'n': 63, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.8286813263076767}, 'config/max_tokens': 57, 'config/n': 63, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 5.786454439163208}\n",
                                    "[flaml.tune.tune: 03-14 15:56:25] {811} INFO - trial 11 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.1989475396788123}, 'max_tokens': 650, 'n': 35, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:56:25] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.1989475396788123}, 'max_tokens': 650, 'n': 35, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.1989475396788123}, 'config/max_tokens': 650, 'config/n': 35, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0012047290802001953}\n",
                                    "[flaml.tune.tune: 03-14 15:56:25] {811} INFO - trial 12 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8839364795611863}, 'max_tokens': 132, 'n': 17, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:56:37] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.09909600000000002, 'cost': 0.004254, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8839364795611863}, 'max_tokens': 132, 'n': 17, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.8839364795611863}, 'config/max_tokens': 132, 'config/n': 17, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 11.63356637954712}\n",
                                    "[flaml.tune.tune: 03-14 15:56:37] {811} INFO - trial 13 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8211056578369285}, 'max_tokens': 78, 'n': 39, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:56:37] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8211056578369285}, 'max_tokens': 78, 'n': 39, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.8211056578369285}, 'config/max_tokens': 78, 'config/n': 39, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0011279582977294922}\n",
                                    "[flaml.tune.tune: 03-14 15:56:37] {811} INFO - trial 14 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.0422875090290305}, 'max_tokens': 144, 'n': 3, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:57:56] {215} INFO - result: {'expected_success': 0.5, 'success': 0.5, 'total_cost': 0.11383600000000002, 'cost': 0.01474, 'inference_cost': 0.000737, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.0422875090290305}, 'max_tokens': 144, 'n': 3, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.0422875090290305}, 'config/max_tokens': 144, 'config/n': 3, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 79.02098870277405}\n",
                                    "[flaml.tune.tune: 03-14 15:57:56] {811} INFO - trial 15 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5747556681857633}, 'max_tokens': 84, 'n': 4, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:59:06] {215} INFO - result: {'expected_success': 0.5615234375, 'success': 0.6, 'total_cost': 0.12956200000000004, 'cost': 0.015726, 'inference_cost': 0.0007863, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5747556681857633}, 'max_tokens': 84, 'n': 4, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.5747556681857633}, 'config/max_tokens': 84, 'config/n': 4, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 70.04061651229858}\n",
                                    "[flaml.tune.tune: 03-14 15:59:06] {811} INFO - trial 16 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.987304244540037}, 'max_tokens': 50, 'n': 18, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 15:59:59] {215} INFO - result: {'expected_success': 0.6079806955418933, 'success': 0.65, 'total_cost': 0.16748000000000013, 'cost': 0.037918, 'inference_cost': 0.0018959, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.987304244540037}, 'max_tokens': 50, 'n': 18, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.987304244540037}, 'config/max_tokens': 50, 'config/n': 18, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 52.74128246307373}\n",
                                    "[flaml.tune.tune: 03-14 15:59:59] {811} INFO - trial 17 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9990362361100525}, 'max_tokens': 51, 'n': 24, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:00:04] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.17164200000000013, 'cost': 0.004162, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9990362361100525}, 'max_tokens': 51, 'n': 24, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9990362361100525}, 'config/max_tokens': 51, 'config/n': 24, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 5.604432821273804}\n",
                                    "[flaml.tune.tune: 03-14 16:00:04] {811} INFO - trial 18 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9892826883313328}, 'max_tokens': 144, 'n': 22, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:00:04] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9892826883313328}, 'max_tokens': 144, 'n': 22, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9892826883313328}, 'config/max_tokens': 144, 'config/n': 22, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0011968612670898438}\n",
                                    "[flaml.tune.tune: 03-14 16:00:04] {811} INFO - trial 19 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6157113450283662}, 'max_tokens': 214, 'n': 95, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:00:04] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6157113450283662}, 'max_tokens': 214, 'n': 95, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6157113450283662}, 'config/max_tokens': 214, 'config/n': 95, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.001064300537109375}\n",
                                    "[flaml.tune.tune: 03-14 16:00:04] {811} INFO - trial 20 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.32687425301877854}, 'max_tokens': 55, 'n': 31, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:00:04] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.32687425301877854}, 'max_tokens': 55, 'n': 31, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.32687425301877854}, 'config/max_tokens': 55, 'config/n': 31, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0010116100311279297}\n",
                                    "[flaml.tune.tune: 03-14 16:00:04] {811} INFO - trial 21 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9915242842721368}, 'max_tokens': 97, 'n': 14, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:00:29] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.18178200000000014, 'cost': 0.01014, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9915242842721368}, 'max_tokens': 97, 'n': 14, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9915242842721368}, 'config/max_tokens': 97, 'config/n': 14, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 24.19725012779236}\n",
                                    "[flaml.tune.tune: 03-14 16:00:29] {811} INFO - trial 22 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6810064227782602}, 'max_tokens': 208, 'n': 52, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:00:29] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6810064227782602}, 'max_tokens': 208, 'n': 52, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6810064227782602}, 'config/max_tokens': 208, 'config/n': 52, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0015413761138916016}\n",
                                    "[flaml.tune.tune: 03-14 16:00:29] {811} INFO - trial 23 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.04646269376572404}, 'max_tokens': 316, 'n': 1, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:01:54] {215} INFO - result: {'expected_success': 0.65, 'success': 0.65, 'total_cost': 0.19081000000000017, 'cost': 0.009028000000000003, 'inference_cost': 0.00045139999999999997, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.04646269376572404}, 'max_tokens': 316, 'n': 1, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.04646269376572404}, 'config/max_tokens': 316, 'config/n': 1, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 85.6433093547821}\n",
                                    "[flaml.tune.tune: 03-14 16:01:54] {811} INFO - trial 24 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.004534682871613183}, 'max_tokens': 462, 'n': 1, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:03:35] {215} INFO - result: {'expected_success': 0.6, 'success': 0.6, 'total_cost': 0.2004560000000003, 'cost': 0.009646000000000002, 'inference_cost': 0.0004823, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.004534682871613183}, 'max_tokens': 462, 'n': 1, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.004534682871613183}, 'config/max_tokens': 462, 'config/n': 1, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 100.7299873828888}\n",
                                    "[flaml.tune.tune: 03-14 16:03:35] {811} INFO - trial 25 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.1457856792803791}, 'max_tokens': 279, 'n': 11, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:04:16] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.2157520000000003, 'cost': 0.015296000000000002, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.1457856792803791}, 'max_tokens': 279, 'n': 11, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.1457856792803791}, 'config/max_tokens': 279, 'config/n': 11, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 40.95030498504639}\n",
                                    "[flaml.tune.tune: 03-14 16:04:16] {811} INFO - trial 26 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.4381807764626776}, 'max_tokens': 169, 'n': 26, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:04:16] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.4381807764626776}, 'max_tokens': 169, 'n': 26, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.4381807764626776}, 'config/max_tokens': 169, 'config/n': 26, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0017774105072021484}\n",
                                    "[flaml.tune.tune: 03-14 16:04:16] {811} INFO - trial 27 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.23162454328453796}, 'max_tokens': 578, 'n': 1, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:05:50] {215} INFO - result: {'expected_success': 0.6, 'success': 0.6, 'total_cost': 0.22521400000000036, 'cost': 0.009462000000000002, 'inference_cost': 0.0004731, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.23162454328453796}, 'max_tokens': 578, 'n': 1, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.23162454328453796}, 'config/max_tokens': 578, 'config/n': 1, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 94.5561830997467}\n",
                                    "[flaml.tune.tune: 03-14 16:05:50] {811} INFO - trial 28 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9510470544558363}, 'max_tokens': 108, 'n': 17, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:05:50] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9510470544558363}, 'max_tokens': 108, 'n': 17, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9510470544558363}, 'config/max_tokens': 108, 'config/n': 17, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0015399456024169922}\n",
                                    "[flaml.tune.tune: 03-14 16:05:50] {811} INFO - trial 29 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7234828168025107}, 'max_tokens': 287, 'n': 6, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:08:02] {215} INFO - result: {'expected_success': 0.8700788751714675, 'success': 0.9, 'total_cost': 0.25329200000000035, 'cost': 0.028078, 'inference_cost': 0.0014039000000000002, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7234828168025107}, 'max_tokens': 287, 'n': 6, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.7234828168025107}, 'config/max_tokens': 287, 'config/n': 6, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 131.33223390579224}\n",
                                    "[flaml.tune.tune: 03-14 16:08:02] {811} INFO - trial 30 config: {'model': 'gpt-3.5-turbo', 'max_tokens': 469, 'n': 1, 'prompt': 0, 'stop': 0, 'temperature_or_top_p': {'temperature': 0.7561202724076562}}\n",
                                    "[flaml.tune.tune: 03-14 16:09:32] {215} INFO - result: {'expected_success': 0.65, 'success': 0.65, 'total_cost': 0.2626020000000004, 'cost': 0.009310000000000002, 'inference_cost': 0.00046550000000000004, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'max_tokens': 469, 'n': 1, 'prompt': 0, 'stop': 0, 'temperature_or_top_p': {'temperature': 0.7561202724076562}}, 'config/model': 'gpt-3.5-turbo', 'config/max_tokens': 469, 'config/n': 1, 'config/prompt': 0, 'config/stop': 0, 'config/temperature_or_top_p': {'temperature': 0.7561202724076562}, 'experiment_tag': 'exp', 'time_total_s': 90.40418004989624}\n",
                                    "[flaml.tune.tune: 03-14 16:09:32] {811} INFO - trial 31 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9990495004030451}, 'max_tokens': 285, 'n': 7, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:11:55] {215} INFO - result: {'expected_success': 0.858994490876615, 'success': 0.9, 'total_cost': 0.2971980000000004, 'cost': 0.034595999999999995, 'inference_cost': 0.0017298, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9990495004030451}, 'max_tokens': 285, 'n': 7, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.9990495004030451}, 'config/max_tokens': 285, 'config/n': 7, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 142.4644045829773}\n",
                                    "[flaml.tune.tune: 03-14 16:11:55] {811} INFO - trial 32 config: {'model': 'gpt-3.5-turbo', 'max_tokens': 234, 'n': 9, 'prompt': 0, 'stop': 0, 'temperature_or_top_p': {'top_p': 0.8486841535714768}}\n",
                                    "[flaml.tune.tune: 03-14 16:13:50] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.3319100000000005, 'cost': 0.03471200000000001, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'max_tokens': 234, 'n': 9, 'prompt': 0, 'stop': 0, 'temperature_or_top_p': {'top_p': 0.8486841535714768}}, 'config/model': 'gpt-3.5-turbo', 'config/max_tokens': 234, 'config/n': 9, 'config/prompt': 0, 'config/stop': 0, 'config/temperature_or_top_p': {'top_p': 0.8486841535714768}, 'experiment_tag': 'exp', 'time_total_s': 115.61605286598206}\n",
                                    "[flaml.tune.tune: 03-14 16:13:50] {811} INFO - trial 33 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9565385266822548}, 'max_tokens': 260, 'n': 9, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:15:59] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.3672420000000006, 'cost': 0.035331999999999995, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9565385266822548}, 'max_tokens': 260, 'n': 9, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.9565385266822548}, 'config/max_tokens': 260, 'config/n': 9, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 128.9455919265747}\n",
                                    "[flaml.tune.tune: 03-14 16:15:59] {811} INFO - trial 34 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9689485352565373}, 'max_tokens': 461, 'n': 29, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:15:59] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9689485352565373}, 'max_tokens': 461, 'n': 29, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.9689485352565373}, 'config/max_tokens': 461, 'config/n': 29, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.001238107681274414}\n",
                                    "[flaml.tune.tune: 03-14 16:15:59] {811} INFO - trial 35 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7058565979745426}, 'max_tokens': 180, 'n': 8, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:17:53] {215} INFO - result: {'expected_success': 0.8783969640731811, 'success': 0.95, 'total_cost': 0.4009880000000007, 'cost': 0.033746000000000005, 'inference_cost': 0.0016873, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7058565979745426}, 'max_tokens': 180, 'n': 8, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.7058565979745426}, 'config/max_tokens': 180, 'config/n': 8, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 113.54444169998169}\n",
                                    "[flaml.tune.tune: 03-14 16:17:53] {811} INFO - trial 36 config: {'model': 'gpt-3.5-turbo', 'max_tokens': 287, 'n': 17, 'prompt': 0, 'stop': 0, 'temperature_or_top_p': {'top_p': 0.7983545225154292}}\n",
                                    "[flaml.tune.tune: 03-14 16:17:53] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'max_tokens': 287, 'n': 17, 'prompt': 0, 'stop': 0, 'temperature_or_top_p': {'top_p': 0.7983545225154292}}, 'config/model': 'gpt-3.5-turbo', 'config/max_tokens': 287, 'config/n': 17, 'config/prompt': 0, 'config/stop': 0, 'config/temperature_or_top_p': {'top_p': 0.7983545225154292}, 'experiment_tag': 'exp', 'time_total_s': 0.0012698173522949219}\n",
                                    "[flaml.tune.tune: 03-14 16:17:53] {811} INFO - trial 37 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7029639899419969}, 'max_tokens': 182, 'n': 9, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:19:39] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.43498200000000065, 'cost': 0.033993999999999996, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7029639899419969}, 'max_tokens': 182, 'n': 9, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.7029639899419969}, 'config/max_tokens': 182, 'config/n': 9, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 106.50576138496399}\n",
                                    "[flaml.tune.tune: 03-14 16:19:39] {811} INFO - trial 38 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7687882521473703}, 'max_tokens': 174, 'n': 7, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:21:40] {215} INFO - result: {'expected_success': 0.7816031464052272, 'success': 0.8, 'total_cost': 0.4646340000000007, 'cost': 0.029652000000000005, 'inference_cost': 0.0014826, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7687882521473703}, 'max_tokens': 174, 'n': 7, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.7687882521473703}, 'config/max_tokens': 174, 'config/n': 7, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 120.58803677558899}\n",
                                    "[flaml.tune.tune: 03-14 16:21:40] {811} INFO - trial 39 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7450067453008995}, 'max_tokens': 260, 'n': 13, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:21:40] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7450067453008995}, 'max_tokens': 260, 'n': 13, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.7450067453008995}, 'config/max_tokens': 260, 'config/n': 13, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0011603832244873047}\n",
                                    "[flaml.tune.tune: 03-14 16:21:40] {811} INFO - trial 40 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9923771779024542}, 'max_tokens': 432, 'n': 22, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:21:40] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9923771779024542}, 'max_tokens': 432, 'n': 22, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.9923771779024542}, 'config/max_tokens': 432, 'config/n': 22, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0013191699981689453}\n",
                                    "[flaml.tune.tune: 03-14 16:21:40] {811} INFO - trial 41 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.5726923990026718}, 'max_tokens': 115, 'n': 6, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {215} INFO - result: {'expected_success': 0.7386402606310013, 'success': 0.8, 'total_cost': 0.4885020000000007, 'cost': 0.023868, 'inference_cost': 0.0011934, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.5726923990026718}, 'max_tokens': 115, 'n': 6, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.5726923990026718}, 'config/max_tokens': 115, 'config/n': 6, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 87.40534114837646}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {811} INFO - trial 42 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.84809714481217}, 'max_tokens': 322, 'n': 97, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.84809714481217}, 'max_tokens': 322, 'n': 97, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.84809714481217}, 'config/max_tokens': 322, 'config/n': 97, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0014872550964355469}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {811} INFO - trial 43 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.35087690106940034}, 'max_tokens': 226, 'n': 80, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.35087690106940034}, 'max_tokens': 226, 'n': 80, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.35087690106940034}, 'config/max_tokens': 226, 'config/n': 80, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0009944438934326172}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {811} INFO - trial 44 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.6227446802645338}, 'max_tokens': 384, 'n': 19, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.6227446802645338}, 'max_tokens': 384, 'n': 19, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.6227446802645338}, 'config/max_tokens': 384, 'config/n': 19, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0012652873992919922}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {811} INFO - trial 45 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.8598724029593373}, 'max_tokens': 995, 'n': 41, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.8598724029593373}, 'max_tokens': 995, 'n': 41, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.8598724029593373}, 'config/max_tokens': 995, 'config/n': 41, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0009145736694335938}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {811} INFO - trial 46 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.39235911392639355}, 'max_tokens': 192, 'n': 12, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.39235911392639355}, 'max_tokens': 192, 'n': 12, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.39235911392639355}, 'config/max_tokens': 192, 'config/n': 12, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0012505054473876953}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {811} INFO - trial 47 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.8657253319857423}, 'max_tokens': 152, 'n': 33, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {215} INFO - result: {'inference_cost': inf, 'expected_success': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.8657253319857423}, 'max_tokens': 152, 'n': 33, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.8657253319857423}, 'config/max_tokens': 152, 'config/n': 33, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.001186370849609375}\n",
                                    "[flaml.tune.tune: 03-14 16:23:07] {811} INFO - trial 48 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7366342696104722}, 'max_tokens': 171, 'n': 7, 'prompt': 0, 'stop': 0}\n",
                                    "[flaml.tune.tune: 03-14 16:23:51] {215} INFO - result: {'expected_success': 0, 'total_cost': 0.5008200000000007, 'cost': 0.012318, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7366342696104722}, 'max_tokens': 171, 'n': 7, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.7366342696104722}, 'config/max_tokens': 171, 'config/n': 7, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 43.35566711425781}\n",
                                    "[flaml.tune.tune: 03-14 16:23:51] {834} WARNING - fail to sample a trial for 100 times in a row, stopping.\n"
                              ]
                        }
                  ],
                  "source": [
                        "import logging\n",
                        "\n",
                        "config, analysis = oai.ChatCompletion.tune(\n",
                        "    data=tune_data,  # the data for tuning\n",
                        "    metric=\"expected_success\",  # the metric to optimize\n",
                        "    mode=\"max\",  # the optimization mode\n",
                        "    eval_func=success_metrics,  # the evaluation function to return the success metrics\n",
                        "    # log_file_name=\"logs/math.log\",  # the log file name\n",
                        "    inference_budget=0.002,  # the inference budget (dollar)\n",
                        "    optimization_budget=0.5,  # the optimization budget (dollar)\n",
                        "    # num_samples can further limit the number of trials for different hyperparameter configurations;\n",
                        "    # -1 means decided by the optimization budget only\n",
                        "    num_samples=-1,\n",
                        "    prompt=prompts,  # the prompt templates to choose from\n",
                        "    stop=\"###\",  # the stop sequence\n",
                        "    logging_level=logging.INFO,  # the logging level\n",
                        ")\n"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Output tuning results\n",
                        "\n",
                        "After the tuning, we can print out the config and the result found by FLAML:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 9,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:41:55.049204Z",
                              "iopub.status.busy": "2023-02-13T23:41:55.048871Z",
                              "iopub.status.idle": "2023-02-13T23:41:55.053284Z",
                              "shell.execute_reply": "2023-02-13T23:41:55.052574Z"
                        }
                  },
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "optimized config {'model': 'gpt-3.5-turbo', 'max_tokens': 180, 'n': 8, 'prompt': <function <lambda> at 0x7f442a35f790>, 'stop': '###', 'top_p': 0.7058565979745426}\n",
                                    "best result on tuning data {'expected_success': 0.8783969640731811, 'success': 0.95, 'total_cost': 0.4009880000000007, 'cost': 0.033746000000000005, 'inference_cost': 0.0016873, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.7058565979745426}, 'max_tokens': 180, 'n': 8, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.7058565979745426}, 'config/max_tokens': 180, 'config/n': 8, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 113.54444169998169}\n"
                              ]
                        }
                  ],
                  "source": [
                        "print(\"optimized config\", config)\n",
                        "print(\"best result on tuning data\", analysis.best_result)"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {
                        "slideshow": {
                              "slide_type": "slide"
                        }
                  },
                  "source": [
                        "### Make a request with the tuned config\n",
                        "\n",
                        "We can apply the tuned config on the request for an example task:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 10,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:41:55.056205Z",
                              "iopub.status.busy": "2023-02-13T23:41:55.055631Z",
                              "iopub.status.idle": "2023-02-13T23:41:56.039259Z",
                              "shell.execute_reply": "2023-02-13T23:41:56.038427Z"
                        },
                        "slideshow": {
                              "slide_type": "subslide"
                        },
                        "tags": []
                  },
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "{\n",
                                    "  \"choices\": [\n",
                                    "    {\n",
                                    "      \"finish_reason\": \"stop\",\n",
                                    "      \"index\": 0,\n",
                                    "      \"message\": {\n",
                                    "        \"content\": \"\\n\\nAnswer: $88 \\\\div 4 \\\\div 2$ is equal to $(88 \\\\div 4) \\\\div 2 = 22 \\\\div 2 = \\\\boxed{11}$.\",\n",
                                    "        \"role\": \"assistant\"\n",
                                    "      }\n",
                                    "    },\n",
                                    "    {\n",
                                    "      \"finish_reason\": \"stop\",\n",
                                    "      \"index\": 1,\n",
                                    "      \"message\": {\n",
                                    "        \"content\": \"\\n\\nAnswer: $88 \\\\div 4 \\\\div 2$ is equal to $(88 \\\\div 4) \\\\div 2 = 22 \\\\div 2 = \\\\boxed{11}$.\",\n",
                                    "        \"role\": \"assistant\"\n",
                                    "      }\n",
                                    "    },\n",
                                    "    {\n",
                                    "      \"finish_reason\": \"stop\",\n",
                                    "      \"index\": 2,\n",
                                    "      \"message\": {\n",
                                    "        \"content\": \"\\n\\nAnswer: $88 \\\\div 4 \\\\div 2$ is equal to $(88 \\\\div 4) \\\\div 2 = 22 \\\\div 2 = \\\\boxed{11}$.\",\n",
                                    "        \"role\": \"assistant\"\n",
                                    "      }\n",
                                    "    },\n",
                                    "    {\n",
                                    "      \"finish_reason\": \"stop\",\n",
                                    "      \"index\": 3,\n",
                                    "      \"message\": {\n",
                                    "        \"content\": \"\\n\\nAnswer: $88 \\\\div 4 \\\\div 2$ is equal to $\\\\frac{88}{4}\\\\div 2 = 22 \\\\div 2 = \\\\boxed{11}$.\",\n",
                                    "        \"role\": \"assistant\"\n",
                                    "      }\n",
                                    "    },\n",
                                    "    {\n",
                                    "      \"finish_reason\": \"stop\",\n",
                                    "      \"index\": 4,\n",
                                    "      \"message\": {\n",
                                    "        \"content\": \"\\n\\nAnswer: $88 \\\\div 4 \\\\div 2$ is equal to $(88 \\\\div 4) \\\\div 2 = 22 \\\\div 2 = \\\\boxed{11}$.\",\n",
                                    "        \"role\": \"assistant\"\n",
                                    "      }\n",
                                    "    },\n",
                                    "    {\n",
                                    "      \"finish_reason\": \"stop\",\n",
                                    "      \"index\": 5,\n",
                                    "      \"message\": {\n",
                                    "        \"content\": \"\\n\\nAnswer: $88 \\\\div 4 \\\\div 2$ is equal to $(88 \\\\div 4) \\\\div 2 = 22 \\\\div 2 = \\\\boxed{11}$.\",\n",
                                    "        \"role\": \"assistant\"\n",
                                    "      }\n",
                                    "    },\n",
                                    "    {\n",
                                    "      \"finish_reason\": null,\n",
                                    "      \"index\": 6,\n",
                                    "      \"message\": {\n",
                                    "        \"content\": \"\\n\\nAnswer: $88 \\\\div 4 \\\\div 2$ is equal to $\\\\frac{88}{4}\\\\div 2=22\\\\div 2=\\\\boxed{11}$.\",\n",
                                    "        \"role\": \"assistant\"\n",
                                    "      }\n",
                                    "    },\n",
                                    "    {\n",
                                    "      \"finish_reason\": \"stop\",\n",
                                    "      \"index\": 7,\n",
                                    "      \"message\": {\n",
                                    "        \"content\": \"\\n\\nAnswer: $88 \\\\div 4 \\\\div 2$ is equal to $(88 \\\\div 4) \\\\div 2 = 22 \\\\div 2 = \\\\boxed{11}$.\",\n",
                                    "        \"role\": \"assistant\"\n",
                                    "      }\n",
                                    "    }\n",
                                    "  ],\n",
                                    "  \"created\": 1678835765,\n",
                                    "  \"id\": \"chatcmpl-6u8A9hG0T4nKqfHlCDErht73LRYpe\",\n",
                                    "  \"model\": \"gpt-3.5-turbo-0301\",\n",
                                    "  \"object\": \"chat.completion\",\n",
                                    "  \"usage\": {\n",
                                    "    \"completion_tokens\": 340,\n",
                                    "    \"prompt_tokens\": 109,\n",
                                    "    \"total_tokens\": 449\n",
                                    "  }\n",
                                    "}\n",
                                    "{'expected_success': 1.0, 'success': True}\n"
                              ]
                        }
                  ],
                  "source": [
                        "responses = oai.ChatCompletion.create(context=tune_data[1], **config)\n",
                        "print(responses)\n",
                        "print(success_metrics([response[\"message\"][\"content\"].rstrip() for response in responses[\"choices\"]], **tune_data[1]))\n"
                  ]
            },
            {
                  "attachments": {},
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Evaluate the success rate on the test data\n",
                        "\n",
                        "You can use flaml's `oai.ChatCompletion.eval` to evaluate the performance of an entire dataset with the tuned config. To do that you need to set `oai.ChatCompletion.data` to the data to evaluate. The following code will take a while to evaluate all the 438 test data instances. Compared to the baseline success rate (45%) on the [HELM benchmark](https://crfm.stanford.edu/helm/v0.2.1/?group=math_chain_of_thought), the tuned config has a success rate of 81%. It can be further improved if the inference budget and optimization budget are further increased. Below is test example on part of the subsampled test data."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 15,
                  "metadata": {
                        "execution": {
                              "iopub.execute_input": "2023-02-13T23:41:56.042764Z",
                              "iopub.status.busy": "2023-02-13T23:41:56.042086Z",
                              "iopub.status.idle": "2023-02-13T23:53:05.597643Z",
                              "shell.execute_reply": "2023-02-13T23:53:05.596603Z"
                        }
                  },
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "{'expected_success': 0.7456149935722352, 'success': 0.8}\n"
                              ]
                        }
                  ],
                  "source": [
                        "\n",
                        "result = oai.ChatCompletion.test(subsample_test_data, config)\n",
                        "print(result)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Use the following code to get results on all the test data."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "result = oai.ChatCompletion.test(test_data, config)\n",
                        "print(result)"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "Python 3",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.9.16"
            },
            "vscode": {
                  "interpreter": {
                        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
                  }
            },
            "widgets": {
                  "application/vnd.jupyter.widget-state+json": {
                        "state": {
                              "2d910cfd2d2a4fc49fc30fbbdc5576a7": {
                                    "model_module": "@jupyter-widgets/base",
                                    "model_module_version": "2.0.0",
                                    "model_name": "LayoutModel",
                                    "state": {
                                          "_model_module": "@jupyter-widgets/base",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "LayoutModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/base",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "LayoutView",
                                          "align_content": null,
                                          "align_items": null,
                                          "align_self": null,
                                          "border_bottom": null,
                                          "border_left": null,
                                          "border_right": null,
                                          "border_top": null,
                                          "bottom": null,
                                          "display": null,
                                          "flex": null,
                                          "flex_flow": null,
                                          "grid_area": null,
                                          "grid_auto_columns": null,
                                          "grid_auto_flow": null,
                                          "grid_auto_rows": null,
                                          "grid_column": null,
                                          "grid_gap": null,
                                          "grid_row": null,
                                          "grid_template_areas": null,
                                          "grid_template_columns": null,
                                          "grid_template_rows": null,
                                          "height": null,
                                          "justify_content": null,
                                          "justify_items": null,
                                          "left": null,
                                          "margin": null,
                                          "max_height": null,
                                          "max_width": null,
                                          "min_height": null,
                                          "min_width": null,
                                          "object_fit": null,
                                          "object_position": null,
                                          "order": null,
                                          "overflow": null,
                                          "padding": null,
                                          "right": null,
                                          "top": null,
                                          "visibility": null,
                                          "width": null
                                    }
                              },
                              "454146d0f7224f038689031002906e6f": {
                                    "model_module": "@jupyter-widgets/controls",
                                    "model_module_version": "2.0.0",
                                    "model_name": "HBoxModel",
                                    "state": {
                                          "_dom_classes": [],
                                          "_model_module": "@jupyter-widgets/controls",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "HBoxModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/controls",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "HBoxView",
                                          "box_style": "",
                                          "children": [
                                                "IPY_MODEL_e4ae2b6f5a974fd4bafb6abb9d12ff26",
                                                "IPY_MODEL_577e1e3cc4db4942b0883577b3b52755",
                                                "IPY_MODEL_b40bdfb1ac1d4cffb7cefcb870c64d45"
                                          ],
                                          "layout": "IPY_MODEL_dc83c7bff2f241309537a8119dfc7555",
                                          "tabbable": null,
                                          "tooltip": null
                                    }
                              },
                              "577e1e3cc4db4942b0883577b3b52755": {
                                    "model_module": "@jupyter-widgets/controls",
                                    "model_module_version": "2.0.0",
                                    "model_name": "FloatProgressModel",
                                    "state": {
                                          "_dom_classes": [],
                                          "_model_module": "@jupyter-widgets/controls",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "FloatProgressModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/controls",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "ProgressView",
                                          "bar_style": "success",
                                          "description": "",
                                          "description_allow_html": false,
                                          "layout": "IPY_MODEL_2d910cfd2d2a4fc49fc30fbbdc5576a7",
                                          "max": 1,
                                          "min": 0,
                                          "orientation": "horizontal",
                                          "style": "IPY_MODEL_74a6ba0c3cbc4051be0a83e152fe1e62",
                                          "tabbable": null,
                                          "tooltip": null,
                                          "value": 1
                                    }
                              },
                              "6086462a12d54bafa59d3c4566f06cb2": {
                                    "model_module": "@jupyter-widgets/base",
                                    "model_module_version": "2.0.0",
                                    "model_name": "LayoutModel",
                                    "state": {
                                          "_model_module": "@jupyter-widgets/base",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "LayoutModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/base",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "LayoutView",
                                          "align_content": null,
                                          "align_items": null,
                                          "align_self": null,
                                          "border_bottom": null,
                                          "border_left": null,
                                          "border_right": null,
                                          "border_top": null,
                                          "bottom": null,
                                          "display": null,
                                          "flex": null,
                                          "flex_flow": null,
                                          "grid_area": null,
                                          "grid_auto_columns": null,
                                          "grid_auto_flow": null,
                                          "grid_auto_rows": null,
                                          "grid_column": null,
                                          "grid_gap": null,
                                          "grid_row": null,
                                          "grid_template_areas": null,
                                          "grid_template_columns": null,
                                          "grid_template_rows": null,
                                          "height": null,
                                          "justify_content": null,
                                          "justify_items": null,
                                          "left": null,
                                          "margin": null,
                                          "max_height": null,
                                          "max_width": null,
                                          "min_height": null,
                                          "min_width": null,
                                          "object_fit": null,
                                          "object_position": null,
                                          "order": null,
                                          "overflow": null,
                                          "padding": null,
                                          "right": null,
                                          "top": null,
                                          "visibility": null,
                                          "width": null
                                    }
                              },
                              "74a6ba0c3cbc4051be0a83e152fe1e62": {
                                    "model_module": "@jupyter-widgets/controls",
                                    "model_module_version": "2.0.0",
                                    "model_name": "ProgressStyleModel",
                                    "state": {
                                          "_model_module": "@jupyter-widgets/controls",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "ProgressStyleModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/base",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "StyleView",
                                          "bar_color": null,
                                          "description_width": ""
                                    }
                              },
                              "7d3f3d9e15894d05a4d188ff4f466554": {
                                    "model_module": "@jupyter-widgets/controls",
                                    "model_module_version": "2.0.0",
                                    "model_name": "HTMLStyleModel",
                                    "state": {
                                          "_model_module": "@jupyter-widgets/controls",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "HTMLStyleModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/base",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "StyleView",
                                          "background": null,
                                          "description_width": "",
                                          "font_size": null,
                                          "text_color": null
                                    }
                              },
                              "b40bdfb1ac1d4cffb7cefcb870c64d45": {
                                    "model_module": "@jupyter-widgets/controls",
                                    "model_module_version": "2.0.0",
                                    "model_name": "HTMLModel",
                                    "state": {
                                          "_dom_classes": [],
                                          "_model_module": "@jupyter-widgets/controls",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "HTMLModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/controls",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "HTMLView",
                                          "description": "",
                                          "description_allow_html": false,
                                          "layout": "IPY_MODEL_f1355871cc6f4dd4b50d9df5af20e5c8",
                                          "placeholder": "",
                                          "style": "IPY_MODEL_ca245376fd9f4354af6b2befe4af4466",
                                          "tabbable": null,
                                          "tooltip": null,
                                          "value": " 1/1 [00:00&lt;00:00, 44.69it/s]"
                                    }
                              },
                              "ca245376fd9f4354af6b2befe4af4466": {
                                    "model_module": "@jupyter-widgets/controls",
                                    "model_module_version": "2.0.0",
                                    "model_name": "HTMLStyleModel",
                                    "state": {
                                          "_model_module": "@jupyter-widgets/controls",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "HTMLStyleModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/base",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "StyleView",
                                          "background": null,
                                          "description_width": "",
                                          "font_size": null,
                                          "text_color": null
                                    }
                              },
                              "dc83c7bff2f241309537a8119dfc7555": {
                                    "model_module": "@jupyter-widgets/base",
                                    "model_module_version": "2.0.0",
                                    "model_name": "LayoutModel",
                                    "state": {
                                          "_model_module": "@jupyter-widgets/base",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "LayoutModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/base",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "LayoutView",
                                          "align_content": null,
                                          "align_items": null,
                                          "align_self": null,
                                          "border_bottom": null,
                                          "border_left": null,
                                          "border_right": null,
                                          "border_top": null,
                                          "bottom": null,
                                          "display": null,
                                          "flex": null,
                                          "flex_flow": null,
                                          "grid_area": null,
                                          "grid_auto_columns": null,
                                          "grid_auto_flow": null,
                                          "grid_auto_rows": null,
                                          "grid_column": null,
                                          "grid_gap": null,
                                          "grid_row": null,
                                          "grid_template_areas": null,
                                          "grid_template_columns": null,
                                          "grid_template_rows": null,
                                          "height": null,
                                          "justify_content": null,
                                          "justify_items": null,
                                          "left": null,
                                          "margin": null,
                                          "max_height": null,
                                          "max_width": null,
                                          "min_height": null,
                                          "min_width": null,
                                          "object_fit": null,
                                          "object_position": null,
                                          "order": null,
                                          "overflow": null,
                                          "padding": null,
                                          "right": null,
                                          "top": null,
                                          "visibility": null,
                                          "width": null
                                    }
                              },
                              "e4ae2b6f5a974fd4bafb6abb9d12ff26": {
                                    "model_module": "@jupyter-widgets/controls",
                                    "model_module_version": "2.0.0",
                                    "model_name": "HTMLModel",
                                    "state": {
                                          "_dom_classes": [],
                                          "_model_module": "@jupyter-widgets/controls",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "HTMLModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/controls",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "HTMLView",
                                          "description": "",
                                          "description_allow_html": false,
                                          "layout": "IPY_MODEL_6086462a12d54bafa59d3c4566f06cb2",
                                          "placeholder": "",
                                          "style": "IPY_MODEL_7d3f3d9e15894d05a4d188ff4f466554",
                                          "tabbable": null,
                                          "tooltip": null,
                                          "value": "100%"
                                    }
                              },
                              "f1355871cc6f4dd4b50d9df5af20e5c8": {
                                    "model_module": "@jupyter-widgets/base",
                                    "model_module_version": "2.0.0",
                                    "model_name": "LayoutModel",
                                    "state": {
                                          "_model_module": "@jupyter-widgets/base",
                                          "_model_module_version": "2.0.0",
                                          "_model_name": "LayoutModel",
                                          "_view_count": null,
                                          "_view_module": "@jupyter-widgets/base",
                                          "_view_module_version": "2.0.0",
                                          "_view_name": "LayoutView",
                                          "align_content": null,
                                          "align_items": null,
                                          "align_self": null,
                                          "border_bottom": null,
                                          "border_left": null,
                                          "border_right": null,
                                          "border_top": null,
                                          "bottom": null,
                                          "display": null,
                                          "flex": null,
                                          "flex_flow": null,
                                          "grid_area": null,
                                          "grid_auto_columns": null,
                                          "grid_auto_flow": null,
                                          "grid_auto_rows": null,
                                          "grid_column": null,
                                          "grid_gap": null,
                                          "grid_row": null,
                                          "grid_template_areas": null,
                                          "grid_template_columns": null,
                                          "grid_template_rows": null,
                                          "height": null,
                                          "justify_content": null,
                                          "justify_items": null,
                                          "left": null,
                                          "margin": null,
                                          "max_height": null,
                                          "max_width": null,
                                          "min_height": null,
                                          "min_width": null,
                                          "object_fit": null,
                                          "object_position": null,
                                          "order": null,
                                          "overflow": null,
                                          "padding": null,
                                          "right": null,
                                          "top": null,
                                          "visibility": null,
                                          "width": null
                                    }
                              }
                        },
                        "version_major": 2,
                        "version_minor": 0
                  }
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
