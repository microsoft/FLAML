{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Use FLAML to Tune ChatGPT\n",
    "\n",
    "In this notebook, we tune OpenAI ChatGPT model for math problem solving. We use [the MATH benchmark](https://crfm.stanford.edu/helm/latest/?group=math_chain_of_thought) for measuring mathematical problem solving on competition math problems with chain-of-thoughts style reasoning. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "FLAML requires `Python>=3.7`. To run this notebook example, please install flaml with the [openai] option:\n",
    "```bash\n",
    "pip install flaml[openai]==1.2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
     "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
     "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
     "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install flaml[openai]==1.2.0 datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your OpenAI key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.324240Z",
     "iopub.status.busy": "2023-02-13T23:40:52.323783Z",
     "iopub.status.idle": "2023-02-13T23:40:52.330570Z",
     "shell.execute_reply": "2023-02-13T23:40:52.329750Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"<your OpenAI API key here>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following to use Azure OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.333547Z",
     "iopub.status.busy": "2023-02-13T23:40:52.333249Z",
     "iopub.status.idle": "2023-02-13T23:40:52.336508Z",
     "shell.execute_reply": "2023-02-13T23:40:52.335858Z"
    }
   },
   "outputs": [],
   "source": [
    "# openai.api_type = \"azure\"\n",
    "# openai.api_base = \"https://<your_endpoint>.openai.azure.com/\"\n",
    "# openai.api_version = \"2022-12-01\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "First, we load the competition_math dataset. The dataset contains 457 \"Level 1\" examples. We use a random sample of 20 examples for tuning the generation hyperparameters and the remaining for evaluation. We use one demonstration example in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.339977Z",
     "iopub.status.busy": "2023-02-13T23:40:52.339556Z",
     "iopub.status.idle": "2023-02-13T23:40:54.603349Z",
     "shell.execute_reply": "2023-02-13T23:40:54.602630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Found cached dataset competition_math (/home/vscode/.cache/huggingface/datasets/competition_math/default/1.0.0/2a2a2995c2847186883ecd64f69be7d602b8a6f6b51950624d4dc2263f93333b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d2c0e53b10466fb3c476f46ce2309e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/vscode/.cache/huggingface/datasets/competition_math/default/1.0.0/2a2a2995c2847186883ecd64f69be7d602b8a6f6b51950624d4dc2263f93333b/cache-f1cfe8228271b121.arrow\n",
      "Loading cached shuffled indices for dataset at /home/vscode/.cache/huggingface/datasets/competition_math/default/1.0.0/2a2a2995c2847186883ecd64f69be7d602b8a6f6b51950624d4dc2263f93333b/cache-d155a2d38c23bd53.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tokens in tuning data's canonical solutions 128\n",
      "20 437\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "seed = 41\n",
    "data = datasets.load_dataset(\"competition_math\")\n",
    "train_data = data[\"train\"].shuffle(seed=seed)\n",
    "test_data = data[\"test\"].shuffle(seed=seed)\n",
    "n_tune_data = 20\n",
    "tune_data = [\n",
    "    {\n",
    "        \"problem\": train_data[x][\"problem\"],\n",
    "        \"solution\": train_data[x][\"solution\"],\n",
    "    }\n",
    "    for x in range(len(train_data)) if train_data[x][\"level\"] == \"Level 1\"\n",
    "][:n_tune_data]\n",
    "test_data = [\n",
    "    {\n",
    "        \"problem\": test_data[x][\"problem\"],\n",
    "        \"solution\": test_data[x][\"solution\"],\n",
    "    }\n",
    "    for x in range(len(test_data)) if test_data[x][\"level\"] == \"Level 1\"\n",
    "]\n",
    "input_field = \"problem\"\n",
    "output_fields = [\"solution\"]\n",
    "print(\"max tokens in tuning data's canonical solutions\", max([len(x[\"solution\"].split()) for x in tune_data]))\n",
    "print(len(tune_data), len(test_data))\n",
    "# prompt template\n",
    "prompts = [\"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\\\boxed{{}}.\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check a tuning example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.607152Z",
     "iopub.status.busy": "2023-02-13T23:40:54.606441Z",
     "iopub.status.idle": "2023-02-13T23:40:54.610504Z",
     "shell.execute_reply": "2023-02-13T23:40:54.609759Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find $\\log_{10} 40 +\\log_{10} 25$.\n"
     ]
    }
   ],
   "source": [
    "print(tune_data[1][\"problem\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one example of the canonical solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.613590Z",
     "iopub.status.busy": "2023-02-13T23:40:54.613168Z",
     "iopub.status.idle": "2023-02-13T23:40:54.616873Z",
     "shell.execute_reply": "2023-02-13T23:40:54.616193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using $\\log x+\\log y=\\log xy,$ we get that $\\log_{10} 40+\\log_{10} 25=\\log_{10}(40\\cdot 25)=\\log 1000.$ That means we want $x$ where $10^x=1000,$ which means $x=3.$ Therefore, $\\log_{10} 40+\\log_{10} 25=\\boxed{3}.$\n"
     ]
    }
   ],
   "source": [
    "print(tune_data[1][\"solution\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Success Metric\n",
    "\n",
    "Before we start tuning, we need to define the success metric we want to opotimize. For each math task, we use voting to select a response with the most common answers out of all the generated responses. If it has an equivalent answer to the canonical solution, we consider the task as successfully solved. Then we can optimize the mean success rate of a collection of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.626998Z",
     "iopub.status.busy": "2023-02-13T23:40:54.626593Z",
     "iopub.status.idle": "2023-02-13T23:40:54.631383Z",
     "shell.execute_reply": "2023-02-13T23:40:54.630770Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def remove_boxed(string: str) -> Optional[str]:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Extract the text within a \\\\boxed{...} environment.\n",
    "    Example:\n",
    "    >>> remove_boxed(\\\\boxed{\\\\frac{2}{3}})\n",
    "    \\\\frac{2}{3}\n",
    "    \"\"\"\n",
    "    left = \"\\\\boxed{\"\n",
    "    try:\n",
    "        assert string[: len(left)] == left\n",
    "        assert string[-1] == \"}\"\n",
    "        return string[len(left) : -1]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def last_boxed_only_string(string: str) -> Optional[str]:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Extract the last \\\\boxed{...} or \\\\fbox{...} element from a string.\n",
    "    \"\"\"\n",
    "    idx = string.rfind(\"\\\\boxed\")\n",
    "    if idx < 0:\n",
    "        idx = string.rfind(\"\\\\fbox\")\n",
    "        if idx < 0:\n",
    "            return None\n",
    "\n",
    "    i = idx\n",
    "    right_brace_idx = None\n",
    "    num_left_braces_open = 0\n",
    "    while i < len(string):\n",
    "        if string[i] == \"{\":\n",
    "            num_left_braces_open += 1\n",
    "        if string[i] == \"}\":\n",
    "            num_left_braces_open -= 1\n",
    "            if num_left_braces_open == 0:\n",
    "                right_brace_idx = i\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    if right_brace_idx is None:\n",
    "        retval = None\n",
    "    else:\n",
    "        retval = string[idx : right_brace_idx + 1]\n",
    "\n",
    "    return retval\n",
    "\n",
    "\n",
    "def _fix_fracs(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Reformat fractions.\n",
    "    Examples:\n",
    "    >>> _fix_fracs(\"\\\\frac1b\")\n",
    "    \\frac{1}{b}\n",
    "    >>> _fix_fracs(\"\\\\frac12\")\n",
    "    \\frac{1}{2}\n",
    "    >>> _fix_fracs(\"\\\\frac1{72}\")\n",
    "    \\frac{1}{72}\n",
    "    \"\"\"\n",
    "    substrs = string.split(\"\\\\frac\")\n",
    "    new_str = substrs[0]\n",
    "    if len(substrs) > 1:\n",
    "        substrs = substrs[1:]\n",
    "        for substr in substrs:\n",
    "            new_str += \"\\\\frac\"\n",
    "            if substr[0] == \"{\":\n",
    "                new_str += substr\n",
    "            else:\n",
    "                try:\n",
    "                    assert len(substr) >= 2\n",
    "                except Exception:\n",
    "                    return string\n",
    "                a = substr[0]\n",
    "                b = substr[1]\n",
    "                if b != \"{\":\n",
    "                    if len(substr) > 2:\n",
    "                        post_substr = substr[2:]\n",
    "                        new_str += \"{\" + a + \"}{\" + b + \"}\" + post_substr\n",
    "                    else:\n",
    "                        new_str += \"{\" + a + \"}{\" + b + \"}\"\n",
    "                else:\n",
    "                    if len(substr) > 2:\n",
    "                        post_substr = substr[2:]\n",
    "                        new_str += \"{\" + a + \"}\" + b + post_substr\n",
    "                    else:\n",
    "                        new_str += \"{\" + a + \"}\" + b\n",
    "    string = new_str\n",
    "    return string\n",
    "\n",
    "\n",
    "def _fix_a_slash_b(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Reformat fractions formatted as a/b to \\\\frac{a}{b}.\n",
    "    Example:\n",
    "    >>> _fix_a_slash_b(\"2/3\")\n",
    "    \\frac{2}{3}\n",
    "    \"\"\"\n",
    "    if len(string.split(\"/\")) != 2:\n",
    "        return string\n",
    "    a_str = string.split(\"/\")[0]\n",
    "    b_str = string.split(\"/\")[1]\n",
    "    try:\n",
    "        a = int(a_str)\n",
    "        b = int(b_str)\n",
    "        assert string == \"{}/{}\".format(a, b)\n",
    "        new_string = \"\\\\frac{\" + str(a) + \"}{\" + str(b) + \"}\"\n",
    "        return new_string\n",
    "    except Exception:\n",
    "        return string\n",
    "\n",
    "\n",
    "def _remove_right_units(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Remove units (on the right).\n",
    "    \"\\\\text{ \" only ever occurs (at least in the val set) when describing units.\n",
    "    \"\"\"\n",
    "    if \"\\\\text{ \" in string:\n",
    "        splits = string.split(\"\\\\text{ \")\n",
    "        assert len(splits) == 2\n",
    "        return splits[0]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "\n",
    "def _fix_sqrt(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Reformat square roots.\n",
    "    Example:\n",
    "    >>> _fix_sqrt(\"\\\\sqrt3\")\n",
    "    \\sqrt{3}\n",
    "    \"\"\"\n",
    "    if \"\\\\sqrt\" not in string:\n",
    "        return string\n",
    "    splits = string.split(\"\\\\sqrt\")\n",
    "    new_string = splits[0]\n",
    "    for split in splits[1:]:\n",
    "        if split[0] != \"{\":\n",
    "            a = split[0]\n",
    "            new_substr = \"\\\\sqrt{\" + a + \"}\" + split[1:]\n",
    "        else:\n",
    "            new_substr = \"\\\\sqrt\" + split\n",
    "        new_string += new_substr\n",
    "    return new_string\n",
    "\n",
    "\n",
    "def _strip_string(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Apply the reformatting helper functions above.\n",
    "    \"\"\"\n",
    "    # linebreaks\n",
    "    string = string.replace(\"\\n\", \"\")\n",
    "    # print(string)\n",
    "\n",
    "    # remove inverse spaces\n",
    "    string = string.replace(\"\\\\!\", \"\")\n",
    "    # print(string)\n",
    "\n",
    "    # replace \\\\ with \\\n",
    "    string = string.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    # print(string)\n",
    "\n",
    "    # replace tfrac and dfrac with frac\n",
    "    string = string.replace(\"tfrac\", \"frac\")\n",
    "    string = string.replace(\"dfrac\", \"frac\")\n",
    "    # print(string)\n",
    "\n",
    "    # remove \\left and \\right\n",
    "    string = string.replace(\"\\\\left\", \"\")\n",
    "    string = string.replace(\"\\\\right\", \"\")\n",
    "    # print(string)\n",
    "\n",
    "    # Remove circ (degrees)\n",
    "    string = string.replace(\"^{\\\\circ}\", \"\")\n",
    "    string = string.replace(\"^\\\\circ\", \"\")\n",
    "\n",
    "    # remove dollar signs\n",
    "    string = string.replace(\"\\\\$\", \"\")\n",
    "\n",
    "    # remove units (on the right)\n",
    "    string = _remove_right_units(string)\n",
    "\n",
    "    # remove percentage\n",
    "    string = string.replace(\"\\\\%\", \"\")\n",
    "    string = string.replace(\"\\%\", \"\")\n",
    "\n",
    "    # \" 0.\" equivalent to \" .\" and \"{0.\" equivalent to \"{.\" Alternatively, add \"0\" if \".\" is the start of the string\n",
    "    string = string.replace(\" .\", \" 0.\")\n",
    "    string = string.replace(\"{.\", \"{0.\")\n",
    "    # if empty, return empty string\n",
    "    if len(string) == 0:\n",
    "        return string\n",
    "    if string[0] == \".\":\n",
    "        string = \"0\" + string\n",
    "\n",
    "    # to consider: get rid of e.g. \"k = \" or \"q = \" at beginning\n",
    "    if len(string.split(\"=\")) == 2:\n",
    "        if len(string.split(\"=\")[0]) <= 2:\n",
    "            string = string.split(\"=\")[1]\n",
    "\n",
    "    # fix sqrt3 --> sqrt{3}\n",
    "    string = _fix_sqrt(string)\n",
    "\n",
    "    # remove spaces\n",
    "    string = string.replace(\" \", \"\")\n",
    "\n",
    "    # \\frac1b or \\frac12 --> \\frac{1}{b} and \\frac{1}{2}, etc.\n",
    "    # Even works with \\frac1{72} (but not \\frac{72}1).\n",
    "    # Also does a/b --> \\\\frac{a}{b}\n",
    "    string = _fix_fracs(string)\n",
    "\n",
    "    # manually change 0.5 --> \\frac{1}{2}\n",
    "    if string == \"0.5\":\n",
    "        string = \"\\\\frac{1}{2}\"\n",
    "\n",
    "    # NOTE: X/Y changed to \\frac{X}{Y} in dataset, but in simple cases fix in case the model output is X/Y\n",
    "    string = _fix_a_slash_b(string)\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "def get_answer(solution: Optional[str]) -> Optional[str]:\n",
    "    if solution is None:\n",
    "        return None\n",
    "    last_boxed = last_boxed_only_string(solution)\n",
    "    if last_boxed is None:\n",
    "        return None\n",
    "    answer = remove_boxed(last_boxed)\n",
    "    if answer is None:\n",
    "        return None\n",
    "    return answer\n",
    "\n",
    "\n",
    "def is_equiv(str1: Optional[str], str2: Optional[str]) -> float:\n",
    "    \"\"\"Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in\n",
    "    - units\n",
    "    - fractions\n",
    "    - square roots\n",
    "    - superfluous LaTeX.\n",
    "    Source: https://github.com/hendrycks/math\n",
    "    \"\"\"\n",
    "    if str1 is None and str2 is None:\n",
    "        print(\"WARNING: Both None\")\n",
    "        return 1.0\n",
    "    if str1 is None or str2 is None:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        ss1 = _strip_string(str1)\n",
    "        ss2 = _strip_string(str2)\n",
    "        return float(ss1 == ss2)\n",
    "    except Exception:\n",
    "        return float(str1 == str2)\n",
    "\n",
    "\n",
    "def is_equiv_chain_of_thought(str1: str, str2: str) -> float:\n",
    "    \"\"\"Strips the solution first before calling `is_equiv`.\"\"\"\n",
    "    ans1 = get_answer(str1)\n",
    "    ans2 = get_answer(str2)\n",
    "\n",
    "    return is_equiv(ans1, ans2)\n",
    "\n",
    "\n",
    "def success_metrics(responses, solution, **args):\n",
    "    \"\"\"Check if each response is correct.\n",
    "    \n",
    "    Args:\n",
    "        responses (list): The list of responses.\n",
    "        solution (str): The canonical solution.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The success metrics.\n",
    "    \"\"\"\n",
    "    success_list = []\n",
    "    n = len(responses)\n",
    "    for i in range(n):\n",
    "        response = responses[i]\n",
    "        succeed = is_equiv_chain_of_thought(response, solution)\n",
    "        success_list.append(succeed)\n",
    "    # voting\n",
    "    answers = {}\n",
    "    for i in range(n):\n",
    "        equiv = i\n",
    "        if get_answer(responses[i]) is None:\n",
    "            # ignore None answers\n",
    "            continue\n",
    "        for j in answers:\n",
    "            if is_equiv_chain_of_thought(responses[i], responses[j]):\n",
    "                equiv = j\n",
    "                break\n",
    "        if equiv in answers:\n",
    "            answers[equiv] += 1\n",
    "        else:\n",
    "            answers[equiv] = 1\n",
    "    # find the answer with highest votes in answers\n",
    "    answer = max(answers.items(), key=lambda x: x[1], default=(0, 0))[0]\n",
    "    # check if the answer is correct\n",
    "    success_vote = is_equiv_chain_of_thought(responses[answer], solution)\n",
    "    return {\n",
    "        \"expected_success\": 1 - pow(1 - sum(success_list) / n, n),\n",
    "        \"success\": any(s for s in success_list),\n",
    "        \"success_vote\": success_vote,\n",
    "        \"voted_answer\": responses[answer],\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use the tuning data to find a good configuration\n",
    "\n",
    "### Import the oai and tune subpackages from flaml.\n",
    "\n",
    "FLAML has provided an API for hyperparameter optimization of OpenAI ChatGPT models: `oai.ChatCompletion.tune` and to make a request with the tuned config: `oai.ChatCompletion.create`. First, we import oai from flaml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.634335Z",
     "iopub.status.busy": "2023-02-13T23:40:54.633929Z",
     "iopub.status.idle": "2023-02-13T23:40:56.105700Z",
     "shell.execute_reply": "2023-02-13T23:40:56.105085Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from flaml import oai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For (local) reproducibility and cost efficiency, we cache responses from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:56.109177Z",
     "iopub.status.busy": "2023-02-13T23:40:56.108624Z",
     "iopub.status.idle": "2023-02-13T23:40:56.112651Z",
     "shell.execute_reply": "2023-02-13T23:40:56.112076Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "oai.ChatCompletion.set_cache(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a disk cache in \".cache/{seed}\". You can change `cache_path` in `set_cache()`. The cache for different seeds are stored separately.\n",
    "\n",
    "### Perform tuning\n",
    "\n",
    "The tuning will take a while to finish, depending on the optimization budget. The tuning will be performed under the specified optimization budgets.\n",
    "\n",
    "* `inference_budget` is the target average inference budget per instance in the benchmark. For example, 0.004 means the target inference budget is 0.004 dollars, which translates to 2000 tokens (input + output combined) if the gpt-3.5-turbo model is used.\n",
    "* `optimization_budget` is the total budget allowed to perform the tuning. For example, 1 means 1 dollars are allowed in total, which translates to 500K tokens for the gpt-3.5-turbo model.\n",
    "* `num_sumples` is the number of different hyperparameter configurations which is allowed to try. The tuning will stop after either num_samples trials or after optimization_budget dollars spent, whichever happens first. -1 means no hard restriction in the number of trials and the actual number is decided by `optimization_budget`.\n",
    "\n",
    "Users can specify tuning data, optimization metric, optimization mode, evaluation function, search spaces etc.. The default search space is:\n",
    "\n",
    "```python\n",
    "price1K = {\n",
    "    \"gpt-3.5-turbo\": 0.002,\n",
    "}\n",
    "\n",
    "default_search_space = {\n",
    "    \"model\": tune.choice(list(price1K.keys())),\n",
    "    \"temperature_or_top_p\": tune.choice(\n",
    "        [\n",
    "            {\"temperature\": tune.uniform(0, 1)},\n",
    "            {\"top_p\": tune.uniform(0, 1)},\n",
    "        ]\n",
    "    ),\n",
    "    \"max_tokens\": tune.lograndint(50, 1000),\n",
    "    \"n\": tune.randint(1, 100),\n",
    "    \"prompt\": \"{prompt}\",\n",
    "}\n",
    "```\n",
    "\n",
    "The default search space can be overridden by users' input.\n",
    "For example, the following code specifies a fixed prompt template and a list of stop sequences. For hyperparameters which don't appear in users' input, the default search space will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:56.115383Z",
     "iopub.status.busy": "2023-02-13T23:40:56.114975Z",
     "iopub.status.idle": "2023-02-13T23:41:55.045654Z",
     "shell.execute_reply": "2023-02-13T23:41:55.044973Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-21 19:36:38,547]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 1 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.36280922847807595}, 'max_tokens': 347, 'n': 10, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.78251724475, 'success': 0.8, 'success_vote': 0.75, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.061032, 'cost': 0.061032, 'inference_cost': 0.0029752, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.36280922847807595}, 'max_tokens': 347, 'n': 10, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.36280922847807595}, 'config/max_tokens': 347, 'config/n': 10, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.010601520538330078}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 2 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6336482349262754}, 'max_tokens': 470, 'n': 50, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.07672400000000001, 'cost': 0.015692, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6336482349262754}, 'max_tokens': 470, 'n': 50, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6336482349262754}, 'config/max_tokens': 470, 'config/n': 50, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.002635955810546875}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 3 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7605307121989587}, 'max_tokens': 82, 'n': 9, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.4082756583377293, 'success': 0.45, 'success_vote': 0.4, 'voted_answer': '\\n\\n\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.10755400000000005, 'cost': 0.03083, 'inference_cost': 0.0015415000000000001, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7605307121989587}, 'max_tokens': 82, 'n': 9, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.7605307121989587}, 'config/max_tokens': 82, 'config/n': 9, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.006795644760131836}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 4 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.003948266327914451}, 'max_tokens': 231, 'n': 81, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.12176400000000005, 'cost': 0.01421, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.003948266327914451}, 'max_tokens': 231, 'n': 81, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.003948266327914451}, 'config/max_tokens': 231, 'config/n': 81, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0020465850830078125}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 5 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.29187606817063316}, 'max_tokens': 781, 'n': 71, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.29187606817063316}, 'max_tokens': 781, 'n': 71, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.29187606817063316}, 'config/max_tokens': 781, 'config/n': 71, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0005726814270019531}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 6 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3733407600514692}, 'max_tokens': 375, 'n': 44, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3733407600514692}, 'max_tokens': 375, 'n': 44, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.3733407600514692}, 'config/max_tokens': 375, 'config/n': 44, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.00045418739318847656}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 7 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.5131382425543909}, 'max_tokens': 350, 'n': 60, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.5131382425543909}, 'max_tokens': 350, 'n': 60, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.5131382425543909}, 'config/max_tokens': 350, 'config/n': 60, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0004146099090576172}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 8 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9086488808086682}, 'max_tokens': 129, 'n': 9, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.6732483350925718, 'success': 0.7, 'success_vote': 0.7, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.16390400000000005, 'cost': 0.042140000000000004, 'inference_cost': 0.0021070000000000004, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9086488808086682}, 'max_tokens': 129, 'n': 9, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9086488808086682}, 'config/max_tokens': 129, 'config/n': 9, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.008260250091552734}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 9 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8286813263076767}, 'max_tokens': 57, 'n': 63, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.17320600000000005, 'cost': 0.009302000000000001, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8286813263076767}, 'max_tokens': 57, 'n': 63, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.8286813263076767}, 'config/max_tokens': 57, 'config/n': 63, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0021750926971435547}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 10 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.1989475396788123}, 'max_tokens': 650, 'n': 35, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.1879280000000001, 'cost': 0.014722, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.1989475396788123}, 'max_tokens': 650, 'n': 35, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.1989475396788123}, 'config/max_tokens': 650, 'config/n': 35, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.002772808074951172}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 11 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8839364795611863}, 'max_tokens': 132, 'n': 17, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.7870303934779195, 'success': 0.8, 'success_vote': 0.75, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\cdot \\\\|\\\\mathbf{v}\\\\| = \\\\boxed{12}.\\\\]', 'total_cost': 0.2664940000000002, 'cost': 0.078566, 'inference_cost': 0.0039283, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8839364795611863}, 'max_tokens': 132, 'n': 17, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.8839364795611863}, 'config/max_tokens': 132, 'config/n': 17, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.010728836059570312}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 12 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8211056578369285}, 'max_tokens': 78, 'n': 39, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.2791580000000002, 'cost': 0.012664, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8211056578369285}, 'max_tokens': 78, 'n': 39, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.8211056578369285}, 'config/max_tokens': 78, 'config/n': 39, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.002050161361694336}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 13 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.0422875090290305}, 'max_tokens': 144, 'n': 23, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.2961880000000002, 'cost': 0.01703, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.0422875090290305}, 'max_tokens': 144, 'n': 23, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.0422875090290305}, 'config/max_tokens': 144, 'config/n': 23, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.002070188522338867}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 14 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9952461897033992}, 'max_tokens': 140, 'n': 4, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.6181640625, 'success': 0.65, 'success_vote': 0.65, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\cdot \\\\|\\\\mathbf{v}\\\\| = \\\\boxed{12}.\\\\]', 'total_cost': 0.3173120000000002, 'cost': 0.021124, 'inference_cost': 0.0010562, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9952461897033992}, 'max_tokens': 140, 'n': 4, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9952461897033992}, 'config/max_tokens': 140, 'config/n': 4, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.005788326263427734}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 15 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5004937024436973}, 'max_tokens': 147, 'n': 18, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.38795800000000014, 'cost': 0.07064600000000001, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5004937024436973}, 'max_tokens': 147, 'n': 18, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.5004937024436973}, 'config/max_tokens': 147, 'config/n': 18, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.005265951156616211}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 16 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9987302802174121}, 'max_tokens': 98, 'n': 98, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9987302802174121}, 'max_tokens': 98, 'n': 98, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9987302802174121}, 'config/max_tokens': 98, 'config/n': 98, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.00043463706970214844}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 17 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.61550634943859}, 'max_tokens': 221, 'n': 1, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.65, 'success': 0.65, 'success_vote': 0.65, 'voted_answer': '\\n\\nWe have that $\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\cdot \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.$', 'total_cost': 0.39580000000000015, 'cost': 0.007842000000000002, 'inference_cost': 0.0003921, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.61550634943859}, 'max_tokens': 221, 'n': 1, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.61550634943859}, 'config/max_tokens': 221, 'config/n': 1, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.004723787307739258}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 18 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9908983998468708}, 'max_tokens': 56, 'n': 24, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.2753267211940856, 'success': 0.35, 'success_vote': 0.3, 'voted_answer': '\\n\\nWe have that $\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.$', 'total_cost': 0.4516700000000002, 'cost': 0.055869999999999996, 'inference_cost': 0.0027935, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9908983998468708}, 'max_tokens': 56, 'n': 24, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9908983998468708}, 'config/max_tokens': 56, 'config/n': 24, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.009643316268920898}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 19 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.2551589672251001}, 'max_tokens': 204, 'n': 13, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.9081357158623893, 'success': 0.95, 'success_vote': 0.75, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.5226100000000004, 'cost': 0.07094, 'inference_cost': 0.0035470000000000002, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.2551589672251001}, 'max_tokens': 204, 'n': 13, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.2551589672251001}, 'config/max_tokens': 204, 'config/n': 13, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.010748147964477539}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 20 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.19145601353204816}, 'max_tokens': 244, 'n': 34, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.19145601353204816}, 'max_tokens': 244, 'n': 34, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.19145601353204816}, 'config/max_tokens': 244, 'config/n': 34, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.000518798828125}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 21 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.31480444425102294}, 'max_tokens': 447, 'n': 19, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.5603520000000005, 'cost': 0.037742, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.31480444425102294}, 'max_tokens': 447, 'n': 19, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.31480444425102294}, 'config/max_tokens': 447, 'config/n': 19, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.00341796875}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 22 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.010205444049939116}, 'max_tokens': 191, 'n': 49, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.010205444049939116}, 'max_tokens': 191, 'n': 49, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.010205444049939116}, 'config/max_tokens': 191, 'config/n': 49, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0005242824554443359}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 23 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.16458777684368253}, 'max_tokens': 302, 'n': 29, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.16458777684368253}, 'max_tokens': 302, 'n': 29, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.16458777684368253}, 'config/max_tokens': 302, 'config/n': 29, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.00042057037353515625}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 24 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6919974617368362}, 'max_tokens': 112, 'n': 12, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.7217133722247956, 'success': 0.8, 'success_vote': 0.7, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\cdot \\\\|\\\\mathbf{v}\\\\| = 3 \\\\|\\\\mathbf{v}\\\\| = 12.\\\\]Thus, $\\\\|-3 \\\\mathbf{v}\\\\| = \\\\boxed{12}.$', 'total_cost': 0.6114640000000007, 'cost': 0.051112, 'inference_cost': 0.0025556, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6919974617368362}, 'max_tokens': 112, 'n': 12, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6919974617368362}, 'config/max_tokens': 112, 'config/n': 12, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.008167743682861328}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 25 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6045678454619268}, 'max_tokens': 105, 'n': 10, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.683779070875, 'success': 0.75, 'success_vote': 0.75, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = 3 \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.6524120000000007, 'cost': 0.040948, 'inference_cost': 0.0020474, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6045678454619268}, 'max_tokens': 105, 'n': 10, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6045678454619268}, 'config/max_tokens': 105, 'config/n': 10, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.007577180862426758}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 26 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5470631684324488}, 'max_tokens': 174, 'n': 1, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.5, 'success': 0.5, 'success_vote': 0.5, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\cdot \\\\|\\\\mathbf{v}\\\\| = \\\\boxed{12}.\\\\]', 'total_cost': 0.6602420000000006, 'cost': 0.00783, 'inference_cost': 0.00039150000000000003, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5470631684324488}, 'max_tokens': 174, 'n': 1, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.5470631684324488}, 'config/max_tokens': 174, 'config/n': 1, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.004479646682739258}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 27 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3951626747864828}, 'max_tokens': 83, 'n': 17, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.4208131375275925, 'success': 0.5, 'success_vote': 0.5, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\cdot \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.7171840000000006, 'cost': 0.056942, 'inference_cost': 0.0028471, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3951626747864828}, 'max_tokens': 83, 'n': 17, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.3951626747864828}, 'config/max_tokens': 83, 'config/n': 17, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.008302450180053711}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 28 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.24508135321478802}, 'max_tokens': 183, 'n': 29, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.24508135321478802}, 'max_tokens': 183, 'n': 29, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.24508135321478802}, 'config/max_tokens': 183, 'config/n': 29, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.00041937828063964844}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 29 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9990495004030453}, 'max_tokens': 110, 'n': 12, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.6579559564282809, 'success': 0.7, 'success_vote': 0.65, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]Note that\\n\\\\begin{align*}\\n\\\\|-3 \\\\mathbf{v}\\\\| &= \\\\sqrt{(-3 \\\\mathbf{v}) \\\\cdot (-3 \\\\mathbf{v})} \\\\\\\\\\n&= \\\\sqrt{(3 \\\\mathbf{v}) \\\\cdot (3 \\\\mathbf{v})} \\\\\\\\', 'total_cost': 0.7683620000000005, 'cost': 0.051178, 'inference_cost': 0.0025589, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9990495004030453}, 'max_tokens': 110, 'n': 12, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.9990495004030453}, 'config/max_tokens': 110, 'config/n': 12, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.008163213729858398}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 30 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6430139114194635}, 'max_tokens': 50, 'n': 1, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.05, 'success': 0.05, 'success_vote': 0.05, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.7730940000000005, 'cost': 0.004732, 'inference_cost': 0.0002366, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6430139114194635}, 'max_tokens': 50, 'n': 1, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6430139114194635}, 'config/max_tokens': 50, 'config/n': 1, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.004233837127685547}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 31 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.42048118271017554}, 'max_tokens': 68, 'n': 7, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.1781281608853454, 'success': 0.2, 'success_vote': 0.2, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.7944900000000008, 'cost': 0.021396000000000002, 'inference_cost': 0.0010698, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.42048118271017554}, 'max_tokens': 68, 'n': 7, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.42048118271017554}, 'config/max_tokens': 68, 'config/n': 7, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.005765438079833984}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 32 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.0931331419671747}, 'max_tokens': 94, 'n': 13, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.3145673990845029, 'success': 0.35, 'success_vote': 0.3, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\cdot \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.8433020000000008, 'cost': 0.048812, 'inference_cost': 0.0024406, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.0931331419671747}, 'max_tokens': 94, 'n': 13, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.0931331419671747}, 'config/max_tokens': 94, 'config/n': 13, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.007876396179199219}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 33 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5282265073646044}, 'max_tokens': 265, 'n': 26, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5282265073646044}, 'max_tokens': 265, 'n': 26, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.5282265073646044}, 'config/max_tokens': 265, 'config/n': 26, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0004019737243652344}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 34 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7933148097588405}, 'max_tokens': 123, 'n': 9, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.6216901258157258, 'success': 0.65, 'success_vote': 0.6, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.8841780000000008, 'cost': 0.040875999999999996, 'inference_cost': 0.0020438, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7933148097588405}, 'max_tokens': 123, 'n': 9, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.7933148097588405}, 'config/max_tokens': 123, 'config/n': 9, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0072994232177734375}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 35 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9171274987516671}, 'max_tokens': 131, 'n': 6, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.6436042524005486, 'success': 0.7, 'success_vote': 0.7, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = 3 \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.9139340000000007, 'cost': 0.029756, 'inference_cost': 0.0014878, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9171274987516671}, 'max_tokens': 131, 'n': 6, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9171274987516671}, 'config/max_tokens': 131, 'config/n': 6, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.006303310394287109}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 36 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6801835235622993}, 'max_tokens': 167, 'n': 15, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'expected_success': 0.8782177535003072, 'success': 0.9, 'success_vote': 0.8, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = \\\\boxed{12}.\\\\]', 'total_cost': 0.9921080000000008, 'cost': 0.078174, 'inference_cost': 0.0039087, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6801835235622993}, 'max_tokens': 167, 'n': 15, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6801835235622993}, 'config/max_tokens': 167, 'config/n': 15, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.010643959045410156}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {811} INFO - trial 37 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9998765149838305}, 'max_tokens': 163, 'n': 20, 'prompt': 0, 'stop': 0}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {215} INFO - result: {'success_vote': 0, 'total_cost': 1.0029080000000008, 'cost': 0.0108, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9998765149838305}, 'max_tokens': 163, 'n': 20, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.9998765149838305}, 'config/max_tokens': 163, 'config/n': 20, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.001580953598022461}\n",
      "[flaml.tune.tune: 03-21 19:36:38] {834} WARNING - fail to sample a trial for 100 times in a row, stopping.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "config, analysis = oai.ChatCompletion.tune(\n",
    "    data=tune_data,  # the data for tuning\n",
    "    metric=\"success_vote\",  # the metric to optimize\n",
    "    mode=\"max\",  # the optimization mode\n",
    "    eval_func=success_metrics,  # the evaluation function to return the success metrics\n",
    "    # log_file_name=\"logs/math.log\",  # the log file name\n",
    "    inference_budget=0.004,  # the inference budget (dollar)\n",
    "    optimization_budget=1,  # the optimization budget (dollar)\n",
    "    # num_samples can further limit the number of trials for different hyperparameter configurations;\n",
    "    # -1 means decided by the optimization budget only\n",
    "    num_samples=-1,\n",
    "    # model=\"chatgpt-35-turbo-0301\",  # uncomment if using Azure OpenAI\n",
    "    prompt=prompts,  # the prompt templates to choose from\n",
    "    stop=\"###\",  # the stop sequence\n",
    "    logging_level=logging.INFO,  # the logging level\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output tuning results\n",
    "\n",
    "After the tuning, we can print out the config and the result found by FLAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:41:55.049204Z",
     "iopub.status.busy": "2023-02-13T23:41:55.048871Z",
     "iopub.status.idle": "2023-02-13T23:41:55.053284Z",
     "shell.execute_reply": "2023-02-13T23:41:55.052574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized config {'model': 'gpt-3.5-turbo', 'max_tokens': 167, 'n': 15, 'prompt': '{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\\\boxed{{}}.', 'stop': '###', 'temperature': 0.6801835235622993}\n",
      "best result on tuning data {'expected_success': 0.8782177535003072, 'success': 0.9, 'success_vote': 0.8, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = \\\\boxed{12}.\\\\]', 'total_cost': 0.9921080000000008, 'cost': 0.078174, 'inference_cost': 0.0039087, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6801835235622993}, 'max_tokens': 167, 'n': 15, 'prompt': 0, 'stop': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6801835235622993}, 'config/max_tokens': 167, 'config/n': 15, 'config/prompt': 0, 'config/stop': 0, 'experiment_tag': 'exp', 'time_total_s': 0.010643959045410156}\n"
     ]
    }
   ],
   "source": [
    "print(\"optimized config\", config)\n",
    "print(\"best result on tuning data\", analysis.best_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Make a request with the tuned config\n",
    "\n",
    "We can apply the tuned config on the request for an example task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:41:55.056205Z",
     "iopub.status.busy": "2023-02-13T23:41:55.055631Z",
     "iopub.status.idle": "2023-02-13T23:41:56.039259Z",
     "shell.execute_reply": "2023-02-13T23:41:56.038427Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nUsing the product property of logarithms, we have \\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10}(40\\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10}(1000) \\\\\\\\\\n&= \\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 1,\n",
      "      \"message\": {\n",
      "        \"content\": \"Using the logarithmic identity $\\\\log_{a}b+\\\\log_{a}c=\\\\log_{a}(bc)$, we can simplify the expression:\\n\\n$$\\\\log_{10} 40 +\\\\log_{10} 25 = \\\\log_{10} (40 \\\\cdot 25) = \\\\log_{10} 1000$$\\n\\nWe know that $10^3=1000$, so $\\\\log_{10} 1000 = \\\\boxed{3}$.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 2,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the fact that $\\\\log a + \\\\log b = \\\\log ab$ for positive $a$ and $b$. Applying this to the given expression, we have \\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10}(40\\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10} 1000 \\\\\\\\\\n&= \\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 3,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nUsing the logarithmic identity $\\\\log_a b+\\\\log_a c=\\\\log_a (bc)$, we have:\\n\\n\\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40\\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10} 1000 \\\\\\\\\\n&= \\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 4,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe start by using the product-to-sum identity $\\\\log_{a} m + \\\\log_{a} n = \\\\log_{a} mn$ to combine the two logarithms. This gives us:\\n\\n$$\\\\log_{10} 40 +\\\\log_{10} 25 = \\\\log_{10} (40\\\\cdot 25)$$\\n\\nWe can simplify the expression inside the logarithm by noticing that $40=2^3\\\\cdot5$ and $25=5^2$. Therefore, $40\\\\cdot 25 = 2^3\\\\cdot 5\\\\cdot 5^2 = 2^3 \\\\cdot 5^3 = 10^3$. Substituting this back into our original expression, we get:\\n\\n$$\\\\log_{10} 40 +\\\\log\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 5,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the fact that $\\\\log_a b + \\\\log_a c = \\\\log_a (bc)$ for all positive real numbers $a$, $b$, and $c$. Applying this rule, we have  \\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40 \\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10} (1000) \\\\\\\\\\n&= \\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 6,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the logarithmic identity $\\\\log_{a} x + \\\\log_{a} y = \\\\log_{a} (xy)$.\\n\\n\\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40\\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10} (1000) \\\\\\\\\\n&= \\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 7,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the product-to-sum formula for logarithms, which states that $\\\\log_a b + \\\\log_a c = \\\\log_a (bc)$ for all positive real numbers $a$, $b$, and $c$. Applying this formula, we get \\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40 \\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10} (1000) \\\\\\\\\\n&= \\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 8,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the logarithmic identity $\\\\log_{a} b + \\\\log_{a} c = \\\\log_{a} bc$ to combine the two logarithms: $$\\\\log_{10} 40 + \\\\log_{10} 25 = \\\\log_{10} (40\\\\cdot25)$$\\n\\nWe simplify $40\\\\cdot 25$ to get $1000$, so $\\\\log_{10} (40\\\\cdot25) = \\\\log_{10} 1000$.\\n\\nFinally, we use the fact that $10^3=1000$ to conclude that $\\\\log_{10} 1000 = \\\\boxed{3}$.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 9,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nUsing the logarithmic identity $\\\\log_a b + \\\\log_a c = \\\\log_a bc$, we have:\\n\\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40 \\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10} 1000 \\\\\\\\\\n&= \\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 10,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the logarithmic identity $\\\\log_{a} b + \\\\log_{a} c = \\\\log_{a} bc$ to combine the given expression: $$\\\\log_{10} 40 +\\\\log_{10} 25 = \\\\log_{10} (40 \\\\cdot 25) = \\\\log_{10} 1000.$$Since $1000 = 10^3$, we have $\\\\log_{10} 1000 = \\\\boxed{3}$.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 11,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the product property of logarithms, which states that $\\\\log_a b + \\\\log_a c = \\\\log_a (bc)$ for all positive numbers $a$, $b$, and $c$. We apply this property to $\\\\log_{10} 40$ and $\\\\log_{10} 25$:\\n\\n\\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40 \\\\cdot 25)\\\\\\\\\\n&=\\\\log_{10} 1000\\\\\\\\\\n&=\\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 12,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the logarithmic identity $\\\\log a + \\\\log b = \\\\log (ab)$:\\n\\n\\\\begin{align*}\\n\\\\log_{10} 40 + \\\\log_{10} 25 &= \\\\log_{10} (40\\\\cdot25) \\\\\\\\\\n&= \\\\log_{10} 1000 \\\\\\\\\\n&= \\\\boxed{3}\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 13,\n",
      "      \"message\": {\n",
      "        \"content\": \"We use the property $\\\\log_{a}b+\\\\log_{a}c=\\\\log_{a}(bc)$ to combine the two logarithms: \\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10}(40\\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10}(1000) \\\\\\\\\\n&= \\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 14,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe can use the logarithmic identity $\\\\log_{a}b+\\\\log_{a}c=\\\\log_{a}(bc)$ to combine the two terms:\\n\\n\\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40\\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10} (1000) \\\\\\\\\\n&= \\\\boxed{3}\\n\\\\end{align*}\\n\\nAnother way to simplify is to recognize that $40=2^3\\\\cdot 5$ and $25=5^2$, so $40\\\\cdot 25=2^3\\\\cdot 5^3$. Therefore, $\\\\log_{10} 40 +\\\\log_{10} 25=\\\\log_{10}(2^3\\\\cdot 5\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1679423554,\n",
      "  \"id\": \"chatcmpl-6wb4ck8aLefwKOKAmh8IRmmqD0wcb\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1597,\n",
      "    \"prompt_tokens\": 46,\n",
      "    \"total_tokens\": 1643\n",
      "  }\n",
      "}\n",
      "{'expected_success': 1.0, 'success': True, 'success_vote': 1.0, 'voted_answer': '\\n\\nUsing the product property of logarithms, we have \\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10}(40\\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10}(1000) \\\\\\\\\\n&= \\\\boxed{3}.\\n\\\\end{align*}'}\n"
     ]
    }
   ],
   "source": [
    "responses = oai.ChatCompletion.create(context=tune_data[1], **config)\n",
    "print(responses)\n",
    "print(success_metrics([response[\"message\"][\"content\"].rstrip() for response in responses[\"choices\"]], **tune_data[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the success rate on the test data\n",
    "\n",
    "You can use flaml's `oai.ChatCompletion.eval` to evaluate the performance of an entire dataset with the tuned config. To do that you need to set `oai.ChatCompletion.data` to the data to evaluate. The following code will take a while (~30mins) to evaluate all the 438 test data instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:41:56.042764Z",
     "iopub.status.busy": "2023-02-13T23:41:56.042086Z",
     "iopub.status.idle": "2023-02-13T23:53:05.597643Z",
     "shell.execute_reply": "2023-02-13T23:53:05.596603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'expected_success': 0.8715764644863164, 'success': 0.8993135011441648, 'success_vote': 0.816933638443936, 'voted_answer': '\\n\\nLet $y = \\\\arccos \\\\frac{1}{3},$ so $\\\\cos y = \\\\frac{1}{3}.$ Then $\\\\sin^2 y = 1 - \\\\cos^2 y = \\\\frac{8}{9},$ so $\\\\sin y = \\\\frac{2 \\\\sqrt{2}}{3}.$\\n\\nThen\\n\\\\[\\\\tan y = \\\\frac{\\\\sin y}{\\\\cos y} = \\\\frac{2 \\\\sqrt{2}}{1} = 2 \\\\sqrt{2},\\\\]so $\\\\tan \\\\left( \\\\arccos \\\\frac{1}{3} \\\\right) = \\\\boxed{2 \\\\sqrt{2}}.$', 'total_cost': 2.5742359999999977, 'cost': 1.5713280000000005, 'inference_cost': 0.0035957162471395884}\n"
     ]
    }
   ],
   "source": [
    "oai.ChatCompletion.data = test_data\n",
    "result = oai.ChatCompletion.eval(analysis.best_config, prune=False, eval_only=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2d910cfd2d2a4fc49fc30fbbdc5576a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "454146d0f7224f038689031002906e6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e4ae2b6f5a974fd4bafb6abb9d12ff26",
        "IPY_MODEL_577e1e3cc4db4942b0883577b3b52755",
        "IPY_MODEL_b40bdfb1ac1d4cffb7cefcb870c64d45"
       ],
       "layout": "IPY_MODEL_dc83c7bff2f241309537a8119dfc7555",
       "tabbable": null,
       "tooltip": null
      }
     },
     "577e1e3cc4db4942b0883577b3b52755": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d910cfd2d2a4fc49fc30fbbdc5576a7",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_74a6ba0c3cbc4051be0a83e152fe1e62",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "6086462a12d54bafa59d3c4566f06cb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74a6ba0c3cbc4051be0a83e152fe1e62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7d3f3d9e15894d05a4d188ff4f466554": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b40bdfb1ac1d4cffb7cefcb870c64d45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f1355871cc6f4dd4b50d9df5af20e5c8",
       "placeholder": "",
       "style": "IPY_MODEL_ca245376fd9f4354af6b2befe4af4466",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 44.69it/s]"
      }
     },
     "ca245376fd9f4354af6b2befe4af4466": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dc83c7bff2f241309537a8119dfc7555": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4ae2b6f5a974fd4bafb6abb9d12ff26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6086462a12d54bafa59d3c4566f06cb2",
       "placeholder": "",
       "style": "IPY_MODEL_7d3f3d9e15894d05a4d188ff4f466554",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "f1355871cc6f4dd4b50d9df5af20e5c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
