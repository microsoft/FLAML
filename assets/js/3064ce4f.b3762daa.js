"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5648],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>d});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),m=p(a),d=r,k=m["".concat(s,".").concat(d)]||m[d]||u[d]||l;return a?n.createElement(k,o(o({ref:t},c),{},{components:a})):n.createElement(k,o({ref:t},c))}));function d(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=a.length,o=new Array(l);o[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:r,o[1]=i;for(var p=2;p<l;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},1388:(e,t,a)=>{a.r(t),a.d(t,{contentTitle:()=>o,default:()=>c,frontMatter:()=>l,metadata:()=>i,toc:()=>s});var n=a(7462),r=(a(7294),a(3905));const l={sidebar_label:"utils",title:"tune.spark.utils"},o=void 0,i={unversionedId:"reference/tune/spark/utils",id:"reference/tune/spark/utils",isDocsHomePage:!1,title:"tune.spark.utils",description:"check\\_spark",source:"@site/docs/reference/tune/spark/utils.md",sourceDirName:"reference/tune/spark",slug:"/reference/tune/spark/utils",permalink:"/FLAML/docs/reference/tune/spark/utils",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/reference/tune/spark/utils.md",tags:[],version:"current",frontMatter:{sidebar_label:"utils",title:"tune.spark.utils"},sidebar:"referenceSideBar",previous:{title:"variant_generator",permalink:"/FLAML/docs/reference/tune/searcher/variant_generator"},next:{title:"analysis",permalink:"/FLAML/docs/reference/tune/analysis"}},s=[{value:"check_spark",id:"check_spark",children:[],level:4},{value:"get_n_cpus",id:"get_n_cpus",children:[],level:4},{value:"with_parameters",id:"with_parameters",children:[],level:4},{value:"broadcast_code",id:"broadcast_code",children:[],level:4},{value:"get_broadcast_data",id:"get_broadcast_data",children:[],level:4},{value:"PySparkOvertimeMonitor Objects",id:"pysparkovertimemonitor-objects",children:[{value:"__init__",id:"__init__",children:[],level:4},{value:"__enter__",id:"__enter__",children:[],level:4},{value:"__exit__",id:"__exit__",children:[],level:4}],level:2}],p={toc:s};function c(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h4",{id:"check_spark"},"check","_","spark"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"@lru_cache(maxsize=2)\ndef check_spark()\n")),(0,r.kt)("p",null,"Check if Spark is installed and running.\nResult of the function will be cached since test once is enough. As lru_cache will not\ncache exceptions, we don't raise exceptions here but only log a warning message."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Returns"),":"),(0,r.kt)("p",null,"  Return (True, None) if the check passes, otherwise log the exception message and\nreturn (False, Exception(msg)). The exception can be raised by the caller."),(0,r.kt)("h4",{id:"get_n_cpus"},"get","_","n","_","cpus"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def get_n_cpus(node="driver")\n')),(0,r.kt)("p",null,"Get the number of CPU cores of the given type of node."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"node")," - string | The type of node to get the number of cores. Can be 'driver' or 'executor'.\nDefault is 'driver'.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Returns"),":"),(0,r.kt)("p",null,"  An int of the number of CPU cores."),(0,r.kt)("h4",{id:"with_parameters"},"with","_","parameters"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def with_parameters(trainable, **kwargs)\n")),(0,r.kt)("p",null,"Wrapper for trainables to pass arbitrary large data objects."),(0,r.kt)("p",null,"This wrapper function will store all passed parameters in the Spark\nBroadcast variable."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"trainable")," - Trainable to wrap."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"**kwargs")," - parameters to store in object store.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Returns"),":"),(0,r.kt)("p",null,"  A new function with partial application of the given arguments\nand keywords. The given arguments and keywords will be broadcasted\nto all the executors."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import pyspark\nimport flaml\nfrom sklearn.datasets import load_iris\ndef train(config, data=None):\n    if isinstance(data, pyspark.broadcast.Broadcast):\n        data = data.value\n    print(config, data)\n\ndata = load_iris()\nwith_parameters_train = flaml.tune.spark.utils.with_parameters(train, data=data)\nwith_parameters_train(config=1)\ntrain(config={"metric": "accuracy"})\n')),(0,r.kt)("h4",{id:"broadcast_code"},"broadcast","_","code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def broadcast_code(custom_code="", file_name="mylearner")\n')),(0,r.kt)("p",null,"Write customized learner/metric code contents to a file for importing.\nIt is necessary for using the customized learner/metric in spark backend.\nThe path of the learner/metric file will be returned."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"custom_code"),' - str, default="" | code contents of the custom learner/metric.'),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"file_name"),' - str, default="mylearner" | file name of the custom learner/metric.')),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Returns"),":"),(0,r.kt)("p",null,"  The path of the custom code file."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from flaml.tune.spark.utils import broadcast_code\nfrom flaml.automl.model import LGBMEstimator\n\ncustom_code = \'\'\'\nfrom flaml.automl.model import LGBMEstimator\nfrom flaml import tune\n\nclass MyLargeLGBM(LGBMEstimator):\n    @classmethod\n    def search_space(cls, **params):\n        return {\n            "n_estimators": {\n                "domain": tune.lograndint(lower=4, upper=32768),\n                "init_value": 32768,\n                "low_cost_init_value": 4,\n            },\n            "num_leaves": {\n                "domain": tune.lograndint(lower=4, upper=32768),\n                "init_value": 32768,\n                "low_cost_init_value": 4,\n            },\n        }\n\'\'\'\n\nbroadcast_code(custom_code=custom_code)\nfrom flaml.tune.spark.mylearner import MyLargeLGBM\nassert isinstance(MyLargeLGBM(), LGBMEstimator)\n')),(0,r.kt)("h4",{id:"get_broadcast_data"},"get","_","broadcast","_","data"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def get_broadcast_data(broadcast_data)\n")),(0,r.kt)("p",null,"Get the broadcast data from the broadcast variable."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"broadcast_data")," - pyspark.broadcast.Broadcast | the broadcast variable.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Returns"),":"),(0,r.kt)("p",null,"  The broadcast data."),(0,r.kt)("h2",{id:"pysparkovertimemonitor-objects"},"PySparkOvertimeMonitor Objects"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"class PySparkOvertimeMonitor()\n")),(0,r.kt)("p",null,"A context manager class to monitor if the PySpark job is overtime."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"with PySparkOvertimeMonitor(time_start, time_budget_s, force_cancel, parallel=parallel):\n    results = parallel(\n        delayed(evaluation_function)(trial_to_run.config)\n        for trial_to_run in trials_to_run\n    )\n")),(0,r.kt)("h4",{id:"__init__"},"_","_","init","_","_"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def __init__(start_time, time_budget_s, force_cancel=False, cancel_func=None, parallel=None, sc=None)\n")),(0,r.kt)("p",null,"Constructor."),(0,r.kt)("p",null,"Specify the time budget and start time of the PySpark job, and specify how to cancel them."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,r.kt)("p",null,"  Args relate to monitoring:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"start_time")," - float | The start time of the PySpark job.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"time_budget_s")," - float | The time budget of the PySpark job in seconds.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"force_cancel")," - boolean, default=False | Whether to forcely cancel the PySpark job if overtime."),(0,r.kt)("p",{parentName:"li"},"Args relate to how to cancel the PySpark job:\n(Only one of the following args will work. Priorities from top to bottom)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"cancel_func")," - function | A function to cancel the PySpark job.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"parallel")," - joblib.parallel.Parallel | Specify this if using joblib_spark as a parallel backend. It will call parallel._backend.terminate() to cancel the jobs.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"sc")," - pyspark.SparkContext object | You can pass a specific SparkContext."),(0,r.kt)("p",{parentName:"li"},"If all three args is None, the monitor will call pyspark.SparkContext.getOrCreate().cancelAllJobs() to cancel the jobs."))),(0,r.kt)("h4",{id:"__enter__"},"_","_","enter","_","_"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def __enter__()\n")),(0,r.kt)("p",null,"Enter the context manager.\nThis will start a monitor thread if spark is available and force_cancel is True."),(0,r.kt)("h4",{id:"__exit__"},"_","_","exit","_","_"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def __exit__(exc_type, exc_value, exc_traceback)\n")),(0,r.kt)("p",null,"Exit the context manager.\nThis will wait for the monitor thread to nicely exit."))}c.isMDXComponent=!0}}]);