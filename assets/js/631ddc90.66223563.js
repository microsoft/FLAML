"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[9601],{5680:(e,t,n)=>{n.d(t,{xA:()=>p,yg:()=>g});var i=n(6540);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=i.createContext({}),m=function(e){var t=i.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=m(e.components);return i.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),u=m(n),g=a,d=u["".concat(s,".").concat(g)]||u[g]||c[g]||r;return n?i.createElement(d,l(l({ref:t},p),{},{components:n})):i.createElement(d,l({ref:t},p))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,l=new Array(r);l[0]=u;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:a,l[1]=o;for(var m=2;m<r;m++)l[m]=n[m];return i.createElement.apply(null,l)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},3993:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>s});var i=n(8168),a=(n(6540),n(5680));const r={},l="Best Practices",o={unversionedId:"Best-Practices",id:"Best-Practices",isDocsHomePage:!1,title:"Best Practices",description:"This page collects practical guidance for using FLAML effectively across common tasks.",source:"@site/docs/Best-Practices.md",sourceDirName:".",slug:"/Best-Practices",permalink:"/FLAML/docs/Best-Practices",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/Best-Practices.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Tune - PyTorch",permalink:"/FLAML/docs/Examples/Tune-PyTorch"},next:{title:"Contributing",permalink:"/FLAML/docs/Contribute"}},s=[{value:"General tips",id:"general-tips",children:[],level:2},{value:"Classification",id:"classification",children:[],level:2},{value:"Regression",id:"regression",children:[],level:2},{value:"Learning to rank",id:"learning-to-rank",children:[],level:2},{value:"Time series forecasting",id:"time-series-forecasting",children:[],level:2},{value:"NLP (Transformers)",id:"nlp-transformers",children:[],level:2},{value:"Speed, stability, and tricky settings",id:"speed-stability-and-tricky-settings",children:[],level:2},{value:"Persisting models",id:"persisting-models",children:[{value:"Option 1: MLflow logging (recommended for production)",id:"option-1-mlflow-logging-recommended-for-production",children:[],level:3},{value:"Option 2: Pickle the full <code>AutoML</code> instance (convenient)",id:"option-2-pickle-the-full-automl-instance-convenient",children:[],level:3}],level:2}],m={toc:s};function p(e){let{components:t,...n}=e;return(0,a.yg)("wrapper",(0,i.A)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"best-practices"},"Best Practices"),(0,a.yg)("p",null,"This page collects practical guidance for using FLAML effectively across common tasks."),(0,a.yg)("h2",{id:"general-tips"},"General tips"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Start simple: set ",(0,a.yg)("inlineCode",{parentName:"li"},"task"),", ",(0,a.yg)("inlineCode",{parentName:"li"},"time_budget"),", and keep ",(0,a.yg)("inlineCode",{parentName:"li"},'metric="auto"')," unless you have a strong reason to override."),(0,a.yg)("li",{parentName:"ul"},"Prefer correct splits: ensure your evaluation strategy matches your data (time series vs i.i.d., grouped data, etc.)."),(0,a.yg)("li",{parentName:"ul"},"Keep estimator lists explicit when debugging: start with a small ",(0,a.yg)("inlineCode",{parentName:"li"},"estimator_list")," and expand."),(0,a.yg)("li",{parentName:"ul"},"Use built-in discovery helpers to avoid stale hardcoded lists:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'from flaml import AutoML\nfrom flaml.automl.task.factory import task_factory\n\nautoml = AutoML()\nprint("Built-in sklearn metrics:", sorted(automl.supported_metrics[0]))\nprint(\n    "classification estimators:",\n    sorted(task_factory("classification").estimators.keys()),\n)\n')),(0,a.yg)("h2",{id:"classification"},"Classification"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Metric"),": for binary classification, ",(0,a.yg)("inlineCode",{parentName:"li"},'metric="roc_auc"')," is common; for multiclass, ",(0,a.yg)("inlineCode",{parentName:"li"},'metric="log_loss"')," is often robust."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Imbalanced data"),":",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"pass ",(0,a.yg)("inlineCode",{parentName:"li"},"sample_weight")," to ",(0,a.yg)("inlineCode",{parentName:"li"},"AutoML.fit()"),";"),(0,a.yg)("li",{parentName:"ul"},"consider setting class weights via ",(0,a.yg)("inlineCode",{parentName:"li"},"custom_hp")," / ",(0,a.yg)("inlineCode",{parentName:"li"},"fit_kwargs_by_estimator")," for specific estimators (see ",(0,a.yg)("a",{parentName:"li",href:"FAQ"},"FAQ"),")."))),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Probability vs label metrics"),": use ",(0,a.yg)("inlineCode",{parentName:"li"},"roc_auc")," / ",(0,a.yg)("inlineCode",{parentName:"li"},"log_loss")," when you care about calibrated probabilities.")),(0,a.yg)("h2",{id:"regression"},"Regression"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Default metric"),": ",(0,a.yg)("inlineCode",{parentName:"li"},'metric="r2"')," (minimizes ",(0,a.yg)("inlineCode",{parentName:"li"},"1 - r2"),")."),(0,a.yg)("li",{parentName:"ul"},"If your target scale matters (e.g., dollar error), consider ",(0,a.yg)("inlineCode",{parentName:"li"},"mae"),"/",(0,a.yg)("inlineCode",{parentName:"li"},"rmse"),".")),(0,a.yg)("h2",{id:"learning-to-rank"},"Learning to rank"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Use ",(0,a.yg)("inlineCode",{parentName:"li"},'task="rank"')," with group information (",(0,a.yg)("inlineCode",{parentName:"li"},"groups")," / ",(0,a.yg)("inlineCode",{parentName:"li"},"groups_val"),") so metrics like ",(0,a.yg)("inlineCode",{parentName:"li"},"ndcg")," and ",(0,a.yg)("inlineCode",{parentName:"li"},"ndcg@k")," are meaningful."),(0,a.yg)("li",{parentName:"ul"},"If you pass ",(0,a.yg)("inlineCode",{parentName:"li"},'metric="ndcg@10"'),", also pass ",(0,a.yg)("inlineCode",{parentName:"li"},"groups")," so FLAML can compute group-aware NDCG.")),(0,a.yg)("h2",{id:"time-series-forecasting"},"Time series forecasting"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Use time-aware splitting. For holdout validation, set ",(0,a.yg)("inlineCode",{parentName:"li"},'eval_method="holdout"')," and use a time-ordered dataset."),(0,a.yg)("li",{parentName:"ul"},"Prefer supplying a DataFrame with a clear time column when possible."),(0,a.yg)("li",{parentName:"ul"},"Optional time-series estimators depend on optional dependencies. To list what is available in your environment:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'from flaml.automl.task.factory import task_factory\n\nprint("forecast:", sorted(task_factory("forecast").estimators.keys()))\n')),(0,a.yg)("h2",{id:"nlp-transformers"},"NLP (Transformers)"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Install the optional dependency: ",(0,a.yg)("inlineCode",{parentName:"li"},'pip install "flaml[hf]"'),"."),(0,a.yg)("li",{parentName:"ul"},"When you provide a custom metric, ensure it returns ",(0,a.yg)("inlineCode",{parentName:"li"},"(metric_to_minimize, metrics_to_log)")," with stable keys.")),(0,a.yg)("h2",{id:"speed-stability-and-tricky-settings"},"Speed, stability, and tricky settings"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Time budget vs convergence"),": if you see warnings about not all estimators converging, increase ",(0,a.yg)("inlineCode",{parentName:"li"},"time_budget")," or reduce ",(0,a.yg)("inlineCode",{parentName:"li"},"estimator_list"),"."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Memory pressure / OOM"),":",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"set ",(0,a.yg)("inlineCode",{parentName:"li"},"free_mem_ratio")," (e.g., ",(0,a.yg)("inlineCode",{parentName:"li"},"0.2"),") to keep free memory above a threshold;"),(0,a.yg)("li",{parentName:"ul"},"set ",(0,a.yg)("inlineCode",{parentName:"li"},"model_history=False")," to reduce stored artifacts;"))),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Reproducibility"),": set ",(0,a.yg)("inlineCode",{parentName:"li"},"seed")," and keep ",(0,a.yg)("inlineCode",{parentName:"li"},"n_jobs")," fixed; expect some runtime variance.")),(0,a.yg)("h2",{id:"persisting-models"},"Persisting models"),(0,a.yg)("p",null,"FLAML supports ",(0,a.yg)("strong",{parentName:"p"},"both")," MLflow logging and pickle-based persistence. For production deployment, MLflow logging is typically the most important option because it plugs into the MLflow ecosystem (tracking, model registry, serving, governance). For quick local reuse, persisting the whole ",(0,a.yg)("inlineCode",{parentName:"p"},"AutoML")," object via pickle is often the most convenient."),(0,a.yg)("h3",{id:"option-1-mlflow-logging-recommended-for-production"},"Option 1: MLflow logging (recommended for production)"),(0,a.yg)("p",null,"When you run ",(0,a.yg)("inlineCode",{parentName:"p"},"AutoML.fit()")," inside an MLflow run, FLAML can log metrics/params automatically (disable via ",(0,a.yg)("inlineCode",{parentName:"p"},"mlflow_logging=False")," if needed). To persist the trained ",(0,a.yg)("inlineCode",{parentName:"p"},"AutoML")," object as a model artifact and reuse MLflow tooling end-to-end:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import mlflow\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom flaml import AutoML\n\n\nX, y = load_iris(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nautoml = AutoML()\nmlflow.set_experiment("flaml")\nwith mlflow.start_run(run_name="flaml_run") as run:\n    automl.fit(X_train, y_train, task="classification", time_budget=3)\n\nrun_id = run.info.run_id\n\n# Later (or in a different process)\nautoml2 = mlflow.sklearn.load_model(f"runs:/{run_id}/model")\nassert np.array_equal(automl2.predict(X_test), automl.predict(X_test))\n')),(0,a.yg)("h3",{id:"option-2-pickle-the-full-automl-instance-convenient"},"Option 2: Pickle the full ",(0,a.yg)("inlineCode",{parentName:"h3"},"AutoML")," instance (convenient)"),(0,a.yg)("p",null,"Pickling stores the ",(0,a.yg)("em",{parentName:"p"},"entire")," ",(0,a.yg)("inlineCode",{parentName:"p"},"AutoML")," instance (not just the best estimator). This is useful when you prefer not to rely on MLflow or when you want to reuse additional attributes of the AutoML object without retraining."),(0,a.yg)("p",null,"In Microsoft Fabric scenarios, additional attributes is particularly important for re-plotting visualization figures without requiring model retraining."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import mlflow\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom flaml import AutoML\n\n\nX, y = load_iris(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nautoml = AutoML()\nmlflow.set_experiment("flaml")\nwith mlflow.start_run(run_name="flaml_run") as run:\n    automl.fit(X_train, y_train, task="classification", time_budget=3)\n\nautoml.pickle("automl.pkl")\nautoml2 = AutoML.load_pickle("automl.pkl")\nassert np.array_equal(automl2.predict(X_test), automl.predict(X_test))\nassert automl.best_config == automl2.best_config\nassert automl.best_loss == automl2.best_loss\nassert automl.mlflow_integration.infos == automl2.mlflow_integration.infos\n')),(0,a.yg)("p",null,"See also: ",(0,a.yg)("a",{parentName:"p",href:"Use-Cases/Task-Oriented-AutoML"},"Task-Oriented AutoML")," and ",(0,a.yg)("a",{parentName:"p",href:"FAQ"},"FAQ"),"."))}p.isMDXComponent=!0}}]);