"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1570],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return c}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),m=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=m(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),d=m(n),c=r,f=d["".concat(s,".").concat(c)]||d[c]||u[c]||i;return n?a.createElement(f,l(l({ref:t},p),{},{components:n})):a.createElement(f,l({ref:t},p))}));function c(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,l=new Array(i);l[0]=d;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:r,l[1]=o;for(var m=2;m<i;m++)l[m]=n[m];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},8250:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return o},contentTitle:function(){return s},metadata:function(){return m},toc:function(){return p},default:function(){return d}});var a=n(7462),r=n(3366),i=(n(7294),n(3905)),l=["components"],o={sidebar_label:"automl",title:"automl"},s=void 0,m={unversionedId:"reference/automl",id:"reference/automl",isDocsHomePage:!1,title:"automl",description:"size",source:"@site/docs/reference/automl.md",sourceDirName:"reference",slug:"/reference/automl",permalink:"/FLAML/docs/reference/automl",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/reference/automl.md",tags:[],version:"current",frontMatter:{sidebar_label:"automl",title:"automl"},sidebar:"referenceSideBar",previous:{title:"tune",permalink:"/FLAML/docs/reference/tune/tune"},next:{title:"data",permalink:"/FLAML/docs/reference/data"}},p=[{value:"size",id:"size",children:[],level:4},{value:"AutoML Objects",id:"automl-objects",children:[{value:"__init__",id:"__init__",children:[],level:4},{value:"config_history",id:"config_history",children:[],level:4},{value:"model",id:"model",children:[],level:4},{value:"best_model_for_estimator",id:"best_model_for_estimator",children:[],level:4},{value:"best_estimator",id:"best_estimator",children:[],level:4},{value:"best_iteration",id:"best_iteration",children:[],level:4},{value:"best_config",id:"best_config",children:[],level:4},{value:"best_config_per_estimator",id:"best_config_per_estimator",children:[],level:4},{value:"best_loss_per_estimator",id:"best_loss_per_estimator",children:[],level:4},{value:"best_loss",id:"best_loss",children:[],level:4},{value:"best_config_train_time",id:"best_config_train_time",children:[],level:4},{value:"classes_",id:"classes_",children:[],level:4},{value:"time_to_find_best_model",id:"time_to_find_best_model",children:[],level:4},{value:"predict",id:"predict",children:[],level:4},{value:"predict_proba",id:"predict_proba",children:[],level:4},{value:"add_learner",id:"add_learner",children:[],level:4},{value:"get_estimator_from_log",id:"get_estimator_from_log",children:[],level:4},{value:"retrain_from_log",id:"retrain_from_log",children:[],level:4},{value:"search_space",id:"search_space",children:[],level:4},{value:"low_cost_partial_config",id:"low_cost_partial_config",children:[],level:4},{value:"cat_hp_cost",id:"cat_hp_cost",children:[],level:4},{value:"points_to_evaluate",id:"points_to_evaluate",children:[],level:4},{value:"resource_attr",id:"resource_attr",children:[],level:4},{value:"min_resource",id:"min_resource",children:[],level:4},{value:"max_resource",id:"max_resource",children:[],level:4},{value:"trainable",id:"trainable",children:[],level:4},{value:"metric_constraints",id:"metric_constraints",children:[],level:4},{value:"fit",id:"fit",children:[],level:4}],level:2}],u={toc:p};function d(e){var t=e.components,n=(0,r.Z)(e,l);return(0,i.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h4",{id:"size"},"size"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def size(state: AutoMLState, config: dict) -> float\n")),(0,i.kt)("p",null,"Size function."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  The mem size in bytes for a config."),(0,i.kt)("h2",{id:"automl-objects"},"AutoML Objects"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"class AutoML(BaseEstimator)\n")),(0,i.kt)("p",null,"The AutoML class."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Example"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'automl = AutoML()\nautoml_settings = {\n    "time_budget": 60,\n    "metric": \'accuracy\',\n    "task": \'classification\',\n    "log_file_name": \'mylog.log\',\n}\nautoml.fit(X_train = X_train, y_train = y_train, **automl_settings)\n')),(0,i.kt)("h4",{id:"__init__"},"_","_","init","_","_"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def __init__(**settings)\n")),(0,i.kt)("p",null,"Constructor."),(0,i.kt)("p",null,"Many settings in fit() can be passed to the constructor too.\nIf an argument in fit() is provided, it will override the setting passed to the constructor.\nIf an argument in fit() is not provided but provided in the constructor, the value passed to the constructor will be used."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"metric")," - A string of the metric name or a function,\ne.g., 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo',\n'f1', 'micro_f1', 'macro_f1', 'log_loss', 'mae', 'mse', 'r2',\n'mape'. Default is 'auto'.\nIf passing a customized metric function, the function needs to\nhave the follwing signature:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def custom_metric(\n    X_test, y_test, estimator, labels,\n    X_train, y_train, weight_test=None, weight_train=None,\n    config=None, groups_test=None, groups_train=None,\n):\n    return metric_to_minimize, metrics_to_log\n")),(0,i.kt)("p",null,"  which returns a float number as the minimization objective,\nand a dictionary as the metrics to log. E.g.,"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def custom_metric(\n    X_val, y_val, estimator, labels,\n    X_train, y_train, weight_val=None, weight_train=None,\n    **args,\n):\n    from sklearn.metrics import log_loss\n    import time\n\n    start = time.time()\n    y_pred = estimator.predict_proba(X_val)\n    pred_time = (time.time() - start) / len(X_val)\n    val_loss = log_loss(y_val, y_pred, labels=labels, sample_weight=weight_val)\n    y_pred = estimator.predict_proba(X_train)\n    train_loss = log_loss(y_train, y_pred, labels=labels, sample_weight=weight_train)\n    alpha = 0.5\n    return val_loss * (1 + alpha) - alpha * train_loss, {\n        "val_loss": val_loss,\n        "train_loss": train_loss,\n        "pred_time": pred_time,\n    }\n')),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"task")," - A string of the task type, e.g.,\n'classification', 'regression', 'ts_forecast', 'rank',\n'seq-classification', 'seq-regression', 'summarization'."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"n_jobs")," - An integer of the number of threads for training."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"gpu_per_trial")," - A float of the number of gpus per trial, only used by TransformersEstimator."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"log_file_name"),' - A string of the log file name. To disable logging,\nset it to be an empty string "".'),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"estimator_list")," - A list of strings for estimator names, or 'auto'\ne.g., ",(0,i.kt)("inlineCode",{parentName:"li"},"['lgbm', 'xgboost', 'xgb_limitdepth', 'catboost', 'rf', 'extra_tree']")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"time_budget")," - A float number of the time budget in seconds.\nUse -1 if no time limit."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"max_iter")," - An integer of the maximal number of iterations."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"sample")," - A boolean of whether to sample the training data during\nsearch."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"ensemble")," - boolean or dict | default=False. Whether to perform\nensemble after search. Can be a dict with keys 'passthrough'\nand 'final_estimator' to specify the passthrough and\nfinal_estimator in the stacker."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"eval_method")," - A string of resampling strategy, one of\n","['auto', 'cv', 'holdout']","."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"split_ratio")," - A float of the valiation data percentage for holdout."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"n_splits")," - An integer of the number of folds for cross - validation."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"log_type")," - A string of the log type, one of\n","['better', 'all']",".\n'better' only logs configs with better loss than previos iters\n'all' logs all the tried configs."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"model_history")," - A boolean of whether to keep the best\nmodel per estimator. Make sure memory is large enough if setting to True."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"log_training_metric")," - A boolean of whether to log the training\nmetric for each model."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"mem_thres")," - A float of the memory size constraint in bytes."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"pred_time_limit")," - A float of the prediction latency constraint in seconds.\nIt refers to the average prediction time per row in validation data."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"train_time_limit")," - A float of the training time constraint in seconds."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"verbose")," - int, default=3 | Controls the verbosity, higher means more\nmessages."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"retrain_full")," - bool or str, default=True | whether to retrain the\nselected model on the full training data when using holdout.\nTrue - retrain only after search finishes; False - no retraining;\n'budget' - do best effort to retrain without violating the time\nbudget."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"split_type"),' - str or splitter object, default="auto" | the data split type.',(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"A valid splitter object is an instance of a derived class of scikit-learn\n",(0,i.kt)("a",{parentName:"li",href:"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold"},"KFold"),"\nand have ",(0,i.kt)("inlineCode",{parentName:"li"},"split")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"get_n_splits"),' methods with the same signatures.\nSet eval_method to "cv" to use the splitter object.'),(0,i.kt)("li",{parentName:"ul"},"Valid str options depend on different tasks.\nFor classification tasks, valid choices are\n","[\"auto\", 'stratified', 'uniform', 'time', 'group']",'. "auto" -> stratified.\nFor regression tasks, valid choices are ',"[\"auto\", 'uniform', 'time']",'.\n"auto" -> uniform.\nFor ts_forecast tasks, must be "auto" or \'time\'.\nFor ranking task, must be "auto" or \'group\'.'))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"hpo_method")," - str, default=\"auto\" | The hyperparameter\noptimization method. By default, CFO is used for sequential\nsearch and BlendSearch is used for parallel search.\nNo need to set when using flaml's default search space or using\na simple customized search space. When set to 'bs', BlendSearch\nis used. BlendSearch can be tried when the search space is\ncomplex, for example, containing multiple disjoint, discontinuous\nsubspaces. When set to 'random', random search is used."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"starting_points")," - A dictionary to specify the starting hyperparameter\nconfig for the estimators.\nKeys are the name of the estimators, and values are the starting\nhyperparamter configurations for the corresponding estimators.\nThe value can be a single hyperparamter configuration dict or a list\nof hyperparamter configuration dicts.\nIn the following code example, we get starting_points from the\n",(0,i.kt)("inlineCode",{parentName:"li"},"automl")," object and use them in the ",(0,i.kt)("inlineCode",{parentName:"li"},"new_automl")," object.\ne.g.,")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flaml import AutoML\nautoml = AutoML()\nX_train, y_train = load_iris(return_X_y=True)\nautoml.fit(X_train, y_train)\nstarting_points = automl.best_config_per_estimator\n\nnew_automl = AutoML()\nnew_automl.fit(X_train, y_train, starting_points=starting_points)\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"seed")," - int or None, default=None | The random seed for hpo."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"n_concurrent_trials")," - ","[Experimental]"," int, default=1 | The number of\nconcurrent trials. For n_concurrent_trials > 1, installation of\nray is required: ",(0,i.kt)("inlineCode",{parentName:"li"},"pip install flaml[ray]"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"keep_search_state")," - boolean, default=False | Whether to keep data needed\nfor model search after fit(). By default the state is deleted for\nspace saving."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"early_stop")," - boolean, default=False | Whether to stop early if the\nsearch is considered to converge."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"append_log")," - boolean, default=False | Whetehr to directly append the log\nrecords to the input log file if it exists."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"auto_augment")," - boolean, default=True | Whether to automatically\naugment rare classes."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"min_sample_size")," - int, default=MIN_SAMPLE_TRAIN | the minimal sample\nsize when sample=True."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"use_ray")," - boolean, default=False | Whether to use ray to run the training\nin separate processes. This can be used to prevent OOM for large\ndatasets, but will incur more overhead in time. Only use it if\nyou run into OOM failures.")),(0,i.kt)("h4",{id:"config_history"},"config","_","history"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef config_history()\n")),(0,i.kt)("p",null,"A dictionary of iter->(estimator, config, time),\nstoring the best estimator, config, and the time when the best\nmodel is updated each time."),(0,i.kt)("h4",{id:"model"},"model"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef model()\n")),(0,i.kt)("p",null,"An object with ",(0,i.kt)("inlineCode",{parentName:"p"},"predict()")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"predict_proba()")," method (for\nclassification), storing the best trained model."),(0,i.kt)("h4",{id:"best_model_for_estimator"},"best","_","model","_","for","_","estimator"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def best_model_for_estimator(estimator_name)\n")),(0,i.kt)("p",null,"Return the best model found for a particular estimator."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"estimator_name")," - a str of the estimator's name.")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  An object storing the best model for estimator_name.\nIf ",(0,i.kt)("inlineCode",{parentName:"p"},"model_history")," was set to False during fit(), then the returned model\nis untrained unless estimator_name is the best estimator.\nIf ",(0,i.kt)("inlineCode",{parentName:"p"},"model_history")," was set to True, then the returned model is trained."),(0,i.kt)("h4",{id:"best_estimator"},"best","_","estimator"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef best_estimator()\n")),(0,i.kt)("p",null,"A string indicating the best estimator found."),(0,i.kt)("h4",{id:"best_iteration"},"best","_","iteration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef best_iteration()\n")),(0,i.kt)("p",null,"An integer of the iteration number where the best\nconfig is found."),(0,i.kt)("h4",{id:"best_config"},"best","_","config"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef best_config()\n")),(0,i.kt)("p",null,"A dictionary of the best configuration."),(0,i.kt)("h4",{id:"best_config_per_estimator"},"best","_","config","_","per","_","estimator"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef best_config_per_estimator()\n")),(0,i.kt)("p",null,"A dictionary of all estimators' best configuration."),(0,i.kt)("h4",{id:"best_loss_per_estimator"},"best","_","loss","_","per","_","estimator"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef best_loss_per_estimator()\n")),(0,i.kt)("p",null,"A dictionary of all estimators' best loss."),(0,i.kt)("h4",{id:"best_loss"},"best","_","loss"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef best_loss()\n")),(0,i.kt)("p",null,"A float of the best loss found."),(0,i.kt)("h4",{id:"best_config_train_time"},"best","_","config","_","train","_","time"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef best_config_train_time()\n")),(0,i.kt)("p",null,"A float of the seconds taken by training the best config."),(0,i.kt)("h4",{id:"classes_"},"classes","_"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef classes_()\n")),(0,i.kt)("p",null,"A list of n_classes elements for class labels."),(0,i.kt)("h4",{id:"time_to_find_best_model"},"time","_","to","_","find","_","best","_","model"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef time_to_find_best_model() -> float\n")),(0,i.kt)("p",null,"Time taken to find best model in seconds."),(0,i.kt)("h4",{id:"predict"},"predict"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def predict(X_test: Union[np.array, pd.DataFrame, List[str], List[List[str]]])\n")),(0,i.kt)("p",null,"Predict label from features."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"X_test")," - A numpy array of featurized instances, shape n * m,\nor for 'ts_forecast' task:\na pandas dataframe with the first column containing\ntimestamp values (datetime type) or an integer n for\nthe predict steps (only valid when the estimator is\narima or sarimax). Other columns in the dataframe\nare assumed to be exogenous variables (categorical\nor numeric).")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"multivariate_X_test = pd.DataFrame({\n    'timeStamp': pd.date_range(start='1/1/2022', end='1/07/2022'),\n    'categorical_col': ['yes', 'yes', 'no', 'no', 'yes', 'no', 'yes'],\n    'continuous_col': [105, 107, 120, 118, 110, 112, 115]\n})\nmodel.predict(multivariate_X_test)\n")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A array-like of shape n * 1: each element is a predicted\nlabel for an instance."),(0,i.kt)("h4",{id:"predict_proba"},"predict","_","proba"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def predict_proba(X_test)\n")),(0,i.kt)("p",null,"Predict the probability of each class from features, only works for\nclassification problems."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"X_test")," - A numpy array of featurized instances, shape n * m.")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A numpy array of shape n * c. c is the  # classes. Each element at\n(i, j) is the probability for instance i to be in class j."),(0,i.kt)("h4",{id:"add_learner"},"add","_","learner"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def add_learner(learner_name, learner_class)\n")),(0,i.kt)("p",null,"Add a customized learner."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"learner_name")," - A string of the learner's name."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"learner_class")," - A subclass of flaml.model.BaseEstimator.")),(0,i.kt)("h4",{id:"get_estimator_from_log"},"get","_","estimator","_","from","_","log"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def get_estimator_from_log(log_file_name, record_id, task)\n")),(0,i.kt)("p",null,"Get the estimator from log file."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"log_file_name")," - A string of the log file name."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"record_id")," - An integer of the record ID in the file,\n0 corresponds to the first trial."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"task")," - A string of the task type,\n'binary', 'multi', 'regression', 'ts_forecast', 'rank'.")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  An estimator object for the given configuration."),(0,i.kt)("h4",{id:"retrain_from_log"},"retrain","_","from","_","log"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def retrain_from_log(log_file_name, X_train=None, y_train=None, dataframe=None, label=None, time_budget=np.inf, task=None, eval_method=None, split_ratio=None, n_splits=None, split_type=None, groups=None, n_jobs=-1, gpu_per_trial=0, train_best=True, train_full=False, record_id=-1, auto_augment=None, **fit_kwargs, ,)\n")),(0,i.kt)("p",null,"Retrain from log file."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"log_file_name")," - A string of the log file name."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"X_train")," - A numpy array or dataframe of training data in shape n*m.\nFor 'ts_forecast' task, the first column of X_train\nmust be the timestamp column (datetime type). Other\ncolumns in the dataframe are assumed to be exogenous\nvariables (categorical or numeric)."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"y_train")," - A numpy array or series of labels in shape n*1."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"dataframe")," - A dataframe of training data including label column.\nFor 'ts_forecast' task, dataframe must be specified and should\nhave at least two columns: timestamp and label, where the first\ncolumn is the timestamp column (datetime type). Other columns\nin the dataframe are assumed to be exogenous variables\n(categorical or numeric)."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"label")," - A str of the label column name, e.g., 'label';"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Note")," - If X_train and y_train are provided,\ndataframe and label are ignored;\nIf not, dataframe and label must be provided."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"time_budget")," - A float number of the time budget in seconds."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"task")," - A string of the task type, e.g.,\n'classification', 'regression', 'ts_forecast', 'rank',\n'seq-classification', 'seq-regression', 'summarization'."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"eval_method")," - A string of resampling strategy, one of\n","['auto', 'cv', 'holdout']","."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"split_ratio")," - A float of the validation data percentage for holdout."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"n_splits")," - An integer of the number of folds for cross-validation."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"split_type"),' - str or splitter object, default="auto" | the data split type.',(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"A valid splitter object is an instance of a derived class of scikit-learn\n",(0,i.kt)("a",{parentName:"li",href:"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold"},"KFold"),"\nand have ",(0,i.kt)("inlineCode",{parentName:"li"},"split")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"get_n_splits"),' methods with the same signatures.\nSet eval_method to "cv" to use the splitter object.'),(0,i.kt)("li",{parentName:"ul"},"Valid str options depend on different tasks.\nFor classification tasks, valid choices are\n","[\"auto\", 'stratified', 'uniform', 'time', 'group']",'. "auto" -> stratified.\nFor regression tasks, valid choices are ',"[\"auto\", 'uniform', 'time']",'.\n"auto" -> uniform.\nFor ts_forecast tasks, must be "auto" or \'time\'.\nFor ranking task, must be "auto" or \'group\'.'))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"groups")," - None or array-like | Group labels (with matching length to\ny_train) or groups counts (with sum equal to length of y_train)\nfor training data."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"n_jobs")," - An integer of the number of threads for training. Use all\navailable resources when n_jobs == -1."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"gpu_per_trial")," - A float of the number of gpus per trial. Only used by TransformersEstimator."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"train_best")," - A boolean of whether to train the best config in the\ntime budget; if false, train the last config in the budget."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"train_full")," - A boolean of whether to train on the full data. If true,\neval_method and sample_size in the log file will be ignored."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"record_id")," - the ID of the training log record from which the model will\nbe retrained. By default ",(0,i.kt)("inlineCode",{parentName:"li"},"record_id = -1")," which means this will be\nignored. ",(0,i.kt)("inlineCode",{parentName:"li"},"record_id = 0")," corresponds to the first trial, and\nwhen ",(0,i.kt)("inlineCode",{parentName:"li"},"record_id >= 0"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"time_budget")," will be ignored."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"auto_augment")," - boolean, default=True | Whether to automatically\naugment rare classes."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"**fit_kwargs")," - Other key word arguments to pass to fit() function of\nthe searched learners, such as sample_weight.")),(0,i.kt)("h4",{id:"search_space"},"search","_","space"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef search_space() -> dict\n")),(0,i.kt)("p",null,"Search space."),(0,i.kt)("p",null,"Must be called after fit(...)\n(use max_iter=0 and retrain_final=False to prevent actual fitting)."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A dict of the search space."),(0,i.kt)("h4",{id:"low_cost_partial_config"},"low","_","cost","_","partial","_","config"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef low_cost_partial_config() -> dict\n")),(0,i.kt)("p",null,"Low cost partial config."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A dict.\n(a) if there is only one estimator in estimator_list, each key is a\nhyperparameter name.\n(b) otherwise, it is a nested dict with 'ml' as the key, and\na list of the low_cost_partial_configs as the value, corresponding\nto each learner's low_cost_partial_config; the estimator index as\nan integer corresponding to the cheapest learner is appended to the\nlist at the end."),(0,i.kt)("h4",{id:"cat_hp_cost"},"cat","_","hp","_","cost"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef cat_hp_cost() -> dict\n")),(0,i.kt)("p",null,"Categorical hyperparameter cost"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A dict.\n(a) if there is only one estimator in estimator_list, each key is a\nhyperparameter name.\n(b) otherwise, it is a nested dict with 'ml' as the key, and\na list of the cat_hp_cost's as the value, corresponding\nto each learner's cat_hp_cost; the cost relative to lgbm for each\nlearner (as a list itself) is appended to the list at the end."),(0,i.kt)("h4",{id:"points_to_evaluate"},"points","_","to","_","evaluate"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef points_to_evaluate() -> dict\n")),(0,i.kt)("p",null,"Initial points to evaluate."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A list of dicts. Each dict is the initial point for each learner."),(0,i.kt)("h4",{id:"resource_attr"},"resource","_","attr"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef resource_attr() -> Optional[str]\n")),(0,i.kt)("p",null,"Attribute of the resource dimension."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A string for the sample size attribute\n(the resource attribute in AutoML) or None."),(0,i.kt)("h4",{id:"min_resource"},"min","_","resource"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef min_resource() -> Optional[float]\n")),(0,i.kt)("p",null,"Attribute for pruning."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A float for the minimal sample size or None."),(0,i.kt)("h4",{id:"max_resource"},"max","_","resource"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef max_resource() -> Optional[float]\n")),(0,i.kt)("p",null,"Attribute for pruning."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A float for the maximal sample size or None."),(0,i.kt)("h4",{id:"trainable"},"trainable"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef trainable() -> Callable[[dict], Optional[float]]\n")),(0,i.kt)("p",null,"Training function."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A function that evaluates each config and returns the loss."),(0,i.kt)("h4",{id:"metric_constraints"},"metric","_","constraints"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"@property\ndef metric_constraints() -> list\n")),(0,i.kt)("p",null,"Metric constraints."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Returns"),":"),(0,i.kt)("p",null,"  A list of the metric constraints."),(0,i.kt)("h4",{id:"fit"},"fit"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def fit(X_train=None, y_train=None, dataframe=None, label=None, metric=None, task=None, n_jobs=None, gpu_per_trial=0, log_file_name=None, estimator_list=None, time_budget=None, max_iter=None, sample=None, ensemble=None, eval_method=None, log_type=None, model_history=None, split_ratio=None, n_splits=None, log_training_metric=None, mem_thres=None, pred_time_limit=None, train_time_limit=None, X_val=None, y_val=None, sample_weight_val=None, groups_val=None, groups=None, verbose=None, retrain_full=None, split_type=None, learner_selector=None, hpo_method=None, starting_points=None, seed=None, n_concurrent_trials=None, keep_search_state=None, early_stop=None, append_log=None, auto_augment=None, min_sample_size=None, use_ray=None, **fit_kwargs, ,)\n")),(0,i.kt)("p",null,"Find a model for a given task."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"X_train")," - A numpy array or a pandas dataframe of training data in\nshape (n, m). For 'ts_forecast' task, the first column of X_train\nmust be the timestamp column (datetime type). Other columns in\nthe dataframe are assumed to be exogenous variables (categorical or numeric)."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"y_train")," - A numpy array or a pandas series of labels in shape (n, )."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"dataframe")," - A dataframe of training data including label column.\nFor 'ts_forecast' task, dataframe must be specified and must have\nat least two columns, timestamp and label, where the first\ncolumn is the timestamp column (datetime type). Other columns in\nthe dataframe are assumed to be exogenous variables (categorical or numeric)."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"label")," - A str of the label column name for, e.g., 'label';"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Note")," - If X_train and y_train are provided,\ndataframe and label are ignored;\nIf not, dataframe and label must be provided."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"metric")," - A string of the metric name or a function,\ne.g., 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo',\n'f1', 'micro_f1', 'macro_f1', 'log_loss', 'mae', 'mse', 'r2',\n'mape'. Default is 'auto'.\nIf passing a customized metric function, the function needs to\nhave the follwing signature:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def custom_metric(\n    X_test, y_test, estimator, labels,\n    X_train, y_train, weight_test=None, weight_train=None,\n    config=None, groups_test=None, groups_train=None,\n):\n    return metric_to_minimize, metrics_to_log\n")),(0,i.kt)("p",null,"  which returns a float number as the minimization objective,\nand a dictionary as the metrics to log. E.g.,"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def custom_metric(\n    X_val, y_val, estimator, labels,\n    X_train, y_train, weight_val=None, weight_train=None,\n    **args,\n):\n    from sklearn.metrics import log_loss\n    import time\n\n    start = time.time()\n    y_pred = estimator.predict_proba(X_val)\n    pred_time = (time.time() - start) / len(X_val)\n    val_loss = log_loss(y_val, y_pred, labels=labels, sample_weight=weight_val)\n    y_pred = estimator.predict_proba(X_train)\n    train_loss = log_loss(y_train, y_pred, labels=labels, sample_weight=weight_train)\n    alpha = 0.5\n    return val_loss * (1 + alpha) - alpha * train_loss, {\n        "val_loss": val_loss,\n        "train_loss": train_loss,\n        "pred_time": pred_time,\n    }\n')),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"task")," - A string of the task type, e.g.,\n'classification', 'regression', 'ts_forecast', 'rank',\n'seq-classification', 'seq-regression', 'summarization'"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"n_jobs")," - An integer of the number of threads for training."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"gpu_per_trial")," - A float of the number of gpus per trial, only used by TransformersEstimator."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"log_file_name"),' - A string of the log file name. To disable logging,\nset it to be an empty string "".'),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"estimator_list")," - A list of strings for estimator names, or 'auto'\ne.g., ",(0,i.kt)("inlineCode",{parentName:"li"},"['lgbm', 'xgboost', 'xgb_limitdepth', 'catboost', 'rf', 'extra_tree']")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"time_budget")," - A float number of the time budget in seconds.\nUse -1 if no time limit."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"max_iter")," - An integer of the maximal number of iterations."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"sample")," - A boolean of whether to sample the training data during\nsearch."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"ensemble")," - boolean or dict | default=False. Whether to perform\nensemble after search. Can be a dict with keys 'passthrough'\nand 'final_estimator' to specify the passthrough and\nfinal_estimator in the stacker."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"eval_method")," - A string of resampling strategy, one of\n","['auto', 'cv', 'holdout']","."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"split_ratio")," - A float of the valiation data percentage for holdout."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"n_splits")," - An integer of the number of folds for cross - validation."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"log_type")," - A string of the log type, one of\n","['better', 'all']",".\n'better' only logs configs with better loss than previos iters\n'all' logs all the tried configs."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"model_history")," - A boolean of whether to keep the trained best\nmodel per estimator. Make sure memory is large enough if setting to True.\nDefault value is False: best_model_for_estimator would return a\nuntrained model for non-best learner."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"log_training_metric")," - A boolean of whether to log the training\nmetric for each model."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"mem_thres")," - A float of the memory size constraint in bytes."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"pred_time_limit")," - A float of the prediction latency constraint in seconds.\nIt refers to the average prediction time per row in validation data."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"train_time_limit")," - A float of the training time constraint in seconds."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"X_val")," - None or a numpy array or a pandas dataframe of validation data."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"y_val")," - None or a numpy array or a pandas series of validation labels."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"sample_weight_val")," - None or a numpy array of the sample weight of\nvalidation data of the same shape as y_val."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"groups_val")," - None or array-like | group labels (with matching length\nto y_val) or group counts (with sum equal to length of y_val)\nfor validation data. Need to be consistent with groups."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"groups")," - None or array-like | Group labels (with matching length to\ny_train) or groups counts (with sum equal to length of y_train)\nfor training data."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"verbose")," - int, default=3 | Controls the verbosity, higher means more\nmessages."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"retrain_full")," - bool or str, default=True | whether to retrain the\nselected model on the full training data when using holdout.\nTrue - retrain only after search finishes; False - no retraining;\n'budget' - do best effort to retrain without violating the time\nbudget."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"split_type"),' - str or splitter object, default="auto" | the data split type.',(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"A valid splitter object is an instance of a derived class of scikit-learn\n",(0,i.kt)("a",{parentName:"li",href:"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold"},"KFold"),"\nand have ",(0,i.kt)("inlineCode",{parentName:"li"},"split")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"get_n_splits"),' methods with the same signatures.\nSet eval_method to "cv" to use the splitter object.'),(0,i.kt)("li",{parentName:"ul"},"Valid str options depend on different tasks.\nFor classification tasks, valid choices are\n","[\"auto\", 'stratified', 'uniform', 'time', 'group']",'. "auto" -> stratified.\nFor regression tasks, valid choices are ',"[\"auto\", 'uniform', 'time']",'.\n"auto" -> uniform.\nFor ts_forecast tasks, must be "auto" or \'time\'.\nFor ranking task, must be "auto" or \'group\'.'))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"hpo_method")," - str, default=\"auto\" | The hyperparameter\noptimization method. By default, CFO is used for sequential\nsearch and BlendSearch is used for parallel search.\nNo need to set when using flaml's default search space or using\na simple customized search space. When set to 'bs', BlendSearch\nis used. BlendSearch can be tried when the search space is\ncomplex, for example, containing multiple disjoint, discontinuous\nsubspaces. When set to 'random', random search is used."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"starting_points")," - A dictionary to specify the starting hyperparameter\nconfig for the estimators.\nKeys are the name of the estimators, and values are the starting\nhyperparamter configurations for the corresponding estimators.\nThe value can be a single hyperparamter configuration dict or a list\nof hyperparamter configuration dicts.\nIn the following code example, we get starting_points from the\n",(0,i.kt)("inlineCode",{parentName:"li"},"automl")," object and use them in the ",(0,i.kt)("inlineCode",{parentName:"li"},"new_automl")," object.\ne.g.,")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flaml import AutoML\nautoml = AutoML()\nX_train, y_train = load_iris(return_X_y=True)\nautoml.fit(X_train, y_train)\nstarting_points = automl.best_config_per_estimator\n\nnew_automl = AutoML()\nnew_automl.fit(X_train, y_train, starting_points=starting_points)\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"seed")," - int or None, default=None | The random seed for hpo."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"n_concurrent_trials")," - ","[Experimental]"," int, default=1 | The number of\nconcurrent trials. For n_concurrent_trials > 1, installation of\nray is required: ",(0,i.kt)("inlineCode",{parentName:"li"},"pip install flaml[ray]"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"keep_search_state")," - boolean, default=False | Whether to keep data needed\nfor model search after fit(). By default the state is deleted for\nspace saving."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"early_stop")," - boolean, default=False | Whether to stop early if the\nsearch is considered to converge."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"append_log")," - boolean, default=False | Whetehr to directly append the log\nrecords to the input log file if it exists."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"auto_augment")," - boolean, default=True | Whether to automatically\naugment rare classes."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"min_sample_size")," - int, default=MIN_SAMPLE_TRAIN | the minimal sample\nsize when sample=True."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"use_ray")," - boolean, default=False | Whether to use ray to run the training\nin separate processes. This can be used to prevent OOM for large\ndatasets, but will incur more overhead in time. Only use it if\nyou run into OOM failures."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"**fit_kwargs")," - Other key word arguments to pass to fit() function of\nthe searched learners, such as sample_weight. Include period as\na key word argument for 'ts_forecast' task.")))}d.isMDXComponent=!0}}]);