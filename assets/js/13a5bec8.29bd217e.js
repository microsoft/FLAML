"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1710],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>d});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function o(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),u=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},c=function(e){var n=u(e.components);return r.createElement(s.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},p=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),p=u(t),d=a,_=p["".concat(s,".").concat(d)]||p[d]||m[d]||i;return t?r.createElement(_,l(l({ref:n},c),{},{components:t})):r.createElement(_,l({ref:n},c))}));function d(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,l=new Array(i);l[0]=p;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o.mdxType="string"==typeof e?e:a,l[1]=o;for(var u=2;u<i;u++)l[u]=t[u];return r.createElement.apply(null,l)}return r.createElement.apply(null,t)}p.displayName="MDXCreateElement"},5971:(e,n,t)=>{t.r(n),t.d(n,{contentTitle:()=>l,default:()=>c,frontMatter:()=>i,metadata:()=>o,toc:()=>s});var r=t(7462),a=(t(7294),t(3905));const i={},l="Tune - HuggingFace",o={unversionedId:"Examples/Tune-HuggingFace",id:"Examples/Tune-HuggingFace",isDocsHomePage:!1,title:"Tune - HuggingFace",description:"This example uses flaml to finetune a transformer model from Huggingface transformers library.",source:"@site/docs/Examples/Tune-HuggingFace.md",sourceDirName:"Examples",slug:"/Examples/Tune-HuggingFace",permalink:"/FLAML/docs/Examples/Tune-HuggingFace",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/Examples/Tune-HuggingFace.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Tune - AzureML pipeline",permalink:"/FLAML/docs/Examples/Tune-AzureML-pipeline"},next:{title:"Tune - PyTorch",permalink:"/FLAML/docs/Examples/Tune-PyTorch"}},s=[{value:"Requirements",id:"requirements",children:[],level:3},{value:"Prepare for tuning",id:"prepare-for-tuning",children:[{value:"Tokenizer",id:"tokenizer",children:[],level:4},{value:"Define training method",id:"define-training-method",children:[],level:4}],level:3},{value:"Define the search",id:"define-the-search",children:[],level:3},{value:"Launch the tuning",id:"launch-the-tuning",children:[],level:3},{value:"Retrieve the results",id:"retrieve-the-results",children:[],level:3}],u={toc:s};function c(e){let{components:n,...t}=e;return(0,a.kt)("wrapper",(0,r.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"tune---huggingface"},"Tune - HuggingFace"),(0,a.kt)("p",null,"This example uses flaml to finetune a transformer model from Huggingface transformers library."),(0,a.kt)("p",null,(0,a.kt)("em",{parentName:"p"},"Note"),": ",(0,a.kt)("inlineCode",{parentName:"p"},"flaml.AutoML")," has built-in support for certain finetuning tasks with a\n",(0,a.kt)("a",{parentName:"p",href:"AutoML-NLP"},"higher-level API"),".\nIt may be easier to use that API unless you have special requirements not handled by that API."),(0,a.kt)("h3",{id:"requirements"},"Requirements"),(0,a.kt)("p",null,"This example requires GPU. Install dependencies:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'pip install torch transformers datasets "flaml[blendsearch,ray]"\n')),(0,a.kt)("h3",{id:"prepare-for-tuning"},"Prepare for tuning"),(0,a.kt)("h4",{id:"tokenizer"},"Tokenizer"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from transformers import AutoTokenizer\n\nMODEL_NAME = "distilbert-base-uncased"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nCOLUMN_NAME = "sentence"\n\ndef tokenize(examples):\n    return tokenizer(examples[COLUMN_NAME], truncation=True)\n')),(0,a.kt)("h4",{id:"define-training-method"},"Define training method"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import flaml\nimport datasets\nfrom transformers import AutoModelForSequenceClassification\n\nTASK = "cola"\nNUM_LABELS = 2\n\ndef train_distilbert(config: dict):\n    # Load CoLA dataset and apply tokenizer\n    cola_raw = datasets.load_dataset("glue", TASK)\n    cola_encoded = cola_raw.map(tokenize, batched=True)\n    train_dataset, eval_dataset = cola_encoded["train"], cola_encoded["validation"]\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME, num_labels=NUM_LABELS\n    )\n    metric = datasets.load_metric("glue", TASK)\n\n    def compute_metrics(eval_pred):\n        predictions, labels = eval_pred\n        predictions = np.argmax(predictions, axis=1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    training_args = TrainingArguments(\n        output_dir=\'.\',\n        do_eval=False,\n        disable_tqdm=True,\n        logging_steps=20000,\n        save_total_limit=0,\n        **config,\n    )\n\n    trainer = Trainer(\n        model,\n        training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n\n    # train model\n    trainer.train()\n\n    # evaluate model\n    eval_output = trainer.evaluate()\n\n    # report the metric to optimize & the metric to log\n    flaml.tune.report(\n        loss=eval_output["eval_loss"],\n        matthews_correlation=eval_output["eval_matthews_correlation"],\n    )\n')),(0,a.kt)("h3",{id:"define-the-search"},"Define the search"),(0,a.kt)("p",null,"We are now ready to define our search. This includes:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"search_space")," for our hyperparameters"),(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"metric")," and the ",(0,a.kt)("inlineCode",{parentName:"li"},"mode")," ('max' or 'min') for optimization"),(0,a.kt)("li",{parentName:"ul"},"The constraints (",(0,a.kt)("inlineCode",{parentName:"li"},"n_cpus"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"n_gpus"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"num_samples"),", and ",(0,a.kt)("inlineCode",{parentName:"li"},"time_budget_s"),")")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'max_num_epoch = 64\nsearch_space = {\n        # You can mix constants with search space objects.\n        "num_train_epochs": flaml.tune.loguniform(1, max_num_epoch),\n        "learning_rate": flaml.tune.loguniform(1e-6, 1e-4),\n        "adam_epsilon": flaml.tune.loguniform(1e-9, 1e-7),\n        "adam_beta1": flaml.tune.uniform(0.8, 0.99),\n        "adam_beta2": flaml.tune.loguniform(98e-2, 9999e-4),\n}\n\n# optimization objective\nHP_METRIC, MODE = "matthews_correlation", "max"\n\n# resources\nnum_cpus = 4\nnum_gpus = 4  # change according to your GPU resources\n\n# constraints\nnum_samples = -1  # number of trials, -1 means unlimited\ntime_budget_s = 3600  # time budget in seconds\n')),(0,a.kt)("h3",{id:"launch-the-tuning"},"Launch the tuning"),(0,a.kt)("p",null,"We are now ready to launch the tuning using ",(0,a.kt)("inlineCode",{parentName:"p"},"flaml.tune.run"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import ray\n\nray.init(num_cpus=num_cpus, num_gpus=num_gpus)\nprint("Tuning started...")\nanalysis = flaml.tune.run(\n    train_distilbert,\n    search_alg=flaml.CFO(\n        space=search_space,\n        metric=HP_METRIC,\n        mode=MODE,\n        low_cost_partial_config={"num_train_epochs": 1}),\n    resources_per_trial={"gpu": num_gpus, "cpu": num_cpus},\n    local_dir=\'logs/\',\n    num_samples=num_samples,\n    time_budget_s=time_budget_s,\n    use_ray=True,\n)\n')),(0,a.kt)("p",null,"This will run tuning for one hour. At the end we will see a summary."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"== Status ==\nMemory usage on this node: 32.0/251.6 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/4 CPUs, 0/4 GPUs, 0.0/150.39 GiB heap, 0.0/47.22 GiB objects (0/1.0 accelerator_type:V100)\nResult logdir: /home/chiw/FLAML/notebook/logs/train_distilbert_2021-05-07_02-35-58\nNumber of trials: 22/infinite (22 TERMINATED)\nTrial name  status  loc adam_beta1  adam_beta2  adam_epsilon    learning_rate   num_train_epochs    iter    total time (s)  loss    matthews_correlation\ntrain_distilbert_a0c303d0   TERMINATED      0.939079    0.991865    7.96945e-08 5.61152e-06 1   1   55.6909 0.587986    0\ntrain_distilbert_a0c303d1   TERMINATED      0.811036    0.997214    2.05111e-09 2.05134e-06 1.44427 1   71.7663 0.603018    0\ntrain_distilbert_c39b2ef0   TERMINATED      0.909395    0.993715    1e-07   5.26543e-06 1   1   53.7619 0.586518    0\ntrain_distilbert_f00776e2   TERMINATED      0.968763    0.990019    4.38943e-08 5.98035e-06 1.02723 1   56.8382 0.581313    0\ntrain_distilbert_11ab3900   TERMINATED      0.962198    0.991838    7.09296e-08 5.06608e-06 1   1   54.0231 0.585576    0\ntrain_distilbert_353025b6   TERMINATED      0.91596 0.991892    8.95426e-08 6.21568e-06 2.15443 1   98.3233 0.531632    0.388893\ntrain_distilbert_5728a1de   TERMINATED      0.926933    0.993146    1e-07   1.00902e-05 1   1   55.3726 0.538505    0.280558\ntrain_distilbert_9394c2e2   TERMINATED      0.928106    0.990614    4.49975e-08 3.45674e-06 2.72935 1   121.388 0.539177    0.327295\ntrain_distilbert_b6543fec   TERMINATED      0.876896    0.992098    1e-07   7.01176e-06 1.59538 1   76.0244 0.527516    0.379177\ntrain_distilbert_0071f998   TERMINATED      0.955024    0.991687    7.39776e-08 5.50998e-06 2.90939 1   126.871 0.516225    0.417157\ntrain_distilbert_2f830be6   TERMINATED      0.886931    0.989628    7.6127e-08  4.37646e-06 1.53338 1   73.8934 0.551629    0.0655887\ntrain_distilbert_7ce03f12   TERMINATED      0.984053    0.993956    8.70144e-08 7.82557e-06 4.08775 1   174.027 0.523732    0.453549\ntrain_distilbert_aaab0508   TERMINATED      0.940707    0.993946    1e-07   8.91979e-06 3.40243 1   146.249 0.511288    0.45085\ntrain_distilbert_14262454   TERMINATED      0.99    0.991696    4.60093e-08 4.83405e-06 3.4954  1   152.008 0.53506 0.400851\ntrain_distilbert_6d211fe6   TERMINATED      0.959277    0.994556    5.40791e-08 1.17333e-05 6.64995 1   271.444 0.609851    0.526802\ntrain_distilbert_c980bae4   TERMINATED      0.99    0.993355    1e-07   5.21929e-06 2.51275 1   111.799 0.542276    0.324968\ntrain_distilbert_6d0d29d6   TERMINATED      0.965773    0.995182    9.9752e-08  1.15549e-05 13.694  1   527.944 0.923802    0.549474\ntrain_distilbert_b16ea82a   TERMINATED      0.952781    0.993931    2.93182e-08 1.19145e-05 3.2293  1   139.844 0.533466    0.451307\ntrain_distilbert_eddf7cc0   TERMINATED      0.99    0.997109    8.13498e-08 1.28515e-05 15.5807 1   614.789 0.983285    0.56993\ntrain_distilbert_43008974   TERMINATED      0.929089    0.993258    1e-07   1.03892e-05 12.0357 1   474.387 0.857461    0.520022\ntrain_distilbert_b3408a4e   TERMINATED      0.99    0.993809    4.67441e-08 1.10418e-05 11.9165 1   474.126 0.828205    0.526164\ntrain_distilbert_cfbfb220   TERMINATED      0.979454    0.9999  1e-07   1.49578e-05 20.3715\n")),(0,a.kt)("h3",{id:"retrieve-the-results"},"Retrieve the results"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"best_trial = analysis.get_best_trial(HP_METRIC, MODE, \"all\")\nmetric = best_trial.metric_analysis[HP_METRIC][MODE]\nprint(f\"n_trials={len(analysis.trials)}\")\nprint(f\"time={time.time()-start_time}\")\nprint(f\"Best model eval {HP_METRIC}: {metric:.4f}\")\nprint(f\"Best model parameters: {best_trial.config}\")\n# n_trials=22\n# time=3999.769361972809\n# Best model eval matthews_correlation: 0.5699\n# Best model parameters: {'num_train_epochs': 15.580684188655825, 'learning_rate': 1.2851507818900338e-05, 'adam_epsilon': 8.134982521948352e-08, 'adam_beta1': 0.99, 'adam_beta2': 0.9971094424784387}\n")),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://github.com/microsoft/FLAML/blob/main/notebook/tune_huggingface.ipynb"},"Link to notebook")," | ",(0,a.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/tune_huggingface.ipynb"},"Open in colab")))}c.isMDXComponent=!0}}]);