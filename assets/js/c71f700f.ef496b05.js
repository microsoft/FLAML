"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4364],{3905:(e,t,r)=>{r.d(t,{Zo:()=>u,kt:()=>_});var a=r(7294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function s(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?s(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):s(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},s=Object.keys(e);for(a=0;a<s.length;a++)r=s[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)r=s[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var i=a.createContext({}),m=function(e){var t=a.useContext(i),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},u=function(e){var t=m(e.components);return a.createElement(i.Provider,{value:t},e.children)},b={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},f=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,s=e.originalType,i=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),f=m(r),_=n,d=f["".concat(i,".").concat(_)]||f[_]||b[_]||s;return r?a.createElement(d,o(o({ref:t},u),{},{components:r})):a.createElement(d,o({ref:t},u))}));function _(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var s=r.length,o=new Array(s);o[0]=f;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l.mdxType="string"==typeof e?e:n,o[1]=l;for(var m=2;m<s;m++)o[m]=r[m];return a.createElement.apply(null,o)}return a.createElement.apply(null,r)}f.displayName="MDXCreateElement"},7308:(e,t,r)=>{r.r(t),r.d(t,{contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>l,toc:()=>i});var a=r(7462),n=(r(7294),r(3905));const s={},o="AutoML - Time Series Forecast",l={unversionedId:"Examples/AutoML-Time series forecast",id:"Examples/AutoML-Time series forecast",isDocsHomePage:!1,title:"AutoML - Time Series Forecast",description:"Prerequisites",source:"@site/docs/Examples/AutoML-Time series forecast.md",sourceDirName:"Examples",slug:"/Examples/AutoML-Time series forecast",permalink:"/FLAML/docs/Examples/AutoML-Time series forecast",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/Examples/AutoML-Time series forecast.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"AutoML - Regression",permalink:"/FLAML/docs/Examples/AutoML-Regression"},next:{title:"AutoML for LightGBM",permalink:"/FLAML/docs/Examples/AutoML-for-LightGBM"}},i=[{value:"Prerequisites",id:"prerequisites",children:[],level:3},{value:"Simple NumPy Example",id:"simple-numpy-example",children:[{value:"Sample output",id:"sample-output",children:[],level:4}],level:3},{value:"Univariate time series",id:"univariate-time-series",children:[{value:"Sample output",id:"sample-output-1",children:[],level:4},{value:"Compute and plot predictions",id:"compute-and-plot-predictions",children:[],level:4}],level:3},{value:"Multivariate Time Series (Forecasting with Exogeneous Variables)",id:"multivariate-time-series-forecasting-with-exogeneous-variables",children:[{value:"Sample Output",id:"sample-output-2",children:[],level:4}],level:3},{value:"Forecasting Discrete Variables",id:"forecasting-discrete-variables",children:[{value:"Sample Output",id:"sample-output-3",children:[],level:4}],level:3},{value:"Forecasting with Panel Datasets",id:"forecasting-with-panel-datasets",children:[{value:"Sample Output",id:"sample-output-4",children:[],level:4}],level:3}],m={toc:i};function u(e){let{components:t,...s}=e;return(0,n.kt)("wrapper",(0,a.Z)({},m,s,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"automl---time-series-forecast"},"AutoML - Time Series Forecast"),(0,n.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,n.kt)("p",null,"Install the ","[ts_forecast]"," option."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'pip install "flaml[ts_forecast]"\n')),(0,n.kt)("h3",{id:"simple-numpy-example"},"Simple NumPy Example"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nfrom flaml import AutoML\n\nX_train = np.arange('2014-01', '2022-01', dtype='datetime64[M]')\ny_train = np.random.random(size=84)\nautoml = AutoML()\nautoml.fit(X_train=X_train[:84],  # a single column of timestamp\n           y_train=y_train,  # value for each timestamp\n           period=12,  # time horizon to forecast, e.g., 12 months\n           task='ts_forecast', time_budget=15,  # time budget in seconds\n           log_file_name=\"ts_forecast.log\",\n           eval_method=\"holdout\",\n          )\nprint(automl.predict(X_train[84:]))\n")),(0,n.kt)("h4",{id:"sample-output"},"Sample output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"[flaml.automl: 01-21 08:01:20] {2018} INFO - task = ts_forecast\n[flaml.automl: 01-21 08:01:20] {2020} INFO - Data split method: time\n[flaml.automl: 01-21 08:01:20] {2024} INFO - Evaluation method: holdout\n[flaml.automl: 01-21 08:01:20] {2124} INFO - Minimizing error metric: mape\n[flaml.automl: 01-21 08:01:21] {2181} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'prophet', 'arima', 'sarimax']\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 0, current learner lgbm\n[flaml.automl: 01-21 08:01:21] {2547} INFO - Estimated sufficient time budget=1429s. Estimated necessary time budget=1s.\n[flaml.automl: 01-21 08:01:21] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 1, current learner lgbm\n[flaml.automl: 01-21 08:01:21] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 2, current learner lgbm\n[flaml.automl: 01-21 08:01:21] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 3, current learner lgbm\n[flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 4, current learner lgbm\n[flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 5, current learner lgbm\n[flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 6, current learner lgbm\n[flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9652,     best estimator lgbm's best error=0.9652\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 7, current learner lgbm\n[flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 8, current learner lgbm\n[flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 9, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 10, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 11, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 12, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 13, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 14, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 15, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 16, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 17, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 18, current learner rf\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator rf's best error=1.0994,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 19, current learner rf\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator rf's best error=1.0848,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 20, current learner xgboost\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0271,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 21, current learner rf\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator rf's best error=1.0848,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 22, current learner xgboost\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 23, current learner xgboost\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 24, current learner xgboost\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 25, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 26, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 27, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 28, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 29, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 30, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 31, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 32, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 33, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 34, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 35, current learner xgboost\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 36, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 37, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 38, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 39, current learner xgboost\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 40, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 41, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 42, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 43, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 44, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator xgb_limitdepth's best error=1.5815,   best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 45, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.8s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 46, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.8s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 47, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.8s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 48, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 49, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 50, current learner extra_tree\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 51, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 52, current learner xgboost\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 53, current learner xgboost\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 54, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 55, current learner lgbm\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 56, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 57, current learner rf\n[flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator rf's best error=1.0848,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 58, current learner xgboost\n[flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 59, current learner extra_tree\n[flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 60, current learner lgbm\n[flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 61, current learner extra_tree\n[flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 62, current learner lgbm\n[flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 63, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.2s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 64, current learner prophet\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.2s,  estimator prophet's best error=1.5706,  best estimator lgbm's best error=0.9466\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 65, current learner arima\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.2s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 66, current learner arima\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.4s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 67, current learner sarimax\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.4s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 68, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.5s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 69, current learner sarimax\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 70, current learner sarimax\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 71, current learner arima\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 72, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 73, current learner arima\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.7s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 74, current learner sarimax\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.7s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 75, current learner arima\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.8s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 76, current learner sarimax\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.9s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 77, current learner arima\n[flaml.automl: 01-21 08:01:25] {2594} INFO -  at 5.0s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 78, current learner sarimax\n[flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 79, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 80, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 81, current learner sarimax\n[flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 82, current learner prophet\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator prophet's best error=1.4076,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 83, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 84, current learner sarimax\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 85, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 86, current learner sarimax\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.8s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 87, current learner arima\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.8s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 88, current learner sarimax\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.9s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 89, current learner arima\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.9s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 90, current learner arima\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 7.0s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 91, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 7.0s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 92, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:27] {2594} INFO -  at 7.0s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693\n[flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 93, current learner sarimax\n[flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.0s,  estimator sarimax's best error=0.5600,  best estimator sarimax's best error=0.5600\n[flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 94, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.1s,  estimator xgb_limitdepth's best error=0.9683,   best estimator sarimax's best error=0.5600\n[flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 95, current learner sarimax\n[flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.2s,  estimator sarimax's best error=0.5600,  best estimator sarimax's best error=0.5600\n[flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 96, current learner arima\n[flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.2s,  estimator arima's best error=0.5693,    best estimator sarimax's best error=0.5600\n[flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 97, current learner arima\n[flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.2s,  estimator arima's best error=0.5693,    best estimator sarimax's best error=0.5600\n[flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 98, current learner extra_tree\n[flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.3s,  estimator extra_tree's best error=0.9499,       best estimator sarimax's best error=0.5600\n[flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 99, current learner sarimax\n[flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.3s,  estimator sarimax's best error=0.5600,  best estimator sarimax's best error=0.5600\n[flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 100, current learner xgb_limitdepth\n[flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.3s,  estimator xgb_limitdepth's best error=0.9683,   best estimator sarimax's best error=0.5600\n")),(0,n.kt)("h3",{id:"univariate-time-series"},"Univariate time series"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"import statsmodels.api as sm\n\ndata = sm.datasets.co2.load_pandas().data\n# data is given in weeks, but the task is to predict monthly, so use monthly averages instead\ndata = data['co2'].resample('MS').mean()\ndata = data.bfill().ffill()  # makes sure there are no missing values\ndata = data.to_frame().reset_index()\nnum_samples = data.shape[0]\ntime_horizon = 12\nsplit_idx = num_samples - time_horizon\ntrain_df = data[:split_idx]  # train_df is a dataframe with two columns: timestamp and label\nX_test = data[split_idx:]['index'].to_frame()  # X_test is a dataframe with dates for prediction\ny_test = data[split_idx:]['co2']  # y_test is a series of the values corresponding to the dates for prediction\n\nfrom flaml import AutoML\n\nautoml = AutoML()\nsettings = {\n    \"time_budget\": 10,  # total running time in seconds\n    \"metric\": 'mape',  # primary metric for validation: 'mape' is generally used for forecast tasks\n    \"task\": 'ts_forecast',  # task type\n    \"log_file_name\": 'CO2_forecast.log',  # flaml log file\n    \"eval_method\": \"holdout\",  # validation method can be chosen from ['auto', 'holdout', 'cv']\n    \"seed\": 7654321,  # random seed\n}\n\nautoml.fit(dataframe=train_df,  # training data\n           label='co2',  # label column\n           period=time_horizon,  # key word argument 'period' must be included for forecast task)\n           **settings)\n")),(0,n.kt)("h4",{id:"sample-output-1"},"Sample output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"[flaml.automl: 01-21 07:54:04] {2018} INFO - task = ts_forecast\n[flaml.automl: 01-21 07:54:04] {2020} INFO - Data split method: time\n[flaml.automl: 01-21 07:54:04] {2024} INFO - Evaluation method: holdout\n[flaml.automl: 01-21 07:54:04] {2124} INFO - Minimizing error metric: mape\nImporting plotly failed. Interactive plots will not work.\n[flaml.automl: 01-21 07:54:04] {2181} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'prophet', 'arima', 'sarimax']\n[flaml.automl: 01-21 07:54:04] {2434} INFO - iteration 0, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2547} INFO - Estimated sufficient time budget=2145s. Estimated necessary time budget=2s.\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.0621,     best estimator lgbm's best error=0.0621\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 1, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0574,     best estimator lgbm's best error=0.0574\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 2, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0464,     best estimator lgbm's best error=0.0464\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 3, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0464,     best estimator lgbm's best error=0.0464\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 4, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0365,     best estimator lgbm's best error=0.0365\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 5, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.0192,     best estimator lgbm's best error=0.0192\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 6, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.0192,     best estimator lgbm's best error=0.0192\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 7, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.0192,     best estimator lgbm's best error=0.0192\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 8, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.0110,     best estimator lgbm's best error=0.0110\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 9, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.0110,     best estimator lgbm's best error=0.0110\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 10, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.0036,     best estimator lgbm's best error=0.0036\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 11, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.4s,  estimator lgbm's best error=0.0023,     best estimator lgbm's best error=0.0023\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 12, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.4s,  estimator lgbm's best error=0.0023,     best estimator lgbm's best error=0.0023\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 13, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.0021,     best estimator lgbm's best error=0.0021\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 14, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.6s,  estimator lgbm's best error=0.0021,     best estimator lgbm's best error=0.0021\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 15, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.7s,  estimator lgbm's best error=0.0020,     best estimator lgbm's best error=0.0020\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 16, current learner lgbm\n[flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.8s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 17, current learner lgbm\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 1.9s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 18, current learner lgbm\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.0s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 19, current learner lgbm\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.1s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 20, current learner rf\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.1s,  estimator rf's best error=0.0228,       best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 21, current learner rf\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.1s,  estimator rf's best error=0.0210,       best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 22, current learner xgboost\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.2s,  estimator xgboost's best error=0.6738,  best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 23, current learner xgboost\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.2s,  estimator xgboost's best error=0.6738,  best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 24, current learner xgboost\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.2s,  estimator xgboost's best error=0.1717,  best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 25, current learner xgboost\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.3s,  estimator xgboost's best error=0.0249,  best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 26, current learner xgboost\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.3s,  estimator xgboost's best error=0.0249,  best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 27, current learner xgboost\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.3s,  estimator xgboost's best error=0.0242,  best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 28, current learner extra_tree\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.4s,  estimator extra_tree's best error=0.0245,       best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 29, current learner extra_tree\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.4s,  estimator extra_tree's best error=0.0160,       best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 30, current learner lgbm\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.5s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 31, current learner lgbm\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.6s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 32, current learner rf\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.6s,  estimator rf's best error=0.0210,       best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 33, current learner extra_tree\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.6s,  estimator extra_tree's best error=0.0160,       best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 34, current learner lgbm\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.8s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 35, current learner extra_tree\n[flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.8s,  estimator extra_tree's best error=0.0158,       best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 36, current learner xgb_limitdepth\n[flaml.automl: 01-21 07:54:07] {2594} INFO -  at 2.8s,  estimator xgb_limitdepth's best error=0.0447,   best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 37, current learner xgb_limitdepth\n[flaml.automl: 01-21 07:54:07] {2594} INFO -  at 2.9s,  estimator xgb_limitdepth's best error=0.0447,   best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 38, current learner xgb_limitdepth\n[flaml.automl: 01-21 07:54:07] {2594} INFO -  at 2.9s,  estimator xgb_limitdepth's best error=0.0029,   best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 39, current learner xgb_limitdepth\n[flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.0s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 40, current learner xgb_limitdepth\n[flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.1s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 41, current learner xgb_limitdepth\n[flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.1s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 42, current learner xgb_limitdepth\n[flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.3s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017\n[flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 43, current learner prophet\n[flaml.automl: 01-21 07:54:09] {2594} INFO -  at 5.5s,  estimator prophet's best error=0.0008,  best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:09] {2434} INFO - iteration 44, current learner arima\n[flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.1s,  estimator arima's best error=0.0047,    best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 45, current learner sarimax\n[flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.4s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 46, current learner lgbm\n[flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.5s,  estimator lgbm's best error=0.0017,     best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 47, current learner sarimax\n[flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.6s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 48, current learner sarimax\n[flaml.automl: 01-21 07:54:11] {2594} INFO -  at 6.9s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 49, current learner arima\n[flaml.automl: 01-21 07:54:11] {2594} INFO -  at 6.9s,  estimator arima's best error=0.0047,    best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 50, current learner xgb_limitdepth\n[flaml.automl: 01-21 07:54:11] {2594} INFO -  at 7.0s,  estimator xgb_limitdepth's best error=0.0018,   best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 51, current learner sarimax\n[flaml.automl: 01-21 07:54:11] {2594} INFO -  at 7.5s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 52, current learner xgboost\n[flaml.automl: 01-21 07:54:11] {2594} INFO -  at 7.6s,  estimator xgboost's best error=0.0242,  best estimator prophet's best error=0.0008\n[flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 53, current learner prophet\n[flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.3s,  estimator prophet's best error=0.0005,  best estimator prophet's best error=0.0005\n[flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 54, current learner sarimax\n[flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.4s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0005\n[flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 55, current learner xgb_limitdepth\n[flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.8s,  estimator xgb_limitdepth's best error=0.0018,   best estimator prophet's best error=0.0005\n[flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 56, current learner xgboost\n[flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.8s,  estimator xgboost's best error=0.0242,  best estimator prophet's best error=0.0005\n[flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 57, current learner lgbm\n[flaml.automl: 01-21 07:54:14] {2594} INFO -  at 9.9s,  estimator lgbm's best error=0.0017,     best estimator prophet's best error=0.0005\n[flaml.automl: 01-21 07:54:14] {2434} INFO - iteration 58, current learner rf\n[flaml.automl: 01-21 07:54:14] {2594} INFO -  at 10.0s, estimator rf's best error=0.0146,       best estimator prophet's best error=0.0005\n[flaml.automl: 01-21 07:54:14] {2824} INFO - retrain prophet for 0.6s\n[flaml.automl: 01-21 07:54:14] {2831} INFO - retrained model: <prophet.forecaster.Prophet object at 0x7fb68ea65d60>\n[flaml.automl: 01-21 07:54:14] {2210} INFO - fit succeeded\n[flaml.automl: 01-21 07:54:14] {2211} INFO - Time taken to find the best model: 9.339771270751953\n[flaml.automl: 01-21 07:54:14] {2222} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n")),(0,n.kt)("h4",{id:"compute-and-plot-predictions"},"Compute and plot predictions"),(0,n.kt)("p",null,"The example plotting code requires matplotlib."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"flaml_y_pred = automl.predict(X_test)\nimport matplotlib.pyplot as plt\n\nplt.plot(X_test, y_test, label='Actual level')\nplt.plot(X_test, flaml_y_pred, label='FLAML forecast')\nplt.xlabel('Date')\nplt.ylabel('CO2 Levels')\nplt.legend()\n")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"png",src:r(6806).Z})),(0,n.kt)("h3",{id:"multivariate-time-series-forecasting-with-exogeneous-variables"},"Multivariate Time Series (Forecasting with Exogeneous Variables)"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'import pandas as pd\n\n# pd.set_option("display.max_rows", None, "display.max_columns", None)\nmulti_df = pd.read_csv(\n    "https://raw.githubusercontent.com/srivatsan88/YouTubeLI/master/dataset/nyc_energy_consumption.csv"\n)\n\n# preprocessing data\nmulti_df["timeStamp"] = pd.to_datetime(multi_df["timeStamp"])\nmulti_df = multi_df.set_index("timeStamp")\nmulti_df = multi_df.resample("D").mean()\nmulti_df["temp"] = multi_df["temp"].fillna(method="ffill")\nmulti_df["precip"] = multi_df["precip"].fillna(method="ffill")\nmulti_df = multi_df[:-2]  # last two rows are NaN for \'demand\' column so remove them\nmulti_df = multi_df.reset_index()\n\n# Using temperature values create categorical values\n# where 1 denotes daily tempurature is above monthly average and 0 is below.\ndef get_monthly_avg(data):\n    data["month"] = data["timeStamp"].dt.month\n    data = data[["month", "temp"]].groupby("month")\n    data = data.agg({"temp": "mean"})\n    return data\n\nmonthly_avg = get_monthly_avg(multi_df).to_dict().get("temp")\n\ndef above_monthly_avg(date, temp):\n    month = date.month\n    if temp > monthly_avg.get(month):\n        return 1\n    else:\n        return 0\n\nmulti_df["temp_above_monthly_avg"] = multi_df.apply(\n    lambda x: above_monthly_avg(x["timeStamp"], x["temp"]), axis=1\n)\n\ndel multi_df["temp"], multi_df["month"]  # remove temperature column to reduce redundancy\n\n# split data into train and test\nnum_samples = multi_df.shape[0]\nmulti_time_horizon = 180\nsplit_idx = num_samples - multi_time_horizon\nmulti_train_df = multi_df[:split_idx]\nmulti_test_df = multi_df[split_idx:]\n\nmulti_X_test = multi_test_df[\n    ["timeStamp", "precip", "temp_above_monthly_avg"]\n]  # test dataframe must contain values for the regressors / multivariate variables\nmulti_y_test = multi_test_df["demand"]\n\n# initialize AutoML instance\nautoml = AutoML()\n\n# configure AutoML settings\nsettings = {\n    "time_budget": 10,  # total running time in seconds\n    "metric": "mape",  # primary metric\n    "task": "ts_forecast",  # task type\n    "log_file_name": "energy_forecast_categorical.log",  # flaml log file\n    "eval_method": "holdout",\n    "log_type": "all",\n    "label": "demand",\n}\n\n# train the model\nautoml.fit(dataframe=df, **settings, period=time_horizon)\n\n# predictions\nprint(automl.predict(multi_X_test))\n')),(0,n.kt)("h4",{id:"sample-output-2"},"Sample Output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 15, current learner xgboost\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.2s,  estimator xgboost's best error=0.0959,  best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 16, current learner extra_tree\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.2s,  estimator extra_tree's best error=0.0961,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 17, current learner extra_tree\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.2s,  estimator extra_tree's best error=0.0961,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 18, current learner xgboost\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.2s,  estimator xgboost's best error=0.0959,  best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 19, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.3s,  estimator xgb_limitdepth's best error=0.0820,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 20, current learner xgboost\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.3s,  estimator xgboost's best error=0.0834,  best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 21, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.4s,  estimator xgb_limitdepth's best error=0.0820,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 22, current learner lgbm\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.4s,  estimator lgbm's best error=0.0925, best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 23, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.4s,  estimator xgb_limitdepth's best error=0.0820,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 24, current learner extra_tree\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.5s,  estimator extra_tree's best error=0.0922,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 25, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.5s,  estimator xgb_limitdepth's best error=0.0820,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 26, current learner rf\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.5s,  estimator rf's best error=0.0862,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 27, current learner rf\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.6s,  estimator rf's best error=0.0856,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:26] {2458} INFO - iteration 28, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:32:26] {2620} INFO -  at 6.6s,  estimator xgb_limitdepth's best error=0.0820,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:27] {2458} INFO - iteration 29, current learner sarimax\n[flaml.automl: 02-28 21:32:28] {2620} INFO -  at 7.9s,  estimator sarimax's best error=0.5313,  best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:28] {2458} INFO - iteration 30, current learner xgboost\n[flaml.automl: 02-28 21:32:28] {2620} INFO -  at 8.0s,  estimator xgboost's best error=0.0834,  best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:28] {2458} INFO - iteration 31, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:32:28] {2620} INFO -  at 8.0s,  estimator xgb_limitdepth's best error=0.0791,   best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:28] {2458} INFO - iteration 32, current learner arima\n[flaml.automl: 02-28 21:32:30] {2620} INFO -  at 10.3s, estimator arima's best error=0.5998,    best estimator prophet's best error=0.0592\n[flaml.automl: 02-28 21:32:32] {2850} INFO - retrain prophet for 2.2s\n[flaml.automl: 02-28 21:32:32] {2857} INFO - retrained model: <prophet.forecaster.Prophet object at 0x000001B1D3EE2B80>\n[flaml.automl: 02-28 21:32:32] {2234} INFO - fit succeeded\n[flaml.automl: 02-28 21:32:32] {2235} INFO - Time taken to find the best model: 4.351356506347656\n")),(0,n.kt)("h3",{id:"forecasting-discrete-variables"},"Forecasting Discrete Variables"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'from hcrystalball.utils import get_sales_data\nimport numpy as np\nfrom flaml import AutoML\n\ntime_horizon = 30\ndf = get_sales_data(n_dates=180, n_assortments=1, n_states=1, n_stores=1)\ndf = df[["Sales", "Open", "Promo", "Promo2"]]\n\n# feature engineering - create a discrete value column\n# 1 denotes above mean and 0 denotes below mean\ndf["above_mean_sales"] = np.where(df["Sales"] > df["Sales"].mean(), 1, 0)\ndf.reset_index(inplace=True)\n\n# train-test split\ndiscrete_train_df = df[:-time_horizon]\ndiscrete_test_df = df[-time_horizon:]\ndiscrete_X_train, discrete_X_test = (\n    discrete_train_df[["Date", "Open", "Promo", "Promo2"]],\n    discrete_test_df[["Date", "Open", "Promo", "Promo2"]],\n)\ndiscrete_y_train, discrete_y_test = discrete_train_df["above_mean_sales"], discrete_test_df["above_mean_sales"]\n\n# initialize AutoML instance\nautoml = AutoML()\n\n# configure the settings\nsettings = {\n    "time_budget": 15,  # total running time in seconds\n    "metric": "accuracy",  # primary metric\n    "task": "ts_forecast_classification",  # task type\n    "log_file_name": "sales_classification_forecast.log",  # flaml log file\n    "eval_method": "holdout",\n}\n\n# train the model\nautoml.fit(X_train=discrete_X_train,\n           y_train=discrete_y_train,\n           **settings,\n           period=time_horizon)\n\n# make predictions\ndiscrete_y_pred = automl.predict(discrete_X_test)\nprint("Predicted label", discrete_y_pred)\nprint("True label", discrete_y_test)\n')),(0,n.kt)("h4",{id:"sample-output-3"},"Sample Output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"[flaml.automl: 02-28 21:53:03] {2060} INFO - task = ts_forecast_classification\n[flaml.automl: 02-28 21:53:03] {2062} INFO - Data split method: time\n[flaml.automl: 02-28 21:53:03] {2066} INFO - Evaluation method: holdout\n[flaml.automl: 02-28 21:53:03] {2147} INFO - Minimizing error metric: 1-accuracy\n[flaml.automl: 02-28 21:53:03] {2205} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 0, current learner lgbm\n[flaml.automl: 02-28 21:53:03] {2573} INFO - Estimated sufficient time budget=269s. Estimated necessary time budget=0s.\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.1s,  estimator lgbm's best error=0.2667, best estimator lgbm's best error=0.2667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 1, current learner lgbm\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.1s,  estimator lgbm's best error=0.2667, best estimator lgbm's best error=0.2667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 2, current learner lgbm\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.1s,  estimator lgbm's best error=0.1333, best estimator lgbm's best error=0.1333\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 3, current learner rf\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.2s,  estimator rf's best error=0.1333,   best estimator lgbm's best error=0.1333\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 4, current learner xgboost\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.2s,  estimator xgboost's best error=0.1333,  best estimator lgbm's best error=0.1333\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 5, current learner lgbm\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.2s,  estimator lgbm's best error=0.1333, best estimator lgbm's best error=0.1333\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 6, current learner rf\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.3s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 7, current learner lgbm\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.3s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 8, current learner lgbm\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.3s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 9, current learner lgbm\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.4s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 10, current learner rf\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.4s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 11, current learner rf\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.4s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 12, current learner xgboost\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.5s,  estimator xgboost's best error=0.1333,  best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 13, current learner extra_tree\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.5s,  estimator extra_tree's best error=0.1333,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 14, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.5s,  estimator xgb_limitdepth's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 15, current learner xgboost\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.6s,  estimator xgboost's best error=0.0667,  best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 16, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.6s,  estimator xgb_limitdepth's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 17, current learner rf\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.6s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 18, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator xgb_limitdepth's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 19, current learner lgbm\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 20, current learner extra_tree\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator extra_tree's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 21, current learner xgboost\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator xgboost's best error=0.0667,  best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 22, current learner extra_tree\n[flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.8s,  estimator extra_tree's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 23, current learner rf\n[flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.8s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667\n[flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 24, current learner xgboost\n[flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgboost's best error=0.0333,  best estimator xgboost's best error=0.0333\n[flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 25, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgb_limitdepth's best error=0.0667,   best estimator xgboost's best error=0.0333\n[flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 26, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgb_limitdepth's best error=0.0667,   best estimator xgboost's best error=0.0333\n[flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 27, current learner xgboost\n[flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgboost's best error=0.0333,  best estimator xgboost's best error=0.0333\n[flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 28, current learner extra_tree\n[flaml.automl: 02-28 21:53:04] {2620} INFO -  at 1.0s,  estimator extra_tree's best error=0.0667,   best estimator xgboost's best error=0.0333\n[flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 29, current learner xgb_limitdepth\n[flaml.automl: 02-28 21:53:04] {2620} INFO -  at 1.0s,  estimator xgb_limitdepth's best error=0.0667,   best estimator xgboost's best error=0.0333\n[flaml.automl: 02-28 21:53:04] {2850} INFO - retrain xgboost for 0.0s\n[flaml.automl: 02-28 21:53:04] {2857} INFO - retrained model: XGBClassifier(base_score=0.5, booster='gbtree',\n              colsample_bylevel=0.9826753651836615, colsample_bynode=1,\n              colsample_bytree=0.9725493834064914, gamma=0, gpu_id=-1,\n              grow_policy='lossguide', importance_type='gain',\n              interaction_constraints='', learning_rate=0.1665803484560213,\n              max_delta_step=0, max_depth=0, max_leaves=4,\n              min_child_weight=0.5649012460525115, missing=nan,\n              monotone_constraints='()', n_estimators=4, n_jobs=-1,\n              num_parallel_tree=1, objective='binary:logistic', random_state=0,\n              reg_alpha=0.009638363373006869, reg_lambda=0.143703802530408,\n              scale_pos_weight=1, subsample=0.9643606787051899,\n              tree_method='hist', use_label_encoder=False,\n              validate_parameters=1, verbosity=0)\n[flaml.automl: 02-28 21:53:04] {2234} INFO - fit succeeded\n[flaml.automl: 02-28 21:53:04] {2235} INFO - Time taken to find the best model: 0.8547139167785645\n")),(0,n.kt)("h3",{id:"forecasting-with-panel-datasets"},"Forecasting with Panel Datasets"),(0,n.kt)("p",null,"Panel time series datasets involves multiple individual time series. For example, see Stallion demand dataset from PyTorch Forecasting, orginally from Kaggle."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'def get_stalliion_data():\n    from pytorch_forecasting.data.examples import get_stallion_data\n\n    data = get_stallion_data()\n    # add time index - For datasets with no missing values, FLAML will automate this process\n    data["time_idx"] = data["date"].dt.year * 12 + data["date"].dt.month\n    data["time_idx"] -= data["time_idx"].min()\n    # add additional features\n    data["month"] = data.date.dt.month.astype(str).astype(\n        "category"\n    )  # categories have be strings\n    data["log_volume"] = np.log(data.volume + 1e-8)\n    data["avg_volume_by_sku"] = data.groupby(\n        ["time_idx", "sku"], observed=True\n    ).volume.transform("mean")\n    data["avg_volume_by_agency"] = data.groupby(\n        ["time_idx", "agency"], observed=True\n    ).volume.transform("mean")\n    # we want to encode special days as one variable and thus need to first reverse one-hot encoding\n    special_days = [\n        "easter_day",\n        "good_friday",\n        "new_year",\n        "christmas",\n        "labor_day",\n        "independence_day",\n        "revolution_day_memorial",\n        "regional_games",\n        "beer_capital",\n        "music_fest",\n    ]\n    data[special_days] = (\n        data[special_days]\n        .apply(lambda x: x.map({0: "-", 1: x.name}))\n        .astype("category")\n    )\n    return data, special_days\n\ndata, special_days = get_stalliion_data()\ntime_horizon = 6  # predict six months\ntraining_cutoff = data["time_idx"].max() - time_horizon\ndata["time_idx"] = data["time_idx"].astype("int")\nts_col = data.pop("date")\ndata.insert(0, "date", ts_col)\n# FLAML assumes input is not sorted, but we sort here for comparison purposes with y_test\ndata = data.sort_values(["agency", "sku", "date"])\nX_train = data[lambda x: x.time_idx <= training_cutoff]\nX_test = data[lambda x: x.time_idx > training_cutoff]\ny_train = X_train.pop("volume")\ny_test = X_test.pop("volume")\nautoml = AutoML()\n# Configure settings for FLAML model\nsettings = {\n    "time_budget": budget,  # total running time in seconds\n    "metric": "mape",  # primary metric\n    "task": "ts_forecast_panel",  # task type\n    "log_file_name": "test/stallion_forecast.log",  # flaml log file\n    "eval_method": "holdout",\n}\n# Specify kwargs for TimeSeriesDataSet used by TemporalFusionTransformerEstimator\nfit_kwargs_by_estimator = {\n    "tft": {\n        "max_encoder_length": 24,\n        "static_categoricals": ["agency", "sku"],\n        "static_reals": ["avg_population_2017", "avg_yearly_household_income_2017"],\n        "time_varying_known_categoricals": ["special_days", "month"],\n        "variable_groups": {\n            "special_days": special_days\n        },  # group of categorical variables can be treated as one variable\n        "time_varying_known_reals": [\n            "time_idx",\n            "price_regular",\n            "discount_in_percent",\n        ],\n        "time_varying_unknown_categoricals": [],\n        "time_varying_unknown_reals": [\n            "y",  # always need a \'y\' column for the target column\n            "log_volume",\n            "industry_volume",\n            "soda_volume",\n            "avg_max_temp",\n            "avg_volume_by_agency",\n            "avg_volume_by_sku",\n        ],\n        "batch_size": 256,\n        "max_epochs": 1,\n        "gpu_per_trial": -1,\n    }\n}\n# Train the model\nautoml.fit(\n    X_train=X_train,\n    y_train=y_train,\n    **settings,\n    period=time_horizon,\n    group_ids=["agency", "sku"],\n    fit_kwargs_by_estimator=fit_kwargs_by_estimator,\n)\n# Compute predictions of testing dataset\ny_pred = automl.predict(X_test)\nprint(y_test)\nprint(y_pred)\n# best model\nprint(automl.model.estimator)\n')),(0,n.kt)("h4",{id:"sample-output-4"},"Sample Output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"[flaml.automl: 07-28 21:26:03] {2478} INFO - task = ts_forecast_panel\n[flaml.automl: 07-28 21:26:03] {2480} INFO - Data split method: time\n[flaml.automl: 07-28 21:26:03] {2483} INFO - Evaluation method: holdout\n[flaml.automl: 07-28 21:26:03] {2552} INFO - Minimizing error metric: mape\n[flaml.automl: 07-28 21:26:03] {2694} INFO - List of ML learners in AutoML Run: ['tft']\n[flaml.automl: 07-28 21:26:03] {2986} INFO - iteration 0, current learner tft\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\n\n   | Name                               | Type                            | Params\n----------------------------------------------------------------------------------------\n0  | loss                               | QuantileLoss                    | 0\n1  | logging_metrics                    | ModuleList                      | 0\n2  | input_embeddings                   | MultiEmbedding                  | 1.3 K\n3  | prescalers                         | ModuleDict                      | 256\n4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K\n5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K\n6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K\n7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K\n8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K\n9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K\n10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K\n11 | lstm_encoder                       | LSTM                            | 4.4 K\n12 | lstm_decoder                       | LSTM                            | 4.4 K\n13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544\n14 | post_lstm_add_norm_encoder         | AddNorm                         | 32\n15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K\n16 | multihead_attn                     | InterpretableMultiHeadAttention | 676\n17 | post_attn_gate_norm                | GateAddNorm                     | 576\n18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K\n19 | pre_output_gate_norm               | GateAddNorm                     | 576\n20 | output_layer                       | Linear                          | 119\n----------------------------------------------------------------------------------------\n33.6 K    Trainable params\n0         Non-trainable params\n33.6 K    Total params\n0.135     Total estimated model params size (MB)\n\nEpoch 19: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 129/129 [00:56<00:00,  2.27it/s, loss=45.9, v_num=2, train_loss_step=43.00, val_loss=65.20, train_loss_epoch=46.50]\n\n[flaml.automl: 07-28 21:46:46] {3114} INFO - Estimated sufficient time budget=12424212s. Estimated necessary time budget=12424s.\n[flaml.automl: 07-28 21:46:46] {3161} INFO -  at 1242.6s,\\testimator tft's best error=1324290483134574.7500,\\tbest estimator tft's best error=1324290483134574.7500\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\n\n   | Name                               | Type                            | Params\n----------------------------------------------------------------------------------------\n0  | loss                               | QuantileLoss                    | 0\n1  | logging_metrics                    | ModuleList                      | 0\n2  | input_embeddings                   | MultiEmbedding                  | 1.3 K\n3  | prescalers                         | ModuleDict                      | 256\n4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K\n5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K\n6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K\n7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K\n8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K\n9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K\n10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K\n11 | lstm_encoder                       | LSTM                            | 4.4 K\n12 | lstm_decoder                       | LSTM                            | 4.4 K\n13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544\n14 | post_lstm_add_norm_encoder         | AddNorm                         | 32\n15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K\n16 | multihead_attn                     | InterpretableMultiHeadAttention | 676\n17 | post_attn_gate_norm                | GateAddNorm                     | 576\n18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K\n19 | pre_output_gate_norm               | GateAddNorm                     | 576\n20 | output_layer                       | Linear                          | 119\n----------------------------------------------------------------------------------------\n33.6 K    Trainable params\n0         Non-trainable params\n33.6 K    Total params\n0.135     Total estimated model params size (MB)\nEpoch 19: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 145/145 [01:03<00:00,  2.28it/s, loss=45.2, v_num=3, train_loss_step=46.30, val_loss=67.60, train_loss_epoch=48.10]\n[flaml.automl: 07-28 22:08:05] {3425} INFO - retrain tft for 1279.6s\n[flaml.automl: 07-28 22:08:05] {3432} INFO - retrained model: TemporalFusionTransformer(\n  (loss): QuantileLoss()\n  (logging_metrics): ModuleList(\n    (0): SMAPE()\n    (1): MAE()\n    (2): RMSE()\n    (3): MAPE()\n  )\n  (input_embeddings): MultiEmbedding(\n    (embeddings): ModuleDict(\n      (agency): Embedding(58, 16)\n      (sku): Embedding(25, 10)\n      (special_days): TimeDistributedEmbeddingBag(11, 6, mode=sum)\n      (month): Embedding(12, 6)\n    )\n  )\n  (prescalers): ModuleDict(\n    (avg_population_2017): Linear(in_features=1, out_features=8, bias=True)\n    (avg_yearly_household_income_2017): Linear(in_features=1, out_features=8, bias=True)\n    (encoder_length): Linear(in_features=1, out_features=8, bias=True)\n    (y_center): Linear(in_features=1, out_features=8, bias=True)\n    (y_scale): Linear(in_features=1, out_features=8, bias=True)\n    (time_idx): Linear(in_features=1, out_features=8, bias=True)\n    (price_regular): Linear(in_features=1, out_features=8, bias=True)\n    (discount_in_percent): Linear(in_features=1, out_features=8, bias=True)\n    (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)\n    (y): Linear(in_features=1, out_features=8, bias=True)\n    (log_volume): Linear(in_features=1, out_features=8, bias=True)\n    (industry_volume): Linear(in_features=1, out_features=8, bias=True)\n    (soda_volume): Linear(in_features=1, out_features=8, bias=True)\n    (avg_max_temp): Linear(in_features=1, out_features=8, bias=True)\n    (avg_volume_by_agency): Linear(in_features=1, out_features=8, bias=True)\n    (avg_volume_by_sku): Linear(in_features=1, out_features=8, bias=True)\n  )\n  (static_variable_selection): VariableSelectionNetwork(\n    (flattened_grn): GatedResidualNetwork(\n      (resample_norm): ResampleNorm(\n        (resample): TimeDistributedInterpolation()\n        (gate): Sigmoid()\n        (norm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n      )\n      (fc1): Linear(in_features=66, out_features=7, bias=True)\n      (elu): ELU(alpha=1.0)\n      (fc2): Linear(in_features=7, out_features=7, bias=True)\n      (gate_norm): GateAddNorm(\n        (glu): GatedLinearUnit(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (fc): Linear(in_features=7, out_features=14, bias=True)\n        )\n        (add_norm): AddNorm(\n          (norm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (single_variable_grns): ModuleDict(\n      (agency): ResampleNorm(\n        (gate): Sigmoid()\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n      (sku): ResampleNorm(\n        (resample): TimeDistributedInterpolation()\n        (gate): Sigmoid()\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n      (avg_population_2017): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (avg_yearly_household_income_2017): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (encoder_length): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (y_center): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (y_scale): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (prescalers): ModuleDict(\n      (avg_population_2017): Linear(in_features=1, out_features=8, bias=True)\n      (avg_yearly_household_income_2017): Linear(in_features=1, out_features=8, bias=True)\n      (encoder_length): Linear(in_features=1, out_features=8, bias=True)\n      (y_center): Linear(in_features=1, out_features=8, bias=True)\n      (y_scale): Linear(in_features=1, out_features=8, bias=True)\n    )\n    (softmax): Softmax(dim=-1)\n  )\n  (encoder_variable_selection): VariableSelectionNetwork(\n    (flattened_grn): GatedResidualNetwork(\n      (resample_norm): ResampleNorm(\n        (resample): TimeDistributedInterpolation()\n        (gate): Sigmoid()\n        (norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n      )\n      (fc1): Linear(in_features=100, out_features=13, bias=True)\n      (elu): ELU(alpha=1.0)\n      (context): Linear(in_features=16, out_features=13, bias=False)\n      (fc2): Linear(in_features=13, out_features=13, bias=True)\n      (gate_norm): GateAddNorm(\n        (glu): GatedLinearUnit(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (fc): Linear(in_features=13, out_features=26, bias=True)\n        )\n        (add_norm): AddNorm(\n          (norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (single_variable_grns): ModuleDict(\n      (special_days): ResampleNorm(\n        (resample): TimeDistributedInterpolation()\n        (gate): Sigmoid()\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n      (month): ResampleNorm(\n        (resample): TimeDistributedInterpolation()\n        (gate): Sigmoid()\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n      (time_idx): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (price_regular): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (discount_in_percent): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (relative_time_idx): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (y): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (log_volume): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (industry_volume): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (soda_volume): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (avg_max_temp): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (avg_volume_by_agency): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (avg_volume_by_sku): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (prescalers): ModuleDict(\n      (time_idx): Linear(in_features=1, out_features=8, bias=True)\n      (price_regular): Linear(in_features=1, out_features=8, bias=True)\n      (discount_in_percent): Linear(in_features=1, out_features=8, bias=True)\n      (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)\n      (y): Linear(in_features=1, out_features=8, bias=True)\n      (log_volume): Linear(in_features=1, out_features=8, bias=True)\n      (industry_volume): Linear(in_features=1, out_features=8, bias=True)\n      (soda_volume): Linear(in_features=1, out_features=8, bias=True)\n      (avg_max_temp): Linear(in_features=1, out_features=8, bias=True)\n      (avg_volume_by_agency): Linear(in_features=1, out_features=8, bias=True)\n      (avg_volume_by_sku): Linear(in_features=1, out_features=8, bias=True)\n    )\n    (softmax): Softmax(dim=-1)\n  )\n  (decoder_variable_selection): VariableSelectionNetwork(\n    (flattened_grn): GatedResidualNetwork(\n      (resample_norm): ResampleNorm(\n        (resample): TimeDistributedInterpolation()\n        (gate): Sigmoid()\n        (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n      )\n      (fc1): Linear(in_features=44, out_features=6, bias=True)\n      (elu): ELU(alpha=1.0)\n      (context): Linear(in_features=16, out_features=6, bias=False)\n      (fc2): Linear(in_features=6, out_features=6, bias=True)\n      (gate_norm): GateAddNorm(\n        (glu): GatedLinearUnit(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (fc): Linear(in_features=6, out_features=12, bias=True)\n        )\n        (add_norm): AddNorm(\n          (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (single_variable_grns): ModuleDict(\n      (special_days): ResampleNorm(\n        (resample): TimeDistributedInterpolation()\n        (gate): Sigmoid()\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n      (month): ResampleNorm(\n        (resample): TimeDistributedInterpolation()\n        (gate): Sigmoid()\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n      (time_idx): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (price_regular): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (discount_in_percent): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (relative_time_idx): GatedResidualNetwork(\n        (resample_norm): ResampleNorm(\n          (resample): TimeDistributedInterpolation()\n          (gate): Sigmoid()\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n        )\n        (fc1): Linear(in_features=8, out_features=8, bias=True)\n        (elu): ELU(alpha=1.0)\n        (fc2): Linear(in_features=8, out_features=8, bias=True)\n        (gate_norm): GateAddNorm(\n          (glu): GatedLinearUnit(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc): Linear(in_features=8, out_features=32, bias=True)\n          )\n          (add_norm): AddNorm(\n            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (prescalers): ModuleDict(\n      (time_idx): Linear(in_features=1, out_features=8, bias=True)\n      (price_regular): Linear(in_features=1, out_features=8, bias=True)\n      (discount_in_percent): Linear(in_features=1, out_features=8, bias=True)\n      (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)\n    )\n    (softmax): Softmax(dim=-1)\n  )\n  (static_context_variable_selection): GatedResidualNetwork(\n    (fc1): Linear(in_features=16, out_features=16, bias=True)\n    (elu): ELU(alpha=1.0)\n    (fc2): Linear(in_features=16, out_features=16, bias=True)\n    (gate_norm): GateAddNorm(\n      (glu): GatedLinearUnit(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (fc): Linear(in_features=16, out_features=32, bias=True)\n      )\n      (add_norm): AddNorm(\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (static_context_initial_hidden_lstm): GatedResidualNetwork(\n    (fc1): Linear(in_features=16, out_features=16, bias=True)\n    (elu): ELU(alpha=1.0)\n    (fc2): Linear(in_features=16, out_features=16, bias=True)\n    (gate_norm): GateAddNorm(\n      (glu): GatedLinearUnit(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (fc): Linear(in_features=16, out_features=32, bias=True)\n      )\n      (add_norm): AddNorm(\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (static_context_initial_cell_lstm): GatedResidualNetwork(\n    (fc1): Linear(in_features=16, out_features=16, bias=True)\n    (elu): ELU(alpha=1.0)\n    (fc2): Linear(in_features=16, out_features=16, bias=True)\n    (gate_norm): GateAddNorm(\n      (glu): GatedLinearUnit(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (fc): Linear(in_features=16, out_features=32, bias=True)\n      )\n      (add_norm): AddNorm(\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (static_context_enrichment): GatedResidualNetwork(\n    (fc1): Linear(in_features=16, out_features=16, bias=True)\n    (elu): ELU(alpha=1.0)\n    (fc2): Linear(in_features=16, out_features=16, bias=True)\n    (gate_norm): GateAddNorm(\n      (glu): GatedLinearUnit(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (fc): Linear(in_features=16, out_features=32, bias=True)\n      )\n      (add_norm): AddNorm(\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (lstm_encoder): LSTM(16, 16, num_layers=2, batch_first=True, dropout=0.1)\n  (lstm_decoder): LSTM(16, 16, num_layers=2, batch_first=True, dropout=0.1)\n  (post_lstm_gate_encoder): GatedLinearUnit(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (fc): Linear(in_features=16, out_features=32, bias=True)\n  )\n  (post_lstm_gate_decoder): GatedLinearUnit(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (fc): Linear(in_features=16, out_features=32, bias=True)\n  )\n  (post_lstm_add_norm_encoder): AddNorm(\n    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n  )\n  (post_lstm_add_norm_decoder): AddNorm(\n    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n  )\n  (static_enrichment): GatedResidualNetwork(\n    (fc1): Linear(in_features=16, out_features=16, bias=True)\n    (elu): ELU(alpha=1.0)\n    (context): Linear(in_features=16, out_features=16, bias=False)\n    (fc2): Linear(in_features=16, out_features=16, bias=True)\n    (gate_norm): GateAddNorm(\n      (glu): GatedLinearUnit(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (fc): Linear(in_features=16, out_features=32, bias=True)\n      )\n      (add_norm): AddNorm(\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (multihead_attn): InterpretableMultiHeadAttention(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (v_layer): Linear(in_features=16, out_features=4, bias=True)\n    (q_layers): ModuleList(\n      (0): Linear(in_features=16, out_features=4, bias=True)\n      (1): Linear(in_features=16, out_features=4, bias=True)\n      (2): Linear(in_features=16, out_features=4, bias=True)\n      (3): Linear(in_features=16, out_features=4, bias=True)\n    )\n    (k_layers): ModuleList(\n      (0): Linear(in_features=16, out_features=4, bias=True)\n      (1): Linear(in_features=16, out_features=4, bias=True)\n      (2): Linear(in_features=16, out_features=4, bias=True)\n      (3): Linear(in_features=16, out_features=4, bias=True)\n    )\n    (attention): ScaledDotProductAttention(\n      (softmax): Softmax(dim=2)\n    )\n    (w_h): Linear(in_features=4, out_features=16, bias=False)\n  )\n  (post_attn_gate_norm): GateAddNorm(\n    (glu): GatedLinearUnit(\n      (dropout): Dropout(p=0.1, inplace=False)\n      (fc): Linear(in_features=16, out_features=32, bias=True)\n    )\n    (add_norm): AddNorm(\n      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (pos_wise_ff): GatedResidualNetwork(\n    (fc1): Linear(in_features=16, out_features=16, bias=True)\n    (elu): ELU(alpha=1.0)\n    (fc2): Linear(in_features=16, out_features=16, bias=True)\n    (gate_norm): GateAddNorm(\n      (glu): GatedLinearUnit(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (fc): Linear(in_features=16, out_features=32, bias=True)\n      )\n      (add_norm): AddNorm(\n        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (pre_output_gate_norm): GateAddNorm(\n    (glu): GatedLinearUnit(\n      (fc): Linear(in_features=16, out_features=32, bias=True)\n    )\n    (add_norm): AddNorm(\n      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (output_layer): Linear(in_features=16, out_features=7, bias=True)\n)\n[flaml.automl: 07-28 22:08:05] {2725} INFO - fit succeeded\n[flaml.automl: 07-28 22:08:05] {2726} INFO - Time taken to find the best model: 1242.6435902118683\n[flaml.automl: 07-28 22:08:05] {2737} WARNING - Time taken to find the best model is 414% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\\n\"\n     ]\n    }\n   ],\n")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/microsoft/FLAML/blob/main/notebook/automl_time_series_forecast.ipynb"},"Link to notebook")," | ",(0,n.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_time_series_forecast.ipynb"},"Open in colab")))}u.isMDXComponent=!0},6806:(e,t,r)=>{r.d(t,{Z:()=>a});const a=r.p+"assets/images/CO2-8a52a5b6467f2f3c0b4bc0fc516d5a62.png"}}]);